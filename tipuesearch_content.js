var tipuesearch = {"pages":[{"title":"About","text":"Hi，我叫Fang Ying，Yori Fang是我的英文名字。 出生于1990年7月，毕业于： 浙江大学信息与电子工程学院 (Master) 2013-2016 东北大学信息科学与工程学院 (Bachelor) 2009-2013 现在从事与linux内核和操作系统虚拟化方面的工作。主要工作在QEMU/KVM虚拟方向，致力于使能ICT云计算基础架构。 您可以通过下面的方式联系到我： Mail: fangying2725@gmail.com Github: https://github.com/fangying WeChat: Yori Fang 也可以通过WeChat赞赏来支持这个站点（所有的赞赏都会列出在本站中）。 Hi all, my name is Fang Ying and Yori Fang is my English name. I was born in July 1990. Graduated From: College of Electronic and Infomation Zhejiang University as Master 2013-2016 College of Computer Science Northeastern University as Bachelor 2009-2013 I'm now working on things related to linux kernel and OS virtualization. I'm a virtualization developmenter, working on KVM and QEMU and delicated to enable ICT on Cloud Computing. Mail: fangying2725@gmail.com Github: https://github.com/fangying WeChat: Yori Fang You can conntact with me via WeChat, and can also encourage me to keep writting by donation.","tags":"pages","url":"https://kernelgo.org/pages/about.html","loc":"https://kernelgo.org/pages/about.html"},{"title":"Virtio Overview","text":"摘要 半虚拟化设备(Virtio Device)在当前云计算虚拟化场景下已经得到了非常广泛的应用， 并且现在也有越来越多的物理设备也开始支持Virtio协议，即所谓的Virtio Offload， 通过将virtio协议卸载到硬件上（例如网卡）让物理机和虚拟机都能够获得加速体验。 本文中我们来重点了解一下virtio技术中的一些关键点，方便我们加深对半虚拟化的理解。 本文适合对IO虚拟化有一定了解的人群阅读，本文的目的是对想要了解virtio内部机制的读者提供一下帮助。 在开始了解virtio之前，我们先思考一下几个相关问题： virtio设备是基于PCI设备标准，virtio设备的PCI配置空间都有哪些内容？ virtio前端和后端基于共享内存机制进行通信，它是凭什么可以做到无锁的？ virtio机制中有那几个关键的数据结构？virtio是如何工作的？ virtio前后端是如何进行通信的？irqfd和ioeventfd是什么回事儿？在virtio前后端通信中是怎么用到的？ virtio设备支持MSIx，在qemu/kvm中具体是怎么实现对MSIx的模拟呢？ virtio modern相对于virtio legay多了哪些新特性？ 0. 简单了解一下Virtio Spec协议 virtio协议标准最早又IBM提出，virtio作为一套标准协议现在有专门的技术委员会进行管理， 具体的标准可以访问 virtio官网 ， 开发者可以向技术委员会提供新的virtio设备提案（RFC），经过委员会通过后可以增加新的virtio设备类型。 组成一个virtio设备的四要素包括： 设备状态域，feature bits，设备配置空间，一个或者多个virtqueue。 其中设备状态域包含6种状态： ACKNOWLEDGE（1）：GuestOS发现了这个设备，并且认为这是一个有效的virtio设备； DRIVER (2) : GuestOS知道该如何驱动这个设备； FAILED (128) : GuestOS无法正常驱动这个设备，Something is wriong； FEATURES_OK (8) : GuestOS认识所有的feature，并且feature协商一完成； DRIVER_OK (4) : 驱动加载完成，设备可以投入使用了； DEVICE_NEEDS_RESET (64) ：设备触发了错误，需要重置才能继续工作。 feature bits用来标志设备支持那个特性，其中bit0-bit23是特定设备可以使用的feature bits， bit24-bit37预给队列和feature协商机制，bit38以上保留给未来其他用途。 例如：对于virtio-net设备而言，feature bit0表示网卡设备支持checksum校验。 VIRTIO_F_VERSION_1这个feature bit用来表示设备是否支持virtio 1.0 spec标准。 在virtio协议中，所有的设备都使用virtqueue来进行数据的传输。 每个设备可以有0个或者多个virtqueue ，每个virtqueue占用2个或者更多个4K的物理页。 virtqueue有Split Virtqueues和Packed Virtqueues两种模式，在Split virtqueues模式下virtqueue被分成若干个部分， 每个部分都是前端驱动或者后端单向可写的（不能两端同时写）。 每个virtqueue都有一个16bit的queue size参数，表示队列的总长度。 每个virtqueue由3个部分组成： +-------------------+--------------------------------+-----------------------+ | Descriptor Table | Available Ring (padding) | Used Ring | +-------------------+--------------------------------+-----------------------+ Descriptor Table：存放IO传输请求信息； Available Ring：记录了Descriptor Table表中的哪些项被更新了，前端Driver可写但后端只读； Used Ring：记录Descriptor Table表中哪些请求已经被提交到硬件，前端Driver只读但后端可写。 整个virtio协议中设备IO请求的工作机制可以简单地概括为： 前端驱动将IO请求放到Descriptor Table中，然后将索引更新到Available Ring中，然后kick后端去取数据； 后端取出IO请求进行处理，然后结果刷新到Descriptor Table中再更新Using Ring，然后发送中断notify前端。 从virtio协议可以了解到virtio设备支持3种设备呈现模式： Virtio Over PCI BUS，依旧遵循PCI规范，挂在到PCI总线上； Virtio Over MMIO，部分不支持PCI协议的虚拟化平台可以使用这种工作模式； Virtio Over Channel I/O：主要用在s390平台上，virtio-ccw使用这种基于channel I/O的机制。 其中，Virtio Over PCI BUS的使用比较广泛，作为PCI设备需按照规范要通过PCI配置空间来向操作系统报告设备支持的特性集合， 这样操作系统才知道这是一个什么类型的virtio设备，并调用对应的前端驱动和这个设备进行握手，进而将设备驱动起来。 QEMU会给virtio设备模拟PCI配置空间，对于virtio设备来说PCI Vendor ID固定为0x1AF4， PCI Device ID 为 0x1000到0x107F之间的是virtio设备。 前面提到virtio设备有feature bits，virtqueue等四要素，那么在virtio-pci模式下是如何呈现的呢？ 从virtio spec来看，老的virtio协议和新的virtio协议在这一块有很大改动。 virtio legacy（virtio 0.95）协议规定，对应的配置数据结构（virtio common configuration structure） 应该存放在设备的BAR0里面，我们称之为virtio legay interface，其结构如下： virtio legacy ==> PCI BA0 + ------------------------------------------------------------------+ | Host Feature Bits [ 0 : 31 ] | + ------------------------------------------------------------------+ | Guest Feature Bits [ 0 : 31 ] | + ------------------------------------------------------------------+ | Virtqueue Address PFN | + ---------------------------------+--------------------------------+ | Queue Select | Queue Size | + ----------------+----------------+--------------------------------+ | ISR Status | Device Stat | Queue Notify | + ----------------+----------------+--------------------------------+ | MSI Config Vector | MSI Queue Vector | + ---------------------------------+--------------------------------+ 对于新的virtio modern，协议将配置结构划分为5种类型： /* Common configuration */ # define VIRTIO_PCI_CAP_COMMON_CFG 1 /* Notifications */ # define VIRTIO_PCI_CAP_NOTIFY_CFG 2 /* ISR Status */ # define VIRTIO_PCI_CAP_ISR_CFG 3 /* Device specific configuration */ # define VIRTIO_PCI_CAP_DEVICE_CFG 4 /* PCI configuration access */ # define VIRTIO_PCI_CAP_PCI_CFG 5 以上的每种配置结构是直接映射到virtio设备的BAR空间内，那么如何指定每种配置结构的位置呢？ 答案是通过PCI Capability list方式去指定，这和物理PCI设备是一样的，体现了virtio-pci的协议兼容性。 struct virtio_pci_cap { u8 cap_vndr ; /* Generic PCI field: PCI_CAP_ID_VNDR */ u8 cap_next ; /* Generic PCI field: next ptr. */ u8 cap_len ; /* Generic PCI field: capability length */ u8 cfg_type ; /* Identifies the structure. */ u8 bar ; /* Where to find it. */ u8 padding [ 3 ] ; /* Pad to full dword. */ le32 offset ; /* Offset within bar. */ le32 length ; /* Length of the structure, in bytes. */ } ; 只是略微不同的是，virtio-pci的Capability有一个统一的结构， 其中cfg_type表示Cap的类型，bar表示这个配置结构被映射到的BAR空间号。 这样每个配置结构都可以通过BAR空间直接访问，或者通过PCI配置空间的VIRTIO_PCI_CAP_PCI_CFG域进行访问。 每个Cap的具体结构定义可以参考virtio spec 4.1.4.3小节。 1. 前后端数据共享 传统的纯模拟设备在工作的时候，会触发频繁的陷入陷出， 而且IO请求的内容要进行多次拷贝传递，严重影响了设备的IO性能。 virtio为了提升设备的IO性能，采用了共享内存机制， 前端驱动会提前申请好一段物理地址空间用来存放IO请求，然后将这段地址的GPA告诉QEMU。 前端驱动在下发IO请求后，QEMU可以直接从共享内存中取出请求，然后将完成后的结果又直接写到虚拟机对应地址上去。 整个过程中可以做到直拿直取，省去了很多开销。 Virtqueue是整个virtio方案的灵魂所在。每个virtqueue都包含3张表， Descriptor Table存放了IO请求描述符，available ring记录了当前哪些描述符是可用的，used ring记录了哪些描述符已经被后端使用了。 + ------------------------------------+ | virtio guest driver | + -----------------+------------------+ / | &#94; / | \\ put update get / | \\ V V \\ + ----------+ +------------+ +----------+ | | | | | | + ----------+ +------------+ +----------+ | available | | descriptor | | used | | ring | | table | | ring | + ----------+ +------------+ +----------+ | | | | | | + ----------+ +------------+ +----------+ | | | | | | + ----------+ +------------+ +----------+ \\ &#94; &#94; \\ | / get update put \\ | / V | / + ----------------+-------------------+ | virtio host backend | + ------------------------------------+ Desriptor Table中存放的是一个一个的virtq_desc元素，每个virq_desc元素占用16个字节。 +-----------------------------------------------------------+ | addr / gpa [ 0 : 63 ] | +-------------------------+-----------------+---------------+ | len [ 0 : 31 ] | flags [ 0 : 15 ] | next [ 0 : 15 ] | +-------------------------+-----------------+---------------+ 其中，addr占用64bit存放了单个IO请求的GPA地址信息，例如addr可能表示某个DMA buffer的起始地址。 len占用32bit表示IO请求的长度，flags的取值有3种， VIRTQ_DESC_F_NEXT表示这个IO请求和下一个virtq_desc描述的是连续的， IRTQ_DESC_F_WRITE表示这段buffer是write only的， VIRTQ_DESC_F_INDIRECT表示这段buffer里面放的内容是另外一组buffer的virtq_desc（相当于重定向）， next是指向下一个virtq_desc的索引号（前提是VIRTQ_DESC_F_NEXT & flags）。 Available Ring是前端驱动用来告知后端那些IO buffer是的请求需要处理，每个Ring中包含一个virtq_avail占用8个字节。 其中，flags取值为VIRTQ_AVAIL_F_NO_INTERRUPT时表示前端驱动告诉后端：\"当你消耗完一个IO buffer的时候，不要立刻给我发中断\"。 idx表示下次前端驱动要放置Descriptor Entry的地方。 + --------------+-------------+--------------+---------------------+ | flags [ 0 : 15 ] | idx [ 0 : 15 ] | ring [ 0 : 15 ] | used_event [ 0 : 15 ] | + --------------+-------------+--------------+---------------------+ Used Ring结构稍微不一样，flags的值如果为VIRTIO_F_EVENT_IDX并且前后端协商VIRTIO_F_EVENT_IDX feature成功, 那么Guest会将used ring index放在available ring的末尾，告诉后端说：\"Hi 小老弟，当你处理完这个请求的时候，给我发个中断通知我一下\"， 同时host也会将avail_event index放到used ring的末尾，告诉guest说：\"Hi 老兄，记得把这个idx的请求kick给我哈\"。 VIRTIO_F_EVENT_IDX对virtio通知/中断有一定的优化，在某些场景下能够提升IO性能。 /* The Guest publishes the used index for which it expects an interrupt * at the end of the avail ring. Host should ignore the avail->flags field. */ /* The Host publishes the avail index for which it expects a kick * at the end of the used ring. Guest should ignore the used->flags field. */ struct virtq_used { # define VIRTQ_USED_F_NO_NOTIFY 1 le16 flags ; le16 idx ; struct virtq_used_elem ring [ /* Queue Size */ ] ; le16 avail_event ; /* Only if VIRTIO_F_EVENT_IDX */ } ; /* le32 is used here for ids for padding reasons. */ struct virtq_used_elem { /* Index of start of used descriptor chain. */ le32 id ; /* Total length of the descriptor chain which was used (written to) */ le32 len ; } ; 原理就到这里，后面会以virtio网卡为例进行详细流程说明。 2. 前后端通信机制（irqfd 与 ioeventfd） 共享内存方式解决了传统设备IO过程中内存拷贝带来的性能损耗问题，除此之外前端驱动和后端驱动的通信问题也是有可以改进的地方。 Virtio前后端通信概括起来只有两个方向，即GuestOS通知QEMU和QEMU通知GuestOS。 当前端驱动准备好IO buffer之后，需要通知后端（QEMU），告诉后端：\"小老弟，我有一波IO请求已经准备好了，你帮我处理一下\"。 前端通知出去后，就可以等待IO结果了（操作系统可以进行一次调度），这时候vCPU可以去干点其他的事情。 后端收到消息后开始处理IO请求，当IO请求处理完成之后，后端就通过中断机制通知GuestOS：\"老哥，你的IO给你处理好了，你来取一下\"。 前后端通信机制如下图所示： + -------------+ +-------------+ | | | | | | | | | GuestOS | | QEMU | | | | | | | | | + ---+---------+ +----+--------+ | &#94; | &#94; | | | | + ---|-----|-------------------------|----|---+ | | | irqfd | | | | | + -------------------------+ | | | | ioeventfd | | | + ------------------------------------+ | | KVM | + --------------------------------------------+ 前端驱动通知后端比较简单，QEMU设置一段特定的MMIO地址空间，前端驱动访问这段MMIO触发VMExit， 退出到KVM后利用ioeventfd机制通知到用户态的QEMU，QEMU主循环（main_loop poll）检测到ioeventfd事件后调用callback进行处理。 前端驱动通知后端： 内核流程 mark一下 ， PCI设备驱动流程这个后面可以学习一下 ，先扫描 PCI bus发现是virtio设备再扫描virtio - bus 。 worker_thread --> process_one_work --> pciehp_power_thread --> pciehp_enable_slot --> pciehp_configure_device --> pci_bus_add_devices --> pci_bus_add_device --> device_attach --> __device_attach --> bus_for_each_drv --> __device_attach_driver --> driver_probe_device --> pci_device_probe --> local_pci_probe --> virtio_pci_probe --> register_virtio_device --> device_register --> device_add --> bus_probe_device --> device_initial_probe --> __device_attach --> bus_for_each_drv --> __device_attach_driver --> driver_probe_device --> virtio_dev_probe --> virtnet_probe ( 网卡设备驱动加载的入口 ) static int virtnet_probe ( struct virtio_device * vdev ) { virtio_device_ready ( vdev ); } /** * virtio_device_ready - enable vq use in probe function * @vdev: the device * * Driver must call this to use vqs in the probe function. * * Note: vqs are enabled automatically after probe returns. */ static inline void virtio_device_ready ( struct virtio_device * dev ) { unsigned status = dev -> config -> get_status ( dev ); BUG_ON ( status & VIRTIO_CONFIG_S_DRIVER_OK ); dev -> config -> set_status ( dev , status | VIRTIO_CONFIG_S_DRIVER_OK ); } QEMU / KVM后端的处理流程如下 ： 前端驱动写 Status位 ， val & VIRTIO_CONFIG_S_DRIVER_OK ，这时候前端驱动已经 ready virtio_pci_config_write --> virtio_ioport_write --> virtio_pci_start_ioeventfd --> virtio_bus_set_host_notifier --> virtio_bus_start_ioeventfd --> virtio_device_start_ioeventfd_impl --> virtio_bus_set_host_notifier --> virtio_pci_ioeventfd_assign --> memory_region_add_eventfd --> memory_region_transaction_commit --> address_space_update_ioeventfds --> address_space_add_del_ioeventfds --> kvm_io_ioeventfd_add / vhost_eventfd_add --> kvm_set_ioeventfd_pio --> kvm_vm_ioctl ( kvm_state , KVM_IOEVENTFD , & kick ) 其实，这就是QEMU的fast mmio实现机制。我们可以看到，QEMU会为每个设备MMIO对应的MemoryRegion注册一个ioeventfd。 最后调用了一个KVM_IOEVENTFD ioctl到KVM内核里面，而在KVM内核中会将MMIO对应的（gpa,len,eventfd）信息会注册到KVM_FAST_MMIO_BUS上。 这样当Guest访问MMIO地址范围退出后（触发EPT Misconfig），KVM会查询一下访问的GPA是否落在某段MMIO地址空间range内部， 如果是的话就直接写eventfd告知QEMU，QEMU就会从coalesced mmio ring page中取MMIO请求（注：pio page和 mmio page是QEMU和KVM内核之间的共享内存页，已经提前mmap好了）。 kvm内核代码virt / kvm / eventfd . c中 kvm_vm_ioctl ( KVM_IOEVENTFD ) --> kvm_ioeventfd --> kvm_assign_ioeventfd --> kvm_assign_ioeventfd_idx MMIO处理流程中 （ handle_ept_misconfig ）最后会调用到 ioeventfd_write通知QEMU 。 /* MMIO/PIO writes trigger an event if the addr/val match */ static int ioeventfd_write ( struct kvm_vcpu * vcpu , struct kvm_io_device * this , gpa_t addr , int len , const void * val ) { struct _ioeventfd * p = to_ioeventfd ( this ); if ( ! ioeventfd_in_range ( p , addr , len , val )) return - EOPNOTSUPP ; eventfd_signal ( p -> eventfd , 1 ); return 0 ; } 不了解MMIO是如何模拟的童鞋，可以结合本站的文章 MMIO模拟实现分析 去了解一下，不懂的可以在文章下面评论。 后端通知前端，是通过中断的方式 ，QEMU/KVM中有一套完整的中断模拟实现框架， 如果对QEMU/KVM中断模拟不熟悉的童鞋， 建议阅读一下这篇文章： QEMU学习笔记-中断 。 对于virtio-pci设备，可以通过Cap呈现MSIx给虚拟机，这样在前端驱动加载的时候就会尝试去使能MSIx中断， 后端在这个时候建立起MSIx通道。 前端驱动加载(probe)的过程中，会去初始化virtqueue，这个时候会去申请MSIx中断： virtnet_probe --> init_vqs --> virtnet_find_vqs --> vi -> vdev -> config -> find_vqs [ vp_modern_find_vqs ] --> vp_find_vqs --> vp_find_vqs_msix // 为每virtqueue申请一个MSIx中断，通常收发各一个队列 --> vp_request_msix_vectors // 主要的MSIx中断申请逻辑都在这个函数里面 --> pci_alloc_irq_vectors_affinity //这里申请MSIx中断描述符 --> request_irq // virtio-net网卡至少申请了3个MSIx中断： // 一个是configuration change中断（配置空间发生变化后，QEMU通知前端） // 发送队列1个MSIx中断，接收队列1MSIx中断 在QEMU/KVM这一侧，开始模拟MSIx中断，具体流程大致如下： virtio_pci_config_write --> virtio_ioport_write --> virtio_set_status --> virtio_net_vhost_status --> vhost_net_start --> virtio_pci_set_guest_notifiers --> kvm_virtio_pci_vector_use |--> kvm_irqchip_add_msi_route // 更新中断路由表 |--> kvm_virtio_pci_irqfd_use // 使能 MSI 中断 --> kvm_irqchip_add_irqfd_notifier_gsi --> kvm_irqchip_assign_irqfd 申请 MSIx 中断的时候，会为 MSIx 分配一个 gsi ，并为这个 gsi 绑定一个 irqfd ，然后调用 ioctl KVM_IRQFD 注册到内核中。 static int kvm_irqchip_assign_irqfd ( KVMState * s , int fd , int rfd , int virq , bool assign ) { struct kvm_irqfd irqfd = { . fd = fd , . gsi = virq , . flags = assign ? 0 : KVM_IRQFD_FLAG_DEASSIGN , } ; if ( rfd != - 1 ) { irqfd . flags |= KVM_IRQFD_FLAG_RESAMPLE ; irqfd . resamplefd = rfd ; } if ( ! kvm_irqfds_enabled ()) { return - ENOSYS ; } return kvm_vm_ioctl ( s , KVM_IRQFD , & irqfd ) ; } KVM 内核代码 virt / kvm / eventfd . c kvm_vm_ioctl ( s , KVM_IRQFD , & irqfd ) --> kvm_irqfd_assign --> vfs_poll ( f . file , & irqfd -> pt ) // 在内核中 poll 这个 irqfd 从上面的流程可以看出，QEMU/KVM使用irqfd机制来模拟MSIx中断， 即设备申请MSIx中断的时候会为MSIx分配一个gsi（这个时候会刷新irq routing table）， 并为这个gsi绑定一个irqfd，最后在内核中去poll这个irqfd。 当QEMU处理完IO之后，就写MSIx对应的irqfd，给前端注入一个MSIx中断，告知前端我已经处理好IO了你可以来取结果了。 例如，virtio-scsi从前端取出IO请求后会取做DMA操作（DMA是异步的，QEMU协程中负责处理）。 当DMA完成后QEMU需要告知前端IO请求已完成（Complete），那么怎么去投递这个MSIx中断呢？ 答案是调用 virtio_notify_irqfd 注入一个MSIx中断。 #0 0x00005604798d569b in virtio_notify_irqfd (vdev=0x56047d12d670, vq=0x7fab10006110) at hw/virtio/virtio.c:1684 #1 0x00005604798adea4 in virtio_scsi_complete_req (req=0x56047d09fa70) at hw/scsi/virtio-scsi.c:76 #2 0x00005604798aecfb in virtio_scsi_complete_cmd_req (req=0x56047d09fa70) at hw/scsi/virtio-scsi.c:468 #3 0x00005604798aee9d in virtio_scsi_command_complete (r=0x56047ccb0be0, status=0, resid=0) at hw/scsi/virtio-scsi.c:495 #4 0x0000560479b397cf in scsi_req_complete (req=0x56047ccb0be0, status=0) at hw/scsi/scsi-bus.c:1404 #5 0x0000560479b2b503 in scsi_dma_complete_noio (r=0x56047ccb0be0, ret=0) at hw/scsi/scsi-disk.c:279 #6 0x0000560479b2b610 in scsi_dma_complete (opaque=0x56047ccb0be0, ret=0) at hw/scsi/scsi-disk.c:300 #7 0x00005604799b89e3 in dma_complete (dbs=0x56047c6e9ab0, ret=0) at dma-helpers.c:118 #8 0x00005604799b8a90 in dma_blk_cb (opaque=0x56047c6e9ab0, ret=0) at dma-helpers.c:136 #9 0x0000560479cf5220 in blk_aio_complete (acb=0x56047cd77d40) at block/block-backend.c:1327 #10 0x0000560479cf5470 in blk_aio_read_entry (opaque=0x56047cd77d40) at block/block-backend.c:1387 #11 0x0000560479df49c4 in coroutine_trampoline (i0=2095821104, i1=22020) at util/coroutine-ucontext.c:115 #12 0x00007fab214d82c0 in __start_context () at /usr/lib64/libc.so.6 在 virtio_notify_irqfd 函数中，会去写irqfd，给内核发送一个信号。 void virtio_notify_irqfd ( VirtIODevice * vdev , VirtQueue * vq ) { ... /* * virtio spec 1.0 says ISR bit 0 should be ignored with MSI, but * windows drivers included in virtio-win 1.8.0 (circa 2015) are * incorrectly polling this bit during crashdump and hibernation * in MSI mode, causing a hang if this bit is never updated. * Recent releases of Windows do not really shut down, but rather * log out and hibernate to make the next startup faster. Hence, * this manifested as a more serious hang during shutdown with * * Next driver release from 2016 fixed this problem, so working around it * is not a must, but it's easy to do so let's do it here. * * Note: it's safe to update ISR from any thread as it was switched * to an atomic operation. */ virtio_set_isr ( vq -> vdev , 0x1 ); event_notifier_set ( & vq -> guest_notifier ); //写vq->guest_notifier，即irqfd } QEMU写了这个irqfd后，KVM内核模块中的irqfd poll就收到一个POLL_IN事件，然后将MSIx中断自动投递给对应的LAPIC。 大致流程是：POLL_IN -> kvm_arch_set_irq_inatomic -> kvm_set_msi_irq, kvm_irq_delivery_to_apic_fast static int irqfd_wakeup ( wait_queue_entry_t * wait , unsigned mode , int sync , void * key ) { if ( flags & EPOLLIN ) { idx = srcu_read_lock ( & kvm -> irq_srcu ); do { seq = read_seqcount_begin ( & irqfd -> irq_entry_sc ); irq = irqfd -> irq_entry ; } while ( read_seqcount_retry ( & irqfd -> irq_entry_sc , seq )); /* An event has been signaled, inject an interrupt */ if ( kvm_arch_set_irq_inatomic ( & irq , kvm , KVM_USERSPACE_IRQ_SOURCE_ID , 1 , false ) == - EWOULDBLOCK ) schedule_work ( & irqfd -> inject ); srcu_read_unlock ( & kvm -> irq_srcu , idx ); } 这里还有一点没有想明白，结合代码和调试来看，virtio-blk/virtio-scsi的msi中断走irqfd机制， 但是virtio-net（不开启vhost的情况下）不走irqfd，而是直接调用virtio_notify/virtio_pci_notify，最后通过KVM的ioctl投递中断？ 从代码路径上来看，后者明显路径更长，谁知道原因告诉我一下。 Once in virtio_notify_irqfd , once in virtio_queue_guest_notifier_read . Unfortunately , for virtio - blk + MSI + KVM + old Windows drivers we need the one in virtio_notify_irqfd . For virtio - net + vhost + INTx we need the one in virtio_queue_guest_notifier_read . ，这显然更长啊。 https : // patchwork . kernel . org / patch / 9531577 / Ok，到这里virtio前后端通信机制已经明了，最后一个小节我们以virtio-net为例，梳理一下virtio中的部分核心代码流程。 3. virtio核心代码分析，以virtio-net为例 这里我们已virtio-net网卡为例，在不适用vhost的情况下（ ），网卡后端收发包都走QEMU处理。 3.1 前后端握手流程 QEM模拟PCI设备对GuestOS进行呈现，设备驱动加载的时候尝试去初始化设备。 # 先在PCI总线上调用probe设备，调用了virtio_pci_probe，然后再virtio-bus上调用virtio_dev_probe # virtio_dev_probe最后调用到virtnet_probe pci_device_probe --> local_pci_probe --> virtio_pci_probe --> register_virtio_device --> device_register --> device_add --> bus_probe_device --> device_initial_probe --> __device_attach --> bus_for_each_drv --> __device_attach_driver --> driver_probe_device --> virtio_dev_probe --> virtnet_probe 在 virtio_pci_probe里先尝试以virtio modern方式读取设备配置数据结构 ，如果失败则尝试 virio legacy方式 。 对于 virtio legacy ，我们前面提到了 virtio legacy协议规定设备的配置数据结构放在PCI BAR0里面 。 /* the PCI probing function */ int virtio_pci_legacy_probe ( struct virtio_pci_device * vp_dev ) { rc = pci_request_region ( pci_dev , 0 , \"virtio-pci-legacy\" ); //将设备的BAR0映射到物理地址空间 vp_dev -> ioaddr = pci_iomap ( pci_dev , 0 , 0 ); //获得BAR0的内核地址 } 对于 virtio modern ，通过 capability方式报告配置数据结构的位置 ，配置数据结构有 5 种类型。 int virtio_pci_modern_probe ( struct virtio_pci_device * vp_dev ) { /* check for a common config: if not, use legacy mode (bar 0). */ common = virtio_pci_find_capability ( pci_dev , VIRTIO_PCI_CAP_COMMON_CFG , IORESOURCE_IO | IORESOURCE_MEM , & vp_dev -> modern_bars ); /* If common is there, these should be too... */ isr = virtio_pci_find_capability ( pci_dev , VIRTIO_PCI_CAP_ISR_CFG , IORESOURCE_IO | IORESOURCE_MEM , & vp_dev -> modern_bars ); notify = virtio_pci_find_capability ( pci_dev , VIRTIO_PCI_CAP_NOTIFY_CFG , IORESOURCE_IO | IORESOURCE_MEM , & vp_dev -> modern_bars ); /* Device capability is only mandatory for devices that have * device-specific configuration. */ device = virtio_pci_find_capability ( pci_dev , VIRTIO_PCI_CAP_DEVICE_CFG , IORESOURCE_IO | IORESOURCE_MEM , & vp_dev -> modern_bars ); err = pci_request_selected_regions ( pci_dev , vp_dev -> modern_bars , \"virtio-pci-modern\" ); sizeof ( struct virtio_pci_common_cfg ), 4 , 0 , sizeof ( struct virtio_pci_common_cfg ), NULL ); // 将配virtio置结构所在的BAR空间MAP到内核地址空间里 vp_dev -> common = map_capability ( pci_dev , common , sizeof ( struct virtio_pci_common_cfg ), 4 , 0 , sizeof ( struct virtio_pci_common_cfg ), NULL ); ...... } 接着来到 virtio_dev_probe里面看下 ： static int virtio_dev_probe ( struct device * _d ) { /* We have a driver! */ virtio_add_status ( dev , VIRTIO_CONFIG_S_DRIVER ); // 更新status bit，这里要写配置数据结构 /* Figure out what features the device supports. */ device_features = dev -> config -> get_features ( dev ); // 查询后端支持哪些feature bits // feature set协商，取交集 err = virtio_finalize_features ( dev ); // 调用特定virtio设备的驱动程序probe，例如: virtnet_probe, virtblk_probe err = drv -> probe ( dev ); } 再看下virtnet_probe里面的一些关键的流程，这里包含了virtio-net网卡前端初始化的主要逻辑。 static int virtnet_probe ( struct virtio_device * vdev ) { // check后端是否支持多队列，并按情况创建队列 /* Allocate ourselves a network device with room for our info */ dev = alloc_etherdev_mq ( sizeof ( struct virtnet_info ), max_queue_pairs ); // 定义一个网络设备并配置一些属性，例如MAC地址 dev -> ethtool_ops = & virtnet_ethtool_ops ; SET_NETDEV_DEV ( dev , & vdev -> dev ); // 初始化virtqueue err = init_vqs ( vi ); // 注册一个网络设备 err = register_netdev ( dev ); // 写状态位DRIVER_OK，告诉后端，前端已经ready virtio_device_ready ( vdev ); // 将网卡up起来 netif_carrier_on ( dev ); } 其中关键的流程是init_vqs，在vp_find_vqs_msix流程中会尝试去申请MSIx中断，这里前面已经有分析过了。 其中，\"configuration changed\" 中断服务程序 vp_config_changed ， virtqueue队列的中断服务程序是 vp_vring_interrupt 。 init_vqs --> virtnet_find_vqs --> vi -> vdev -> config -> find_vqs --> vp_modern_find_vqs --> vp_find_vqs --> vp_find_vqs_msix static int vp_find_vqs_msix ( struct virtio_device * vdev , unsigned nvqs , struct virtqueue * vqs [], vq_callback_t * callbacks [], const char * const names [], bool per_vq_vectors , const bool * ctx , struct irq_affinity * desc ) { /* 为configuration change申请MSIx中断 */ err = vp_request_msix_vectors ( vdev , nvectors , per_vq_vectors , per_vq_vectors ? desc : NULL ); for ( i = 0 ; i < nvqs ; ++ i ) { // 创建队列 --> vring_create_virtqueue --> vring_create_virtqueue_split --> vring_alloc_queue vqs [ i ] = vp_setup_vq ( vdev , queue_idx ++ , callbacks [ i ], names [ i ], ctx ? ctx [ i ] : false , msix_vec ); // 每个队列申请一个MSIx中断 err = request_irq ( pci_irq_vector ( vp_dev -> pci_dev , msix_vec ), vring_interrupt , 0 , vp_dev -> msix_names [ msix_vec ], vqs [ i ]); } vp_setup_vq 流程再往下走就开始分配共享内存页，至此建立起共享内存通信通道。 值得注意的是一路传下来的callbacks参数其实传入了发送队列和接收队列的回调处理函数， 好家伙，从 virtnet_find_vqs 一路传递到了 __vring_new_virtqueue 中最终赋值给了 vq->vq.callback 。 static struct virtqueue * vring_create_virtqueue_split ( unsigned int index , unsigned int num , unsigned int vring_align , struct virtio_device * vdev , bool weak_barriers , bool may_reduce_num , bool context , bool ( * notify )( struct virtqueue * ) , void ( * callback )( struct virtqueue * ) , const char * name ) { /* TODO: allocate each queue chunk individually */ for ( ; num && vring_size(num, vring_align) > PAGE_SIZE; num /= 2) { // 申请物理页，地址赋值给 queue queue = vring_alloc_queue ( vdev , vring_size ( num , vring_align ) , & dma_addr , GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO ) ; } queue_size_in_bytes = vring_size ( num , vring_align ) ; vring_init ( & vring , num , queue , vring_align ) ; // 确定 descriptor table, available ring, used ring的位置。 } 我们看下如果virtqueue队列如果收到MSIx中断消息后，会调用哪个hook来处理？ irqreturn_t vring_interrupt ( int irq , void * _vq ) { struct vring_virtqueue * vq = to_vvq ( _vq ); if ( ! more_used ( vq )) { pr_debug ( \"virtqueue interrupt with no work for %p \\n \" , vq ); return IRQ_NONE ; } if ( unlikely ( vq -> broken )) return IRQ_HANDLED ; pr_debug ( \"virtqueue callback for %p (%p) \\n \" , vq , vq -> vq . callback ); if ( vq -> vq . callback ) vq -> vq . callback ( & vq -> vq ); return IRQ_HANDLED ; } EXPORT_SYMBOL_GPL ( vring_interrupt ); 不难想到中断服务程序里面会调用队列上的callback。 我们再回过头来看下 virtnet_find_vqs ，原来接受队列的回调函数是 skb_recv_done ，发送队列的回调函数是 skb_xmit_done 。 static int virtnet_find_vqs ( struct virtnet_info * vi ) { /* Allocate/initialize parameters for send/receive virtqueues */ for ( i = 0 ; i < vi->max_queue_pairs; i++) { callbacks [ rxq2vq ( i ) ] = skb_recv_done ; callbacks [ txq2vq ( i ) ] = skb_xmit_done ; } } OK，这个小节就到这里。Are you clear ?","tags":"virtualization","url":"https://kernelgo.org/virtio, virtio-net,virto-scsi.html","loc":"https://kernelgo.org/virtio, virtio-net,virto-scsi.html"},{"title":"ARMv8 Virtualization Overview","text":"摘要： ARM处理器在移动领域已经大放异彩占据了绝对优势，但在服务器领域当前主要还是X86的天下。 为了能够和X86在服务器领域展开竞争，ARM也逐渐对虚拟化扩展有了较为完善的支持。 本文的目的是介绍一下ARMv8 AArch64处理器的虚拟化扩展中的一些相关知识点， 将主要从ARM体系结构、内存虚拟化、中断虚拟化、I/O虚拟化等几个方面做一些概括总结。 本文将尽可能的在特性层面和X86做一些对比以加深我们对于ARM Virtualizaiton Extension的映像。 0. ARMv8 System Architecture 在进入正题之前先回顾一下ARMv8体系结构的一些基本概念。 ARMv8支持两种执行状态：AArch64和AArch32。 AArch64 64-bit执行状态： 提供31个64bit的通用处理器，其中X30是Procedure link register 提供一个64bit的程序寄存器PC，堆栈指针(SPs)和Exception link registers（ELRs） 提供了32个128bit的寄存器以支持SIMD矢量和标量浮点运算 定义了4个Exception Level （EL0-EL3） 支持64bit虚拟机地址(virtual address) 定义了一些PSTATE eletems来存储PE的状态 过后ELn缀来表示不同Exception Level下可以操作的系统寄存器 AArch32 32-bit执行状态： 提供了13个32bit通用寄存器，1个32bit的PC，SP和Link Register（LR） 为Hyper Mode下的异常返回值提供给了一个单一的ELR 提供32个64bit的寄存器来支持SIMD矢量和标量浮点运算支持 提供了2个指令集，A32和T32, 支持ARMv7-A Exception Mode，基于PE modes并且可以对应到ARMv8的Exception model中 使用32bit的虚拟地址 使用一个单一的CPSR来保存PE的状态 ARM 内存模型： 非对齐的内存访问将产生一个异常 限制应用程序访问指定的内存区域 程序执行中的虚拟地址将被翻译成物理地址 内存访问顺序受控 控制cache和地址翻译的结构 多个PE之间共享内存的访问同步 ARM 内存管理（ 参考ARM Address Translation ）： ARMv8-A架构支持的最大物理内存地址宽度是48bit，支持4KB、16KB、或者64KB的页面大小 使用虚拟内存管理机制，VA的最高有效位（MSB）为0时MMU使用TTBR0的转换表来翻译，VA的最高有效位为1时MMU使用TTBR1的转换表来翻译 EL2和EL3有TTBR0，但是没有TTBR1，这意味着EL2和EL3下只能使用0x0~0x0000FFFF_FFFFFFFF范围的虚拟地址空间 ------------------------------------------------------------------------- AArch64 Linux memory layout with 4 KB pages + 4 levels :: Start End Size Use 0000000000000000 0000 ffffffffffff 256 TB user ffff000000000000 ffffffffffffffff 256 TB kernel ------------------------------------------------------------------------- OK，假装我们现在的ARMv8-A已经有了一个初步的了解，下面再从几个大的维度去看下ARMv8对虚拟化是怎么支持的。 1. ARMv8 Virtualization Extension Overview ARM为了支持虚拟化扩展在CPU的运行级别上引入了Exception Level的概念，AArch64对应的Exception Level视图如下图： EL0：用户态程序的运行级别，Guest内部的App也运行在这个级别 EL1：内核的运行级别，Guest的内核也运行在这个级别 EL2：Hypervisor的运行级别，Guest在运行的过程中会触发特权指令后陷入到EL2级别，将控制权交给Hypervisor EL3：Monitor Mode，CPU在Secure World和 Normal World直接切换的时候会先进入EL3，然后发生World切换 注：当CPU的Virtualization Extension被disable的时候，软件就运行在EL0和EL1上，这时候EL1有权限访问所有的硬件。 与ARMv8不同的是，在X86为支持CPU虚拟化引入了Root Mode和None-Root Mode的概念和一套特殊的VMX指令集， 其中非根模式是Guest CPU的执行环境，根模式是Host CPU的执行环境。 根模式、非根模式与CPU的特权级别是两个完全独立的概念，二者完全正交， 也就是说非根模式下支持和根模式下一样的用户态（Ring 3）、内核态（Ring 0）特权级。 而这和ARM是不同的，ARM CPU是依靠在不同的EL之间切换来支持虚拟化模式切换。 但二者都有一个相同点:那就是ARM和X86在虚拟化模式下如果执行了敏感指令会分别退出到EL2和Root Mode之间。 同时，X86上为了更好地支持Root/Non-root Mode在内存中实现了一个叫做VMCS的数据结构， 用来保存和恢复Root/None-root模式切换过程中的寄存器信息，VMX指令集则专门用来操作VMCS数据结构。 但在RISC-style的ARM处理器上，则没有类似的实现，而是让Hypervisor软件自己来决定哪些信息需要保存和恢复， 这在一定程度上带来了一些灵活性[ Ref1 ]。 2. Memory Virtualization 在ARMv8-A上，每个tarnslation regime可能包括1个stage，也可能包括2个sate。 每个Exception Level都有自己的地址翻译机制，使用不同的页表基地址寄存器，地址翻译可以细分到stage， 大部分的EL包括一个stage的地址翻译过程， Non-Secure EL1&0包括了2个stage的地址翻译过程。 每个stage都有自己独立的一系列Translation tables，每个stage都能独立的enable或者disable。 每个stage都是将输入地址（IA）翻译成输出地址（OA）[ Ref2 ]。 所以在虚拟化场景下，ARM和X86上的方案是类似的，都是采用两阶段地址翻译实现GPA -> HPA的地址翻译过程。 虚拟机运行在None-secure EL1&0，当虚拟机内的进程访问GVA的时候MMU会将GVA翻译成IPA（intermediate physical address，中间物理地址：GPA）， 这就是所谓的stage 1地址翻译。然后MMU会再次将IPA翻译成HPA，这就是所谓的stage 2地址翻译。 在不同的Eexception Level下有不同的Address Space，那么如何去控制不同地址空间的翻译呢？ ARMv8-A上有一个TCR（Translation Control Register）寄存器来控制地址翻译。 例如：对于EL1&0来说，由于在该运行模式下VA存在2个独立的映射空间（User Space和Kernel Space）， 所以需要两套页表来完成地址翻译，这2个页表的及地址分别放在TTBR0_EL1和TTBR1_EL1中。 对于每一个地址翻译阶段: 有一个system control register bit来使能该阶段的地址翻译 有一个system control register bit来决定翻译的时候使用的大小端策略 有一个TCR寄存器来控制整个阶段的地址翻译过程 如果某个地址翻译阶段支持将VA映射到两个subranges，那么该阶段的地址翻译需要为每个VA subrange提供不同的TTBR寄存器 内存虚拟化也没有太多可以说道，理解了原理之后就可以去梳理KVM相关代码，相关代码实现主要在arch/arm/mm/mmu.c里面。 3. I/O Virtualization 设备直通的目的是能够让虚拟机直接访问到物理设备，从而提升IO性能。 在X86上使用VT-d技术就能够实现设备直通，这一切都得益于VFIO驱动和Intel IOMMU的加持。 那么在ARMv8-A上为了支持设备直通，又有哪些不同和改进呢？ 同X86上一样，ARM上的设备直通关键也是要解决DMA重映射和直通设备中断投递的问题。 但和X86上不一样的是，ARMv8-A上使用的是SMMU v3.1来处理设备的DMA重映射， 中断则是使用GICv3中断控制器来完成的，SMMUv3和GICv3在设计的时候考虑了更多跟虚拟化相关的实现， 针对虚拟化场景有一定的改进和优化。 先看下SMMUv3.1的在ARMv8-A中的使用情况以及它为ARM设备直通上做了哪些改进[ Ref3 ]。 SMMUv3规定必须实现的特性有： SMMU支持2阶段地址翻译，这和内存虚拟化场景下MMU支持2阶段地址翻译类似， 第一阶段的地址翻译被用做进程（software entity）之间的隔离或者OS内的DMA隔离， 第二阶段的地址翻译被用来做DMA重映射，即将Guest发起的DMA映射到Guest的地址空间内。 支持16bit的ASIDs 支持16bit的VMIDs 支持SMMU页表共享，允许软件选择一个已经创建好的共享SMMU页表或者创建一个私有的SMMU页表 支持49bit虚拟地址 (matching ARMv8-A's 2×48-bit translation table input sizes)，SMMUv3.1支持52bit VA，IPA，PA SMMUv3支持的可选特性有： Stage1和Stage2同时支持AArch32(LPAE: Large Page Address Extension)和AArch64地址翻译表格式（兼容性考虑） 支持Secure Stream （安全的DMA流传输） 支持SMMU TLB Invalidation广播 支持HTTU(Hardware Translation Table Update)硬件自动刷新页表的Access/Dirty标志位 支持PCIE ATS和PRI（PRI特性非常厉害，后面单独介绍） 支持16K或者64K页表粒度 我们知道，一个平台上可以有多个SMMU设备，每个SMMU设备下面可能连接着多个Endpoint， 多个设备互相之间可能不会复用同一个页表，需要加以区分，SMMU用StreamID来做这个区分， 通过StreamID去索引Stream Table中的STE（Stream Table Entry）。 同样x86上也有类似的区分机制，不同的是x86是使用Request ID来区分的，Request ID默认是PCI设备分配到的BDF号。 不过看SMMUv3 Spec，又有说明：对于PCI设备StreamID就是PCI设备的RequestID， 好吧，两个名词其实表示同一个东西，只是一个是从SMMU的角度去看就成为StreamID，从PCIe的角度去看就称之为RequestID。 同时，一个设备可能被多个进程使用，多个进程有多个页表，设备需要对其进行区分，SMMU使用SubstreamID来对其进行表示。 SubstreamID的概念和PCIe PASID是等效的，这只不过又是在ARM上的另外一种称呼而已。 SubstreamID最大支持20bit和PCIe PASID的最大宽度是一致的。 STE里面都有啥呢？Spec里面有说明： STE里面包含一个指向stage2地址翻译表的指针，并且同时还包含一个指向CD（Context Descriptor）的指针 CD是一个特定格式的数据结构，包含了指向stage1地址翻译表的基地址指针 理论上，多个设备可以关联到一个虚拟机上，所以多个STE可以共享一个stage2的翻译表。 类似的，多个设备(stream)可以共享一个stage1的配置，因此多个STE可以共享同一个CD。 Stream Table是存在内存中的一张表，在SMMU设备初始化的时候由驱动程序创建好。 Stream Table支持2种格式，Linear Stream Table 和 2-level Stream Table， Linear Stream Table就是将整个Stream Table在内存中线性展开为一个数组，优点是索引方便快捷，缺点是当平台上外设较少的时候浪费连续的内存空间。 2-level Stream Table则是将Stream Table拆成2级去索引，优点是更加节省内存。 在使能SMMU两阶段地址翻译的情况下，stage1负责将设备DMA请求发出的VA翻译为IPA并作为stage2的输入， stage2则利用stage1输出的IPA再次进行翻译得到PA，从而DMA请求正确地访问到Guest的要操作的地址空间上。 在stage1地址翻译阶段：硬件先通过StreamID索引到STE，然后用SubstreamID索引到CD， CD里面包含了stage1地址翻译（把进程的GVA/IOVA翻译成IPA）过程中需要的页表基地址信息、per-stream的配置信息以及ASID。 在stage1翻译的过程中，多个CD对应着多个stage1的地址翻译，通过Substream去确定对应的stage1地址翻译页表。 所以，Stage1地址翻译其实是一个（RequestID, PASID） => GPA的映射查找过程。 注意：只有在使能了stage1地址翻译的情况下，SubstreamID才有意义，否则该DMA请求会被丢弃。 在stage2地址翻译阶段：STE里面包含了stage2地址翻译的页表基地址（IPA->HPA）和VMID信息。 如果多个设备被直通给同一个虚拟机，那么意味着他们共享同一个stage2地址翻译页表[ Ref4 ]。 值得注意的是：CD中包含一个ASID，STE中包含了VMID，CD和VMID存在的目的是作为地址翻译过程中的TLB Tag，用来加速地址翻译的过程。 系统软件通过Command Queue和Event Queue来和SMMU打交道，这2个Queue都是循环队列。 系统软件将Command放到队列中SMMU从队列中读取命令来执行，同时设备在进行DMA传输或者配置发生错误的时候会上报事件， 这些事件就存放在Event Queue当中，系统软件要及时从Event Queue中读取事件以防止队列溢出。 SMMU支持两阶段地址翻译的目的只有1个，那就是为了支持虚拟化场景下的SVM特性（Shared Virtual Memory）。 SVM特性允许虚拟机内的进程都能够独立的访问直通给虚拟机的直通设备,在进程自己的地址空间内向设备发起DMA。 SVM使得虚拟机里面的每个进程都能够独立使用某个直通设备，这能够降低应用编程的复杂度，并提升安全性。 为了实现虚拟化场景下的SVM，QEMU需要模拟一个vSMMU（或者叫vIOMMU）的设备。 虚拟机内部进程要访问直通设备的时候，会调用Guest驱动创建PASID Table（虚拟化场景下这个表在Guest内部）， 在这个场景下PASID将作为虚拟机内进程地址空间的一个标志，设备在发起DMA请求的时候会带上PASID Prefix，这样SMMU就知道如何区分了。 创建PASID Table的时候会访问vSMMU，这样Guest就将PASID Table的地址（GPA）传给了QEMU， 然后QEMU再通过VFIO的IOCTL调用（VFIO_DEVICE_BIND_TASK）将表的信息传给SMMU， 这样SMMU就获得了Guest内部进程的PASID Table的shadow信息，它就知道该如何建立Stage1地址翻译表了。 所以，在两阶段地址翻译场景下，Guest内部DMA请求的处理步骤 Step1 : Guest驱动发起DMA请求 ，这个 DMA请求包含GVA + PASID Prefix Step2 ： DMA请求到达SMMU ， SMMU提取DMA请求中的RequestID就知道这个请求是哪个设备发来的 ，然后去 StreamTable索引对应的STE Step3 : 从对应的 STE表中查找到对应的CD ，然后用 PASID到CD中进行索引找到对应的S1 Page Table Step4 ： IOMMU进行S1 Page Table Walk ，将 GVA翻译成GPA （ IPA ）并作为 S2的输入 Step5 ： IOMMU执行S2 Page Table Walk ，将 GPA翻译成HPA ， done ！ 纵观SMMUv3，从设计上来和Intel IOMMU的设计和功能基本类似，毕竟这里没有太多可以创新的地方。 但ARM SMMUv3有2个比较有意思的改进点： 一个是支持Page Request Interface（PRI），PRI是对ATS的进一步改进。当设备支持PRI特性的时候， 设备发送DMA请求的时候可以缺页IOPF(IO Page Fault)，这就意味着直通虚拟机可以不需要进行内存预占， DMA缺页的时候SMMU会向CPU发送一个缺页请求，CPU建立好页表之后对SMMU进行回复，SMMU这时候再将内容写到DMA Buffer中。 另外一个改进就是，DMA写内存之后产生脏页可以由硬件自动更新Access/Dirty Bit， 这样就对直通设备热迁移比较友好，但这个功能是需要厂商选择性支持的， 而且在这种场景下如何解决SMMU和MMU的Cache一致性是最大的挑战。 4. Interrupt Virtualization ARM的中断系统和x86区别比较大，x86用的是IOAPIC/LAPIC中断系统，ARM则使用的是GIC中断控制器， 并且随着ARM的演进陆续出现了GICv2,GICv3,GICv4等不同版本， 看了GICv3手册感觉着玩儿设计得有点复杂，并不像x86上那样结构清晰。 GICv1和GICv2最大只支持8个PE，这放在现在显然不够用了。 所以，GICv3对这里进行改进，提出了 affinity routing 机制以支持更多的PE。 GICv3定义了以下中断类型[ Ref5 ]： ARM上的中断类型： LPI(Locality-specific Peripheral Interrupt) LPI始终是基于消息的中断，边缘触发、经过ITS路由，它们的配置保存在表中而不是寄存器，比如PCIe的MSI/MSI-x中断，GITS_TRANSLATER控制中断 SGI (Software Generated Interrupt) 软件触发的中断，软件可以通过写GICD_SGIR寄存器来触发一个中断事件，一般用于核间通信（对应x86 IPI中断） PPI(Private Peripheral Interrupt) 私有外设中断，这是每个核心私有的中断，PPI太冗长会送达到指定的CPU上，边缘触发或者电平触发、有Active转态，应用场景有CPU本地时钟，类似于x86上的LAPIC Timer Interrupt SPI(Shared Peripheral Interrupt) 公用的外部设备中断，也定义为共享中断，边缘触发或者电平触发、有Active转态，可以多个CPU或者说Core处理，不限定特定的CPU，SPI支持Message格式（GICv3），GICD_SETSPI_NSR设置中断，GICD_CLRSPI_NSR清除中断 ARM上的中断又可以分为两类： 一类中断要通过Distributor分发的，例如SPI中断。 另一类中断不通过Distributor的，例如LPI中断，直接经过ITS翻译后投递给某个Redistributor。 INTID Interrupt Type Notes 0-15 SGI Banked per PE 16-31 PPI Banked per PE 32-1019 SPI 1020-1023 Special Interrupt Number Used to signal special cases 1024-8191 Reserved 8192- LPI ARM上又搞出来一个 Affinity Routing 的概念，GICv3使用 Affinity Routing 来标志一个特定的PE或者是一组特定的PE， 有点类似于x86上的APICID/X2APIC ID机制。ARM使用4个8bit的域来表示affinity，格式如： < affinity level 3 > . < affinity level 2 > . < affinity level 1 > . < affinity level 0 > 例如，现在有个ARM Big.Little架构的移动处理器SOC，拥有2个Cluster，小核心拥有4个Cortex-A53大核心拥有2个A72，那么可以表示为： 0 . 0 . 0 .[ 0 : 3 ] Cores 0 to 3 of a Cortex - A53 processor 0 . 0 . 1 .[ 0 : 1 ] Cores 0 to 1 of a Cortex - A72 processor GICv3的设计上和x86的IOAPIC/LAPIC架构差异甚远，GICv3的设计架构如下图所示： GICv3中断控制器由Distributor，Redistributor和CPU Interface三个部分组成。 Distributor负责SPI中断管理并将中断发送给Redistributor，Redistributor管理PPI，SGI，LPI中断，并将中断投递给CPU Interface， CPU Interface负责将中断注入到Core里面（CPU Interface本身就在Core内部）。 Distributor的主要功能有： 中断优先级管理和中断分发 启用和禁用SPI 为每个SPI设置设置中断优先级 为每个SPI设置路由信息 设置每个SPI的中断触发属性：边沿触发或者电平触发 生成消息格式的SPI中断 控制SPI中断的active状态和pending状态 每个PE都对应有一个Redistributor与之相连，Distributor的寄存器是memory-mapped， 并且它的配置是全局生效的，直接影响所有的PE。Redistributor的主要功能有： 使能和禁用SGI和PPI 设置SGI和PPI的中断优先级 设置PPI的触发属性：电平触发或者边沿触发 为每个SGI和PPI分配中断组 控制SGI和PPI的状态 控制LPI中断相关数据结构的基地址 对PE的电源管理的支持 每个Redistributor都和一个CPU Interface相连， 在GICv3中CPU Interface的寄存器是是通过System registers(ICC_*ELn)来访问的。 在使用这些寄存器之前软件必须使能系统寄存器，CPU Interface的主要功能： 控制和使能CPU的中断处理。如果中断disable了，即使Distributor分发了一个中断事件到CPU Interface也会被Core屏蔽掉。 应答中断 进行中断优先级检测和中断deassert 为PE设置一个中断优先级mask标志，可以选择屏蔽中断 为PE定义抢占策略 为PE断定当前pending中断的最高优先级（优先级仲裁） GICv3中为了处理LPI中断，专门引入了ITS（Interrupt Translation Service）组件。 外设想发送LPI中断时（比如PCI设备的MSI中断），就去写ITS的寄存器GITS_TRANSLATER，这个写操作就会触发一个LPI中断。 ITS接收到LPI中断后，对其进行解析然后发送给对应的redistributor，然后再由redistributor发送给CPU Interface。 那么这个写操作里面包含了哪些内容呢？主要是2个关键域。 EventID：这个是写入到GITS_TRANSLATER的值，EventID定义了外设要触发的中断号，EventID可以和INTID一样，或者经过ITS翻译后得到一个INTID DeviceID：这个是外设的标志，实现是自定义的，例如可以使用AXI的user信号传递。 ITS使用3种类型的表来完成LPI的翻译和路由： Device Table： 将DeviceID映射到Interrupt Translation Table中 Interrupt Translation Table：包含了EventID到INTID映射关系之间和DeviceID相关的信息，同时也包含了INTID Collection Collection Table：将collections映射到Redistributor上 整个流程大概是： Step1 ： 外设写 GITS_TRANSLATER ， ITS使用DeviceID从Device Table中索引出这个外设该使用哪个Interrupt Translation Table Step2 ： 使用 EventID去选中的Interrupt Translation Table中索引出INTID和对应的Collection ID Step3 ： 使用 Collection ID从Collection Table中选择对应的Collection和路由信息 Step4 ： 把中断送给目标 Redistributor 看来看去总觉得GICv3中断控制器设计比较复杂，不如x86上那样结构清晰，目前只是理了个大概，要深入理解再到代码级熟悉还得花不少时间。 上面说了这么多，还是在将GICv3控制器的逻辑，具体QEMU/KVM上是怎么实现的还得去看代码，为了提升中断的性能， GICv3的模拟是直接放到KVM里面实现的。比如说virtio设备的MSI中断，那肯定类型上是LPI中断，QEMU模拟的时候机制上还是使用irqfd方式来实现的， 前面也有从代码角度去分析过，后面再单独从代码层级去分析具体的实现方案。 5. Overview ARM体系结构和x86存在不少差异，其中差异最大的还是中断控制器这块，这里需要投入事件好好分析一下． 内存虚拟化和I/O虚拟化这块二者可能细节上有些不同，但背后的原理还是近似的． 例如：SMMUv3在设计上和Intel IOMMU都支持了二次地址翻译，但SMMU有针对性的改进点． 后面继续努力，慢慢入门学习ARM虚拟化的知识体系． 6. References ARMv8 Architecture Reference Manual ARMv8-A Address Translation Version 1.0 ARM64 Address Translation SMMU architecture version 3.0 and version 3.1 GICv3 Software Overview Official Release SVM on ARM SMMUv3 SVM and PASID","tags":"virtualization","url":"https://kernelgo.org/armv8-virt-guide.html","loc":"https://kernelgo.org/armv8-virt-guide.html"},{"title":"Lightweight Micro Virtual Machines","text":"轻量级虚拟化技术 云计算领域经过近13年的发展后，整个云软件栈已经变得大而全了。 例如：Openstack + KVM解决方案，这套IaaS解决方案已经比较完善了。 以AWS为首的云计算服务提供商为了更细粒度的划分计算资源，提出了Serverless模型。 为了更好地服务Serverless模型，涌现了若干个轻量级虚拟化方案。 这里简单介绍一下当前已有的４种解决方案．它们分别是Firecracker, gVisor, Rust-VMM 和NEMU。 轻量级虚拟化方案的设计理念是围绕着：安全、快速、轻量、高并高密度，这几个共同点展开的。 下面对这几个轻量级虚拟机化方案进行简要的对比介绍。 AWS Firecracker Firecracker由AWS发布并将firecracker开源， 它的定位是面向Serverless计算业务场景。 Firecracker本质上是基于KVM的轻量级的microVM， 可以同时支持多租户容器和FaaS场景。 Security和Fast是firecracker的首要设计目标。 它的设计理念可以概括为： 基于KVM 精简的设备集（极简主义） 基于Rust语言（Builtin Safety） 定制的guest kernel（快速启动） 优化内存开销（使用musl c） Firecracker使用了极为精简的设备模型（仅有几个关键的模拟设备），目的是减少攻击面已提升安全性。 同时这irecracker使用了一个精简的内核（基于Apline Linux），这使得Firecracker可以做在125ms内拉起一个虚拟机。 Firecracker使用musl libc而不是gnu libc，能够将虚拟机的最低内存开销小到5MB。 注：一个Standard MicroVM的规格是1U 128M Fircracker的架构图如下： google gVisor gVisor由Google出品，目的是为了加强Container的隔离性但走的是另外一条路子。 gVisor是Container Runtime Sandbox，本质是一种进程级虚拟化技术，走的是沙箱的路子。 它的设计理念可以概括为： 基于ptrace syscall截获模拟 runsc直接对接Docker & Kubernates 不需要模拟Devices、Interrupts和IO 文件系统隔离机制 gVisor的架构如下图： gVisor由3个组件构成，Runsc、Sentry和Gopher。 Runsc向上提供Docker & Kubernates的接口(OCI)，Sentry截获syscall并进行模拟，Gopher用来做9p将文件系统呈现给沙箱内部App。 gVisor当前的设计模式中带来的问题有： 不适用于intensive syscall场景 对sysfs、procfs支持不完整 目前仅支持240个syscall的模拟 对ARM架构支持还不友好 对KVM的支持尚处于experimental阶段 更多的信息可以参考： https://lwn.net/Articles/754433/ Kubecon gVisor Talk Rust-VMM Rust-VMM是一个新项目，目前还处于开发阶段。它的愿景是帮助社区构建自定义的VMM和Hypervisor。 使得我们可以能够像搭积木一样去构建一个适用于我们自己应用场景的VMM，而不用去重复造轮子。 社区的参与者包括了Alibaba, AWS, Cloud Base, Crowdstrike, Intel, Google, Red Hat等。 它的设计理念是： 合并CrosVM项目和Firecracker项目 提供更安全、更精简的代码集（Build with Rust Language） 采用基于Rust的crates组件开发模式 高度可定制 项目目前在开发阶段，更多的信息见： https://github.com/rust-vmm NEMU NEMU项目由Intel领导开发，重点是面向IaaS云化场景。 NEMU是基于QEMU 4.0进行开发的，有可能会被QEMU社区接收到upstream中。 NEMU的涉及理念可以概括为： 基于QEMU/KVM 精简的设备集（no legacy devices) 新的virt类型主板 优化内存开销 定制的Guest OS(基于Clear Linux） 编译时可裁剪 个人认为NEMU项目如果用来面向Serverless场景是不够精简的， 它的完备设备集还是很大（包含约20种Moderm设备）。 NEMU选择支持seabios和UEFI目的就是要兼容IaaS方案，所以它的代码集还是很大的。 NEMU的做法更像是对QEMU做了一个减法，让QEMU更加聚焦X86和ARM的云OS场景。 NEMU的好处是原生支持设备直通、热迁移等高级特性。","tags":"virtualization","url":"https://kernelgo.org/microVM.html","loc":"https://kernelgo.org/microVM.html"},{"title":"The Rust Programming Language","text":"1.The Rust Programming Language Rust is a multi-paradigm system programming language[1] focused on safety, especially safe concurrency. Rust is syntactically similar to C++, but is designed to provide better memory safety while maintaining high performance. 2.Rust Lang Guide https://doc.rust-lang.org/book/ https://doc.rust-lang.org/std/index.html#the-rust-standard-library 3.Rust Lang Feature Ownership Rules: First, let's take a look at the ownership rules. Keep these rules in mind as we work through the examples that illustrate them: Each value in Rust has a variable that's called its owner. There can only be one owner at a time. When the owner goes out of scope, the value will be dropped. Mutable Reference: you can have only one mutable reference to a particular piece of data in a particular scope. Reference Rules: At any given time, you can have either one mutable reference or any number of immutable references. References must always be valid. Lifetime Elision Rules: Each parameter that is a reference gets its own lifetime parameter. If there is exactly one input lifetime parameter, that lifetime is assigned to all output lifetime parameters. If there are multiplue input lifetime parameters, but one of them is &self or &mut self because this is a method, the lifetime of of self is assigned to all output lifetime parameters. Closure (Lambda Expressions) Rules: using || instead of () around input variables. optional body delimination ({}) for a single expression (mandatory otherwise). the ability to capture the outer environment variables. Ref Rust Programming Language (wikipedia)","tags":"Programe Language","url":"https://kernelgo.org/rust-lang.html","loc":"https://kernelgo.org/rust-lang.html"},{"title":"Article Archive 2019 Reading Plan","text":"计划阅读的文章 mmap详解-1 网络高并发服务器之epoll接口 ARM v8 Architecture: pptx ARM v8A Architecture Reference Manual: pdf VFIO mdev Userspace NVME Driver In QEMU The Raft Consensus Algorithm Linux Page Cache 机制 ARM SMMU Architecture Spec Share Virtual Memory PCI SIG PASID ARM® Generic Interrupt Controller Architecture Specification ARM Virtualization: Performance and Architectural Implications Writing an OS in Rust CPU Cache Coherency Understanding virtualization facilities in the ARMv8 processor architecture ACPI 6.3 Spec ARM CoreLink CCI550 总线协议 GICv3 Software Overview Memory Hot-Add and Hot-remove 已阅读的文章 浅谈可重入函数和不可重入函数 Rust编程之道-张汉东-电子工业出版社 NETLINK_KOBJECT_UEVENT侦听设备热插拔事件的原理 Sandboxing: Seccomp & AppAmour Mastering the DMA and IOMMU","tags":"utils","url":"https://kernelgo.org/reading2019.html","loc":"https://kernelgo.org/reading2019.html"},{"title":"Virt NMI Emulation","text":"X86 NMI中断 NMI（Nonmaskable Interrupt）中断之所以称之为NMI的原因是：这种类型的中断不能被CPU的EFLAGS寄存器的IF标志位所屏蔽。 而对于可屏蔽中断而言只要IF标志位被清理（例如：CPU执行了cli指令），那么处理器就会禁止INTR Pin和Local APIC上接收到的内部中断请求。 NMI中断有两种触发方式： 外部硬件通过CPU的 NMI Pin 去触发（硬件触发） 软件向CPU系统总线上投递一个NMI类型中断（软件触发） 当CPU从上述两种中断源接收到NMI中断后就立刻调用vector=2（中断向量为2）的中断处理函数来处理NMI中断。 Intel SDM, Volume 3, Chapter 6.7 Nonmaskable Interrupt章节指出： 当一个NMI中断处理函数正在执行的时候，处理器会block后续的NMI直到中断处理函数执行IRET返回。 1.NMI中断的用途 NMI中断的主要用途有两个： 用来告知操作系统有硬件错误（Hardware Failure） 用来做看门狗定时器，检测CPU死锁 除了用来，看门狗定时器在Linux内核中被用来进行死锁检测（Hard Lockup），当CPU长时间不喂狗的时候会触发看门狗超时， 这时候向操作系统注入NMI中断，告知系统异常。 2.NMI中断虚拟化 我们可以通过virsh inject-nmi VMname命令给虚拟机注入NMI中断。 QEMU这边的调用栈为： qmp_inject_nmi => nmi_monitor_handle => nmi_children //传入了struct do_nmi_s ns => do_nmi => nc -> nmi_monitor_handler => x86_nmi => apic_deliver_nmi => kvm_apic_external_nmi => do_inject_external_nmi => kvm_vcpu_ioctl ( cpu , KVM_NMI ) 其中 nmi_children 的设计比较特别，它调用了一个object_child_foreach函数， 会沿着QOM对象树往下遍历，遍历的时候调用 do_nmi 函数。 值得注意的是这里 NMIClass 被设计为一个 interface 类型，而主板类 MachineClass 实现了这个接口。 static const TypeInfo pc_machine_info = { . name = TYPE_PC_MACHINE , . parent = TYPE_MACHINE , . abstract = true , . instance_size = sizeof ( PCMachineState ), . instance_init = pc_machine_initfn , . class_size = sizeof ( PCMachineClass ), . class_init = pc_machine_class_init , . interfaces = ( InterfaceInfo []) { { TYPE_HOTPLUG_HANDLER }, { TYPE_NMI }, { } }, }; 主板类是一个抽象类，实现了 TYPE_HOTPLUG_HANDLER 和 TYPE_NMI 接口，有点Java面向对象的意思。 static int do_nmi ( Object * o , void * opaque ) { struct do_nmi_s * ns = opaque ; NMIState * n = ( NMIState * ) object_dynamic_cast ( o , TYPE_NMI ) ; // 对象动态转换 if ( n ) { // 如果能够成功转换，说明这个对象实现了 NMI 接口，那么可以调用这个对象的处理函数 NMIClass * nc = NMI_GET_CLASS ( n ) ; ns -> handled = true ; nc -> nmi_monitor_handler ( n , ns -> cpu_index , & ns -> err ) ; // nmi_monitor_handler 是NMI接口的方法 if ( ns -> err ) { return - 1 ; } } nmi_children ( o , ns ) ; return 0 ; } nmi_monitor_handle 函数中调用了nmi_children(object_get_root(), &ns)，从Root Object对象开始向下遍历， 在对象上调用 do_nmi 方法，而 do_nmi 里面会检测这个对象是否实现了 TYPE_NMI 类型的接口， 如果这个对象实现了这个接口，那么调用 mi_monitor_handler 方法来发送NMI中断。这里充分体现了QOM面向对象思想。 在看代码的时候，我们可以找到 pc_machine_class_init 里面注册了 mi_monitor_handler 。 这里还不太理解的是x86_nmi里面会遍历所有的CPU，对每个CPU都注了NMI，有这个必要吗？ static void pc_machine_class_init ( ObjectClass * oc , void * data ) { NMIClass * nc = NMI_CLASS ( oc ); // 把对象转换为NMIClass类型对象 nc -> nmi_monitor_handler = x86_nmi ; // 实现接口方法 } QEMU调用完kvm_vcpu_ioctl(cpu, KVM_NMI)之后就开始进入KVM内核进行NMI中断注入， 毕竟LAPIC和IOAPIC现在都放到KVM模拟来提升中断注入的实时性。 KVM x86 . c kvm_arch_vcpu_ioctl => kvm_vcpu_ioctl_nmi => kvm_inject_nmi kvm_inject_nmi 里面将nmi_queued加1，然后make KVM_REQ_NMI request。 为了防止中断嵌套KVM做了一些额外的处理。 void kvm_inject_nmi ( struct kvm_vcpu * vcpu ) { atomic_inc ( & vcpu -> arch . nmi_queued ); kvm_make_request ( KVM_REQ_NMI , vcpu ); } 这样VCPU在下次VM Exit的时候会check标志位，进行NMI注入。 static int vcpu_enter_guest ( struct kvm_vcpu * vcpu ) { if ( kvm_check_request ( KVM_REQ_NMI , vcpu )) process_nmi ( vcpu ); } // 由于NMI中断不能嵌套，这里做了防呆，第一process_nmi的时候limit=2， static void process_nmi ( struct kvm_vcpu * vcpu ) { unsigned limit = 2 ; /* * x86 is limited to one NMI running, and one NMI pending after it. * If an NMI is already in progress, limit further NMIs to just one. * Otherwise, allow two (and we'll inject the first one immediately). */ if ( kvm_x86_ops -> get_nmi_mask ( vcpu ) || vcpu -> arch . nmi_injected ) limit = 1 ; vcpu -> arch . nmi_pending += atomic_xchg ( & vcpu -> arch . nmi_queued , 0 ); vcpu -> arch . nmi_pending = min ( vcpu -> arch . nmi_pending , limit ); kvm_make_request ( KVM_REQ_EVENT , vcpu ); } static int inject_pending_event ( struct kvm_vcpu * vcpu , bool req_int_win ) { kvm_x86_ops -> set_nmi ( vcpu ); // call vmx_inject_nmi } 最后调用 vmx_inject_nmi 函数注入NMI中断给虚拟机（也是通过写VMCS VM_ENTRY_INTR_INFO_FIELD域来实现）。 3.参考文献 Intel SDM Volume 3, Chapter 6","tags":"virtualization","url":"https://kernelgo.org/x86-nmi.html","loc":"https://kernelgo.org/x86-nmi.html"},{"title":"VIM8 Customized Configuration","text":"是时候秀一下我的VIM8自定义配置了！ 我个人是位忠实的vim用户，在浏览开源软件源代码的时候和在写作代码的时候我大部分时间都在使用vim， 它的很多优点使我很享受软件开发的乐趣，我很喜欢折腾一个自己喜欢的vim配置文件然后用它来工作． VIM8已经发布一两年了，已逐渐趋于稳定，所以是时候舍弃VIM7转投功能强大的VIM8了. VIM8最强大的地方是给我们带来了期待已久的异步任务机制． 异步任务机制的强大之处在于它允许各种插件创建异步任务而不会阻塞当前的编辑， 例如:允许ctags再后台为我们生成和更新符号表，允许ale再后台自动给我们编辑的文件做语法校验。 由于这些过程是异步的，用户感知不到。 折腾了一天之后我终于配置好了一个我心仪的版本，是时候秀一下我的VIM8配置文件了! 1.安装vim8 我们选择从源码安装vim8，因为我们需要让vim8支持python解释器和ruby解释器． git clone https://github.com/vim/vim.git cd vim ./configure --with-features = huge \\ --enable-multibyte \\ --enable-rubyinterp = yes \\ --enable-pythoninterp = yes \\ --with-python-config-dir = /usr/lib/python2.7/config \\ --enable-python3interp = yes \\ --with-python3-config-dir = /usr/lib/python3.5/config \\ --enable-luainterp = yes \\ --enable-gui = gtk2 \\ --with-ruby-command = $( which ruby ) \\ --enable-cscope make -j && sudo make install 2.配置vim8前的准备工作 配置VIM8之前，我们需要先安装一下新的插件管理器 plug.vim ． curl -fLo ~/.vim/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 或者 mkdir -pv ~/.vim/autoload wget https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim -O ~/.vim/autoload/plug.vim 安装 GNU Global和 Universal ctags git clone https : // github . com / universal - ctags / ctags . git cd ctags sh autogen . sh . / configure make - j sudo make install wget http : // tamacom . com / global / global - 6 . 6 . 3 . tar . gz --no-check-certificate tar xf global - 6 . 6 . 3 . tar . gz cd global - 6 . 6 . 3 sed - i \"s/(int i = 0;/(i = 0;/g\" gtags - cscope / find . c sed - i \"/regex_t reg;/a\\int i;\" gtags - cscope / find . c . / configure make - j sudo make install pip install pygments # 这个一定要安装！！！ 3.配置vim8的插件系统 先配置一下 plug.vim ，然后让 plug.vim 帮助我们自动管理插件，编辑~/.vimrc文件． \" Specify a directory for plugins \" - For Neovim: ~/.local/share/nvim/plugged \" - Avoid using standard Vim directory names like 'plugin' call plug # begin ( ' ~/.vim/plugged ' ) \" All of your Plugs must be added before the following line call plug # end () \" required plug#begin到plug#end之间的内容是受 plug.vim 管理的，我们大部分的配置文件都放在这里． 下面开始安装插件： if exists ( '&colorcolumn' ) set colorcolumn = 80 endif set paste syntax on \" syntax highlight set nu set nocompatible \" be iMproved , required filetype off \" required set t_Co=256 set backspace=indent,eol,start call plug#begin('~/.vim/plugged') \" basic plug Plug 'tpope/vim-fugitive' \" git support Plug 'vim-scripts/L9' Plug 'rstacruz/sparkup', {'rtp': 'vim/'} \" utils Plug 'vim-scripts/DrawIt' Plug 'mbriggs/mark.vim' Plug 'vim-scripts/tabbar' Plug 'wesleyche/Trinity' Plug 'vim-scripts/Smart-Tabs' Plug 'nvie/vim-togglemouse' Plug 'tpope/vim-unimpaired' Plug 'scrooloose/syntastic' Plug 'bronson/vim-trailing-whitespace' Plug 'tpope/vim-surround' Plug 'junegunn/vim-easy-align' Plug 'Lokaltog/vim-easymotion' Plug 'Yggdroot/indentLine' Plug 'itchyny/lightline.vim' \" YCM Plug 'Valloric/YouCompleteMe' \" file lookup Plug 'vim-scripts/command-t' Plug 'Yggdroot/LeaderF' , { 'do': './install.sh' } \" async grama check \" Plug 'w0rp/ale' Plug 'skywind3000/asyncrun.vim' \" language specific enhance Plug 'vim-scripts/c.vim' Plug 'vim-scripts/a.vim' Plug 'octol/vim-cpp-enhanced-highlight' Plug 'jnwhiteh/vim-golang' Plug 'rust-lang/rust.vim' Plug 'pangloss/vim-javascript' \" color \"Plug 'sunuslee/vim-plugin-random-colorscheme-picker' Plug 'altercation/vim-colors-solarized' Plug 'crusoexia/vim-monokai' Plug 'flazz/vim-colorschemes' \" vim colorschemes Plug 'rafi/awesome-vim-colorschemes' \" vim colorschemes Plug 'lifepillar/vim-solarized8' \" solarized8 \" gtags and gnu global support Plug 'vim-scripts/gtags.vim' Plug 'vim-scripts/autopreview' Plug 'vim-scripts/genutils' Plug 'ludovicchabant/vim-gutentags' Plug 'skywind3000/gutentags_plus' \" gutentags config set cscopeprg='gtags-cscope' set tags=./.tags ;. tags let $ GTAGSLABEL = 'native' let $ GTAGSCONF = '/usr/local/share/gtags/gtags.conf' let g : gutentags_project_root = [ '.git' , '.root' , '.svn' , '.hg' , '.project' ] let g : gutentags_ctags_tagfile = '.tags' let g : gutentags_modules = [] if executable ( 'gtags-cscope' ) && executable ( 'gtags' ) let g : gutentags_modules += [ 'gtags_cscope' ] endif if executable ( 'ctags' ) let g : gutentags_modules += [ 'ctags' ] endif let g : gutentags_cache_dir = expand ( '~/.cache/tags' ) let g : gutentags_ctags_extra_args = [] let g : gutentags_ctags_extra_args = [ '--fields=+niazS' , '--extra=+q' ] let g : gutentags_ctags_extra_args += [ '--c++-kinds=+px' ] let g : gutentags_ctags_extra_args += [ '--c-kinds=+px' ] let g : gutentags_ctags_extra_args += [ '--output-format=e-ctags' ] let g : gutentags_auto_add_gtags_cscope = 0 let g : gutentags_plus_switch = 1 let g : asyncrun_bell = 1 let g : gutentags_define_advanced_commands = 1 let g : gutentags_generate_on_empty_buffer = 1 \" open database \" let g : gutentags_trace = 1 Plug 'skywind3000/vim-preview' \"press shift + p to Preview, press p to close autocmd FileType qf nnoremap <silent><buffer> p :PreviewQuickfix<cr> autocmd FileType qf nnoremap <silent><buffer> P :PreviewClose<cr> noremap <Leader>u :PreviewScroll -1<cr> \" pageup noremap < leader > d : PreviewScroll + 1 < cr > \" pagedown noremap <silent> <leader>gs :GscopeFind s <C-R><C-W><cr> noremap <silent> <leader>gg :GscopeFind g <C-R><C-W><cr> noremap <silent> <leader>gc :GscopeFind c <C-R><C-W><cr> noremap <silent> <leader>gt :GscopeFind t <C-R><C-W><cr> noremap <silent> <leader>ge :GscopeFind e <C-R><C-W><cr> noremap <silent> <leader>gf :GscopeFind f <C-R>=expand(\" < cfile> \")<cr><cr> noremap <silent> <leader>gi :GscopeFind i <C-R>=expand(\" < cfile> \")<cr><cr> noremap <silent> <leader>gd :GscopeFind d <C-R><C-W><cr> noremap <silent> <leader>ga :GscopeFind a <C-R><C-W><cr> \" LeaderF let g : Lf_ShortcutF = '<c-p>' noremap < Leader > ff : LeaderfFunction < cr > noremap < Leader > fb : LeaderfBuffer < cr > noremap < Leader > ft : LeaderfTag < cr > noremap < Leader > fm : LeaderfMru < cr > noremap < Leader > fl : LeaderfLine < cr > let g : Lf_StlSeparator = { 'left': '' , 'right': '' , 'font': '' } let g : Lf_RootMarkers = [ '.project' , '.root' , '.svn' , '.git' ] let g : Lf_WorkingDirectoryMode = 'Ac' let g : Lf_WindowHeight = 0.30 let g : Lf_CacheDirectory = expand ( '~/.vim/cache' ) let g : Lf_ShowRelativePath = 0 let g : Lf_HideHelp = 1 let g : Lf_StlColorscheme = 'powerline' let g : Lf_PreviewResult = { 'Function': 0 , 'BufTag': 0 } let g : Lf_NormalMap = { \\ \"File\" : [[ \"<ESC>\" , ':exec g:Lf_py \"fileExplManager.quit()\"<CR>' ]], \\ \"Buffer\" : [[ \"<ESC>\" , ':exec g:Lf_py \"bufExplManager.quit()\"<CR>' ]], \\ \"Mru\" : [[ \"<ESC>\" , ':exec g:Lf_py \"mruExplManager.quit()\"<CR>' ]], \\ \"Tag\" : [[ \"<ESC>\" , ':exec g:Lf_py \"tagExplManager.quit()\"<CR>' ]], \\ \"Function\" : [[ \"<ESC>\" , ':exec g:Lf_py \"functionExplManager.quit()\"<CR>' ]], \\ \"Colorscheme\" : [[ \"<ESC>\" , ':exec g:Lf_py \"colorschemeExplManager.quit()\"<CR>' ]], \\ } \" latex support Plug 'lervag/vimtex' let g:tex_flavor='latex' let g:vimtex_view_method='zathura' let g:vimtex_quickfix_mode=0 set conceallevel=1 let g:tex_conceal='abdmg' \" UltiSnips Plug 'sirver/ultisnips' let g : UltiSnipsExpandTrigger = '<tab>' let g : UltiSnipsJumpForwardTrigger = '<tab>' let g : UltiSnipsJumpBackwardTrigger = '<s-tab>' \" All of your Plugs must be added before the following line call plug#end() \" required filetype plugin indent on \" required \" To ignore plugin indent changes , instead use : \"filetype plugin on \" Put your non - Plug stuff after this line set tabstop = 8 set softtabstop = 8 set shiftwidth = 4 \"set expandtab set hls set encoding=utf-8 set listchars=tab:>-,trail:- \" set F5 , F6 to find function and symbol nnoremap < F5 > : GscopeFind gs nnoremap < F9 > : GscopeFind gg nnoremap < F4 > :ccl < CR > nnoremap < F2 > :let g : gutentags_trace = 1 < CR > nnoremap < F3 > :let g : gutentags_trace = 0 < CR > \" color desert color Tomorrow - Night - Bright 更新配置文件之后调用 vim +PlugInstall +qall 安装全部插件，执行这个命令Plug会自动下载全部的插件到vim插件目录下。 vim +PlugInstall +qall 这里最重要的一组插件是 vim-gutentags.vim 和 gutentags_plus.vim ， 通过这两个插件配合安装GNU Global和 universal ctags工具， 可以为我们自动异步生成项目的符号表， 可以很方便地查找符号定义和调用关系， 再也不用我们去手动生成和维护tags更新了． 具体配置方式参考: https://zhuanlan.zhihu.com/p/36279445 YCM插件需要自己去编译一下，具体步骤见： https://github.com/ycm-core/YouCompleteMe#installation 其他插件的搭配也非常合理和高效，具体的使用方式参考该插件的help doc. 不早了，洗洗睡了！","tags":"utils","url":"https://kernelgo.org/vim8.html","loc":"https://kernelgo.org/vim8.html"},{"title":"VT-d Interrupt Posting Code Analysis","text":"VT-d Posted Interrupt 代码分析 Posted Interrupt是基于Interrupt Remapping机制实现的，关于VT-d Posted Interrupt的原理可以参考 VT-d Posted Interrupt ，建议先了解原理再来看代码分析。 分析VT-d Posted Interrupt代码的代码需要从vCPU调度入手，为了实现中断的直接投递和中断迁移， 在vCPU调度时候VMM需要为Posted Interrupt做一些额外的工作，但这些额外的工作带来的中断实时性提升是可观的。 per-vCPU Posted Interrupt Descriptor 为了支持VT-d Posted Interrup Inter为vCPU引入了Posted Interrupt Descriptor数据结构，其中有pir，on,sn,nv,ndst等几个关键域。 PIR域记录了要给虚拟机vCPU投递的vector号（由硬件自动写入并由VMM软件读取）， 当中断到来时ON标志位自动置位告知guest我有中断要投递给你了， SN标志位是VMM软件用来告知VT-d硬件当前vCPU不在Running状态你不要给我投中断了我收不到， NV是主机上配合Poste Interrupt工作的一个中断vector（它的值只能是wakeup_vector或者notification vector）， NDST存放当前vCPU所在PCPU的apicid（由VMM负责刷新，确保中断可以自动迁移到目的pCPU上）。 /* Posted-Interrupt Descriptor */ struct pi_desc { u32 pir [ 8 ]; /* Posted interrupt requested */ union { struct { /* bit 256 - Outstanding Notification */ u16 on : 1 , /* bit 257 - Suppress Notification */ sn : 1 , /* bit 271:258 - Reserved */ rsvd_1 : 14 ; /* bit 279:272 - Notification Vector */ u8 nv ; /* bit 287:280 - Reserved */ u8 rsvd_2 ; /* bit 319:288 - Notification Destination */ u32 ndst ; }; u64 control ; }; u32 rsvd [ 6 ]; } __aligned ( 64 ); 首先要明确pi_desc是percpu的，所以在struct vcpu_vmx里面会包含一个pi_desc数据结构。 struct vcpu_vmx { /* Posted interrupt descriptor */ struct pi_desc pi_desc ; } vCPU创建的时候会将NV置成POSTED_INTR_VECTOR也就是notification event的中断号，同时把SN置1（因为这时候vCPU还没有运行）。 kvm_vm_ioctl_create_vcpu => kvm_arch_vcpu_create => vmx_vcpu_create，这里会注册vCPU的preempt notifier， 当调度器选中vCPU线程的时候VMM会收到通知，VMM调用回调函数进行处理。 static struct kvm_vcpu * vmx_create_vcpu ( struct kvm * kvm , unsigned int id ) { preempt_notifier_init ( & vcpu -> preempt_notifier , & kvm_preempt_ops ); #注册 vcpu的preempt notifier /* * Enforce invariant: pi_desc.nv is always either POSTED_INTR_VECTOR * or POSTED_INTR_WAKEUP_VECTOR. */ vmx -> pi_desc . nv = POSTED_INTR_VECTOR ; vmx -> pi_desc . sn = 1 ; } 同时kvm_vm_ioctl_create_vcpu => kvm_arch_vcpu_setup => vcpu_load, vcpu_put会对pi_desc做一些修改，后面结合虚拟机vCPU调度进行代码分析。 vCPU调度与VT-d Posted Interrupt vCPU的运行状态主要有3种： Running 状态：vCPU正处于非根模式下运行 Runnable 状态：vCPU线程被抢占或者时间片到期，等待OS的下一次调度 Blocked 状态： vCPU执行hlt指令后从非根模式block出来准备休眠的状态 vCPU调度就是指在VMM的管理下虚拟机的vCPU线程在这几种状态之间切换的场景， 针对不同的状态转变VMM会干预进来为Posted Interrupt做一些额外的工作以确保中断自动迁移可以顺利进行。 vCPU 从 Runnable => Running 当vCPU被调度器选中运行之前会调用VMM的回调函数，在kvm中这个函数时kvm_sched_in。 static void kvm_sched_in ( struct preempt_notifier * pn , int cpu ) { struct kvm_vcpu * vcpu = preempt_notifier_to_vcpu ( pn ); if ( vcpu -> preempted ) vcpu -> preempted = false ; #将 vcpu被抢占的标志位清零 kvm_arch_sched_in ( vcpu , cpu ); #调整一下 ple window kvm_arch_vcpu_load ( vcpu , cpu ); #将 VMCS加载到pCPU上准备运行了 （这里可能是调度到其他 pCPU上运行 ，也可能是继续在原来 pCPU上运行 ） } kvm_sched_in => kvm_arch_vcpu_load => vmx_vcpu_load => vmx_vcpu_pi_load，vCPU要从Runnable状态切换到Running状态了， 这时候要:刷新NDST为vCPU要运行到的pCPU的apic id，并设置SN=0（告知硬件我现在可以接收Posted Interrupt了）。 static void vmx_vcpu_pi_load ( struct kvm_vcpu * vcpu , int cpu ) { struct pi_desc * pi_desc = vcpu_to_pi_desc ( vcpu ); struct pi_desc old , new ; unsigned int dest ; /* * In case of hot-plug or hot-unplug, we may have to undo * vmx_vcpu_pi_put even if there is no assigned device. And we * always keep PI.NDST up to date for simplicity: it makes the * code easier, and CPU migration is not a fast path. */ if ( ! pi_test_sn ( pi_desc ) && vcpu -> cpu == cpu ) return ; /* * First handle the simple case where no cmpxchg is necessary; just * allow posting non-urgent interrupts. * * If the 'nv' field is POSTED_INTR_WAKEUP_VECTOR, do not change * PI.NDST: pi_post_block will do it for us and the wakeup_handler * expects the VCPU to be on the blocked_vcpu_list that matches * PI.NDST. */ if ( pi_desc -> nv == POSTED_INTR_WAKEUP_VECTOR || vcpu -> cpu == cpu ) { pi_clear_sn ( pi_desc ); return ; } /* The full case. */ do { old . control = new . control = pi_desc -> control ; dest = cpu_physical_id ( cpu ); if ( x2apic_enabled ()) new . ndst = dest ; else new . ndst = ( dest << 8 ) & 0xFF00 ; new . sn = 0 ; } while ( cmpxchg64 ( & pi_desc -> control , old . control , new . control ) != old . control ); } vCPU 从 Running => Runnable 当vCPU被抢占或者时间片到期的时候vCPU被调度出来，这时候会触发回调函数kvm_sched_out。 static void kvm_sched_out ( struct preempt_notifier * pn , struct task_struct * next ) { struct kvm_vcpu * vcpu = preempt_notifier_to_vcpu ( pn ); if ( current -> state == TASK_RUNNING ) vcpu -> preempted = true ; #置上 vcpu被抢占标志位 kvm_arch_vcpu_put ( vcpu ); #将 vCPU的VMCS从当前pCPU上拿下来 ，并且保存一下 vCPU的相关信息到VMCS中 } kvm_sched_out => vmx_vcpu_put => vmx_vcpu_pi_put，这里vCPU要被调度出来的，那么要把SN bit置位，告诉硬件我不在运行了， 先别给我投递中断。 static void vmx_vcpu_pi_put ( struct kvm_vcpu * vcpu ) { struct pi_desc * pi_desc = vcpu_to_pi_desc ( vcpu ); if ( ! kvm_arch_has_assigned_device ( vcpu -> kvm ) || ! irq_remapping_cap ( IRQ_POSTING_CAP ) || ! kvm_vcpu_apicv_active ( vcpu )) return ; /* Set SN when the vCPU is preempted */ if ( vcpu -> preempted ) pi_set_sn ( pi_desc ); # set SN bit here } vCPU 从 Running => Blocked 当vCPU在Running状态下非根模式执行hlt指令后会被VMM截获发生VM Exit（肯定不能让vCPU在非根模式下中止，这样会浪费CPU资源）， 这时候会调用vcpu_block函数来处理。 static inline int vcpu_block ( struct kvm * kvm , struct kvm_vcpu * vcpu ) { if ( ! kvm_arch_vcpu_runnable ( vcpu ) && ( ! kvm_x86_ops -> pre_block || kvm_x86_ops -> pre_block ( vcpu ) == 0 )) { srcu_read_unlock ( & kvm -> srcu , vcpu -> srcu_idx ); kvm_vcpu_block ( vcpu ); vcpu -> srcu_idx = srcu_read_lock ( & kvm -> srcu ); if ( kvm_x86_ops -> post_block ) kvm_x86_ops -> post_block ( vcpu ); if ( ! kvm_check_request ( KVM_REQ_UNHALT , vcpu )) return 1 ; } kvm_apic_accept_events ( vcpu ); switch ( vcpu -> arch . mp_state ) { case KVM_MP_STATE_HALTED : vcpu -> arch . pv . pv_unhalted = false ; vcpu -> arch . mp_state = KVM_MP_STATE_RUNNABLE ; case KVM_MP_STATE_RUNNABLE : vcpu -> arch . apf . halted = false ; break ; case KVM_MP_STATE_INIT_RECEIVED : break ; default : return - EINTR ; break ; } return 1 ; } vcpu_block细分为3个阶段Pre Block, Block 和 Post Block。Pre Block阶段会调用pi_pre_block， 这里会将vCPU添加到一个per pCPU的等待链表上，该链表记录了所有在这个pCPU上休眠的vCPU列表，然后更新NDST域。 static int pi_pre_block ( struct kvm_vcpu * vcpu ) { unsigned int dest ; struct pi_desc old , new ; struct pi_desc * pi_desc = vcpu_to_pi_desc ( vcpu ); # 虚拟机没有配置直通设备 || 不支持Posted Interrupt => 直接返回 if ( ! kvm_arch_has_assigned_device ( vcpu -> kvm ) || ! irq_remapping_cap ( IRQ_POSTING_CAP ) || ! kvm_vcpu_apicv_active ( vcpu )) return 0 ; # 关中断， 将当前vCPU线程加入到上次运行的pCPU的等待列表中 WARN_ON ( irqs_disabled ()); local_irq_disable (); if ( ! WARN_ON_ONCE ( vcpu -> pre_pcpu != - 1 )) { vcpu -> pre_pcpu = vcpu -> cpu ; spin_lock ( & per_cpu ( blocked_vcpu_on_cpu_lock , vcpu -> pre_pcpu )); list_add_tail ( & vcpu -> blocked_vcpu_list , & per_cpu ( blocked_vcpu_on_cpu , vcpu -> pre_pcpu )); spin_unlock ( & per_cpu ( blocked_vcpu_on_cpu_lock , vcpu -> pre_pcpu )); } #刷新NDST，更新NV为wakeup vector do { old . control = new . control = pi_desc -> control ; WARN (( pi_desc -> sn == 1 ), \"Warning: SN field of posted-interrupts \" \"is set before blocking \\n \" ); /* * Since vCPU can be preempted during this process, * vcpu->cpu could be different with pre_pcpu, we * need to set pre_pcpu as the destination of wakeup * notification event, then we can find the right vCPU * to wakeup in wakeup handler if interrupts happen * when the vCPU is in blocked state. */ dest = cpu_physical_id ( vcpu -> pre_pcpu ); if ( x2apic_enabled ()) new . ndst = dest ; else new . ndst = ( dest << 8 ) & 0xFF00 ; /* set 'NV' to 'wakeup vector' */ new . nv = POSTED_INTR_WAKEUP_VECTOR ; } while ( cmpxchg64 ( & pi_desc -> control , old . control , new . control ) != old . control ); #如果在pre block阶段收到了中断，那么就不block了，直接转导Runnable状态去 /* We should not block the vCPU if an interrupt is posted for it. */ if ( pi_test_on ( pi_desc ) == 1 ) __pi_post_block ( vcpu ); local_irq_enable (); return ( vcpu -> pre_pcpu == - 1 ); } Pre Block阶段过后会调用kvm_vcpu_block，在这个函数中会调用schdule()主动把vCPU调度出去（休眠），让出pCPU执行其他vCPU的代码。 vCPU 从 Blocked => Runnable 当vCPU休眠结束之后会调用vmx_post_block => __pi_post_block这时候vCPU结束睡眠被重新调度。 注意这里会更新NDST并将vCPU从pCPU等待链表上删除，并且把NV置位POSTED_INTR_VECTOR。 static void __pi_post_block ( struct kvm_vcpu * vcpu ) { struct pi_desc * pi_desc = vcpu_to_pi_desc ( vcpu ); struct pi_desc old , new ; unsigned int dest ; #再度更新NDST，因为block睡眠之后被再调度出来执行的时候可能换了pCPU！ do { old . control = new . control = pi_desc -> control ; WARN ( old . nv != POSTED_INTR_WAKEUP_VECTOR , \"Wakeup handler not enabled while the VCPU is blocked \\n \" ); dest = cpu_physical_id ( vcpu -> cpu ); if ( x2apic_enabled ()) new . ndst = dest ; else new . ndst = ( dest << 8 ) & 0xFF00 ; /* set 'NV' to 'notification vector' */ new . nv = POSTED_INTR_VECTOR ; } while ( cmpxchg64 ( & pi_desc -> control , old . control , new . control ) != old . control ); #将vCPU从等待列表中删除掉 if ( ! WARN_ON_ONCE ( vcpu -> pre_pcpu == - 1 )) { spin_lock ( & per_cpu ( blocked_vcpu_on_cpu_lock , vcpu -> pre_pcpu )); list_del ( & vcpu -> blocked_vcpu_list ); spin_unlock ( & per_cpu ( blocked_vcpu_on_cpu_lock , vcpu -> pre_pcpu )); vcpu -> pre_pcpu = - 1 ; } } 剩下一种状态转换路径 vCPU从 Runable => Blocked状态，这和从Running状态切换成Blocked状态一致，这里不再赘述！","tags":"virtualization","url":"https://kernelgo.org/vtd-posted-interrupt-code-analysis.html","loc":"https://kernelgo.org/vtd-posted-interrupt-code-analysis.html"},{"title":"VT-d Interrupt Remapping Code Analysis","text":"VT-d 中断重映射分析 本文中我们将一起来分析一下VT-d中断重映射的代码实现， 在看本文前建议先复习一下VT-d中断重映射的原理，可以参考 VT-D Interrupt Remapping 这篇文章。 看完中断重映射的原理我们必须明白：直通设备的中断是无法直接投递到Guest中的，需要先将其中断映射到host的某个中断上，然后再重定向（由VMM投递）到Guest内部． 我们将从 1.中断重映射Enable 2.中断重映射实现 3.中断重映射下中断处理流程 这3个层面去分析VT-d中断重映射的代码实现。 1.中断重映射Enable 当BIOS开启VT-d特性之后，操作系统初始化的时候会通过cpuid去检测硬件平台是否支持VT-d Interrupt Remapping能力， 然后做一些初始化工作后将操作系统的中断处理方式更改为Interrupt Remapping模式。 start_kernel --> late_time_init --> x86_late_time_init --> x86_init . irqs . intr_mode_init () --> apic_intr_mode_init --> default_setup_apic_routing --> enable_IR_x2apic --> irq_remapping_prepare # Step1 : 使能 Interrupt Reampping --> intel_irq_remap_ops . prepare () --> remap_ops = & intel_irq_remap_ops --> irq_remapping_enable # Step2 : 做一些工作 --> remap_ops -> enable () 从代码堆栈可以看到内核初始化的时候会初始化中断，在default_setup_apic_routing中会分2个阶段对Interrupt Remapping进行Enable。 这里涉及到一个关键的数据结构intel_irq_remap_ops，它是Intel提供的Intel CPU平台的中断重映射驱动方法集。 struct irq_remap_ops intel_irq_remap_ops = { . prepare = intel_prepare_irq_remapping , . enable = intel_enable_irq_remapping , . disable = disable_irq_remapping , . reenable = reenable_irq_remapping , . enable_faulting = enable_drhd_fault_handling , . get_ir_irq_domain = intel_get_ir_irq_domain , . get_irq_domain = intel_get_irq_domain , }; 阶段1调用intel_irq_remap_ops的prepare方法。该方法主要做的事情有： 调用了dmar_table_init从ACPI表中解析了DMAR Table关键信息。 关于VT-d相关的ACPI Table信息可以参考 VT-D Spec Chapter 8 BIOS Consideration 和 Introduction to Intel IOMMU 这篇文章。 遍历每个iommu检查是否支持中断重映射功能（dmar_ir_support）。 调用intel_setup_irq_remapping为每个IOMMU分配中断重映射表（Interrupt Remapping Table）。 static int intel_setup_irq_remapping ( struct intel_iommu * iommu ) { ir_table = kzalloc ( sizeof ( struct ir_table ), GFP_KERNEL ); //为IOMMU申请一块内存，存放ir_table和对应的bitmap pages = alloc_pages_node ( iommu -> node , GFP_KERNEL | __GFP_ZERO , INTR_REMAP_PAGE_ORDER ); bitmap = kcalloc ( BITS_TO_LONGS ( INTR_REMAP_TABLE_ENTRIES ), sizeof ( long ), GFP_ATOMIC ); // 创建ir_domain和ir_msi_domain iommu -> ir_domain = irq_domain_create_hierarchy ( arch_get_ir_parent_domain (), 0 , INTR_REMAP_TABLE_ENTRIES , fn , & intel_ir_domain_ops , iommu ); irq_domain_free_fwnode ( fn ); iommu -> ir_msi_domain = arch_create_remap_msi_irq_domain ( iommu -> ir_domain , \"INTEL-IR-MSI\" , iommu -> seq_id ); ir_table -> base = page_address ( pages ); ir_table -> bitmap = bitmap ; iommu -> ir_table = ir_table ; iommu_set_irq_remapping //最后将ir_table地址写入到寄存器中并最后enable中断重映射能力 } 阶段2调用irq_remapping_enable中判断Interrupt Remapping是否Enable如果还没有就Enable一下，然后set_irq_posting_cap设置Posted Interrupt Capability等。 2.中断重映射实现 要使得直通设备的中断能够工作在Interrupt Remapping模式下VFIO中需要做很多的准备工作． 首先，QEMU会通过PCI配置空间向操作系统呈现直通设备的MSI/MSI-X Capability，这样Guest OS加载设备驱动程序时候会尝试去Enable直通设备的MSI/MSI-X中断． 为了方便分析代码流程这里以MSI中断为例．Guest OS设备驱动尝试写配置空间来Enable设备中断，这时候会访问设备PCI配置空间发生VM Exit被QEMU截获处理． 对于MSI中断Enable会调用 vfio_msi_enable 函数． 从PCI Local Bus Spec 可以知道MSI中断的PCI Capability为 xxx QEMU Code : static void vfio_msi_enable ( VFIOPCIDevice * vdev ) { int ret , i ; vfio_disable_interrupts ( vdev ); #先 disable设备中断 vdev -> nr_vectors = msi_nr_vectors_allocated ( & vdev -> pdev ); #从设备配置空间读取设备 Enable的MSI中断数目 vdev -> msi_vectors = g_new0 ( VFIOMSIVector , vdev -> nr_vectors ); for ( i = 0 ; i < vdev -> nr_vectors ; i ++ ) { VFIOMSIVector * vector = & vdev -> msi_vectors [ i ]; vector -> vdev = vdev ; vector -> virq = - 1 ; vector -> use = true ; if ( event_notifier_init ( & vector -> interrupt , 0 )) { error_report ( \"vfio: Error: event_notifier_init failed\" ); } qemu_set_fd_handler ( event_notifier_get_fd ( & vector -> interrupt ), #绑定 irqfd的处理函数 vfio_msi_interrupt , NULL , vector ); # 将中断信息刷新到kvm irq routing table里，gsi和irqfd的映射关系;重点分析 vfio_add_kvm_msi_virq ( vdev , vector , i , false ); } /* Set interrupt type prior to possible interrupts */ vdev -> interrupt = VFIO_INT_MSI ; #使能msi中断！！！重点分析 ret = vfio_enable_vectors ( vdev , false ); .... } vfio_msi_enable 的主要流程是：从配置空间查询支持的中断数目 --> 对每个MSI中断进行初始化（分配一个irqfd） --> 将MSI gsi信息和irqfd绑定并刷新到中断路由表中 --> 使能中断（调用vfio-pci内核ioctl为MSI中断申请irte并刷新中断路由表表项）。 QEMU Code : vfio_pci_write_config --> vfio_msi_enable --> vfio_add_kvm_msi_virq --> kvm_irqchip_add_msi_route #为 MSI中断申请gsi ，并更新 irq routing table （ QEMU里面有一份Copy ） --> kvm_irqchip_commit_routes --> kvm_irqchip_add_irqfd_notifier_gsi #将 irqfd和gsi映射信息注册到kvm内核模块中fd = kvm_interrupt , gsi = virq , flags = 0 , rfd = NULL --> kvm_vm_ioctl ( s , KVM_IRQFD , & irqfd ) --> vfio_enable_vectors --> ioctl ( vdev -> vbasedev . fd , VFIO_DEVICE_SET_IRQS , irq_set ) #调用 vfio - pci内核接口使能中断 ，重点分析！ kvm_irqchip_commit_routes的逻辑比较简单这里跳过，kvm_irqchip_add_irqfd_notifier_gsi的逻辑稍微有些复杂后面专门写一篇来分析，只需要知道这里是将gsi和irqfd信息注册到内核模块中（KVM irqfd提供了一种中断注入机制）并且可以在这个fd上监听事件来达到中断注入的目的。 这里重点分析vfio_enable_vectors的代码流程。 Kernel Code : vfio_enable_vectors --> vfio_pci_ioctl --> vfio_pci_set_irqs_ioctl --> vfio_pci_set_msi_trigger --> vfio_msi_enable # Step1 ：为 MSI中断申请Host IRQ ，这里会一直调用到 Interrupt Remapping框架分配IRTE --> pci_alloc_irq_vectors --> vfio_msi_set_block # Step2 ：这里绑定 irqfd ，建立好中断注入通道 --> vfio_msi_set_vector_signal vfio_pci_set_msi_trigger 函数中主要有2个关键步骤。 vfio_msi_enable vfio_msi_enable -> pci_alloc_irq_vectors -> pci_alloc_irq_vectors_affinity -> __pci_enable_msi_range -> msi_capability_init -> pci_msi_setup_msi_irqs -> arch_setup_msi_irqs -> x86_msi.setup_msi_irqs -> native_setup_msi_irqs -> msi_domain_alloc_irqs -> __irq_domain_alloc_irqs,irq_domain_activate_irq 这里内核调用栈比较深，我们只需要知道 vfio_msi_enable 最终调用到了 __irq_domain_alloc_irqs -> intel_irq_remapping_alloc . 在intel_irq_remapping_alloc中申请这个中断对应的IRTE。这里先调用的alloc_irte函数返回rte在中断重映射表中的index号，再调用 intel_irq_remapping_prepare_irte去填充irte。 static int intel_irq_remapping_alloc ( struct irq_domain * domain , unsigned int virq , unsigned int nr_irqs , void * arg ) { index = alloc_irte ( iommu , virq , & data -> irq_2_iommu , nr_irqs ); #向 Interrupt Remapping Table申请index for ( i = 0 ; i < nr_irqs ; i ++ ) { irq_data = irq_domain_get_irq_data ( domain , virq + i ); irq_cfg = irqd_cfg ( irq_data ); irq_data -> hwirq = ( index << 16 ) + i ; irq_data -> chip_data = ird ; irq_data -> chip = & intel_ir_chip ; intel_irq_remapping_prepare_irte ( ird , irq_cfg , info , index , i ); } } irq_domain_activate_irq 最终会调用到intel_irq_remapping_activate -> intel_ir_reconfigure_irte -> modify_irte 。 modify_irte中会将新的irte刷新到中断重定向表中。 vfio_msi_set_block vfio_msi_set_block 中调用 vfio_msi_set_vector_signal 为每个msi中断安排其Host IRQ的信号处理钩子，用来完成中断注入。 其内核调用栈为： vfio_pci_ioctl vfio_pci_set_irqs_ioctl vfio_pci_set_msi_trigger vfio_msi_set_block irq_bypass_register_producer __connect kvm_arch_irq_bypass_add_producer vmx_update_pi_irte # 在 Posted Interrupt模式下在这里刷新irte为Posted Interrupt 模式 irq_set_vcpu_affinity intel_ir_set_vcpu_affinity modify_irte 再看下一 vfio_msi_set_vector_signal 的代码主要流程。 可以看出vfio_msi_set_vector_signal中为设备MSI中断申请了一个ISR，即vfio_msihandler， 然后注册了一个producer。 static int vfio_msi_set_vector_signal ( struct vfio_pci_device * vdev , int vector , int fd , bool msix ) { irq = pci_irq_vector ( pdev , vector ); #获得每个 MSI中断的irq号 ， trigger = eventfd_ctx_fdget ( fd ); if ( msix ) { struct msi_msg msg ; get_cached_msi_msg ( irq , & msg ); pci_write_msi_msg ( irq , & msg ); } #在host上申请中断处理函数 ret = request_irq ( irq , vfio_msihandler , 0 , vdev -> ctx [ vector ]. name , trigger ); vdev -> ctx [ vector ]. producer . token = trigger ; # irqfd对应的event_ctx vdev -> ctx [ vector ]. producer . irq = irq ; ret = irq_bypass_register_producer ( & vdev -> ctx [ vector ]. producer ); vdev -> ctx [ vector ]. trigger = trigger ; } 这样直通设备的中断会触发Host上的vfio_msihandler这个中断处理函数。在这个函数中向这个irqfd发送了一个信号通知中断到来， 如此一来KVM irqfd机制在poll这个irqfd的时候会受到这个事件，随后调用事件的处理函数注入中断。 irqfd_wakeup -> EPOLLIN -> schedule_work(&irqfd->inject) -> irqfd_inject -> kvm_set_irq这样就把中断注入到虚拟机了。 static irqreturn_t vfio_msihandler ( int irq , void * arg ) { struct eventfd_ctx * trigger = arg ; eventfd_signal ( trigger , 1 ) ; return IRQ_HANDLED ; } 3.中断重映射下中断处理流程 为了方便理解，我花了点时间画了下面这张图，方便读者理解中断重映射场景下直通设备的中断处理流程： 总结一下中断重映射Enable和处理流程： QEMU向虚拟机呈现设备的PCI配置空间信息 -> 设备驱动加载，读写 PCI配置空间Enable MSI -> VM Exit到QEMU中处理vfio_pci_write_config -> QEMU调用vfio_msi_enable使能MSI中断 -> kvm_set_irq_routing更新中断路由表PRT ， kvm_irqfd_assign注册irqfd和gsi的映射关系 ， vfio_pci_set_msi_trigger分配Host irq并分配对应的IRTE和刷新中断重映射表 ， vfio_msi_set_vector_signal注册Host irq的中断处理函数vfio_msihandler -> vfio_msihandler写了irqfd这样就触发了EPOLLIN事件 -> irqfd接受到EPOLLIN事件 ，调用 irqfd_wakeup -> kvm_arch_set_irq_inatomic 尝试直接注入中断，如果被 BLOCK了 （ vCPU没有退出 ？）就调用 schedule_work ( & irqfd -> inject ) ，让 kworker延后处理 -> irqfd_inject向虚拟机注入中断 -> 虚拟机退出的时候写对应 VCPU的vAPIC Page IRR字段注入中断到Guest内部 。 Done!","tags":"virtualization","url":"https://kernelgo.org/vtd_interrupt_remapping_code_analysis.html","loc":"https://kernelgo.org/vtd_interrupt_remapping_code_analysis.html"},{"title":"Debug Linux Kernel Using QEMU and GDB","text":"有的时候为了研究内核原理或者调试bios的时候，可以利用QEMU和gdb的方式来帮助我们调试问题． 这种操作利用了QEMU内建的gdb-stub能力． 重新编译内核 编译的时候开启内核参数CONFIG_DEBUG_INFO和CONFIG_GDB_SCRIPTS再进行编译， 如果硬件支持CONFIG_FRAME_POINTER也一并开启． make modules -j ` nproc ` make -j ` nproc ` 调试内核 用下的命令行拉起QEMU，这里可以从自己的OS上选取一个initramfs传给QEMU， 记得配上 nokaslr 以免内核段基地址被随机映射． 这里 -S 参数可以让QEMU启动后CPU先Pause住不运行， -s 参数是 -gdb tcp::1234 的简写，意思是让QEMU侧的gdb server侦听在1234端口等待调试． /mnt/code/qemu/x86_64-softmmu/qemu-system-x86_64 \\ -machine pc-i440fx-2.8,accel = kvm,kernel_irqchip \\ -cpu host \\ -m 4096 ,slots = 4 ,maxmem = 16950M \\ -smp 4 \\ -chardev pty,id = charserial0 \\ -device isa-serial,chardev = charserial0,id = serial0 \\ -netdev tap,id = tap0,ifname = virbr0-nic,vhost = on,script = no \\ -device virtio-net-pci,netdev = tap0 \\ -kernel $KERNEL_SRC /arch/x86/boot/bzImage \\ -initrd /boot/vmlinuz-4.14.0-rc2-fangying \\ -append 'console=ttyS0 nokaslr' \\ -vnc :9 \\ -S -s gdb可能会报错 Remote 'g' packet reply is too long: ，这个时候的解决办法是打上一个补丁然后重新编译gdb. 问题处在static void process_g_packet (struct regcache *regcache)函数，6113行，屏蔽对buf_len的判断． 如果gdb版本低(7.x)打这个补丁: if ( buf_len > 2 * rsa->sizeof_g_packet ) error ( _ ( \"Remote 'g' packet reply is too long: %s\" ) , rs->buf ) ; 改为: if ( buf_len > 2 * rsa->sizeof_g_packet ) { rsa->sizeof_g_packet = buf_len ; for ( i = 0 ; i < gdbarch_num_regs ( gdbarch ) ; i++ ) { if ( rsa->regs [ i ] .pnum == -1 ) continue ; if ( rsa->regs [ i ] .offset > = rsa->sizeof_g_packet ) rsa->regs [ i ] .in_g_packet = 0 ; else rsa->regs [ i ] .in_g_packet = 1 ; } } 如果gdb版本高(8.x)打这个补丁: if ( buf_len > 2 * rsa->sizeof_g_packet ) error ( _ ( \"Remote 'g' packet reply is too long: %s\" ) , rs->buf ) ; 改为: /* Further sanity checks, with knowledge of the architecture. */ if ( buf_len > 2 * rsa->sizeof_g_packet ) { rsa->sizeof_g_packet = buf_len ; for ( i = 0 ; i < gdbarch_num_regs ( gdbarch ) ; i++ ) { if ( rsa->regs [ i ] .pnum == -1 ) continue ; if ( rsa->regs [ i ] .offset > = rsa->sizeof_g_packet ) rsa->regs [ i ] .in_g_packet = 0 ; else rsa->regs [ i ] .in_g_packet = 1 ; } } 开始愉快地调试内核了: $ gdb vmlinux ( gdb ) target remote :1234 Remote debugging using :1234 0x000000000000fff0 in cpu_hw_events () ( gdb ) hb start_kernel Hardware assisted breakpoint 1 at 0xffffffff827dabb2: file init/main.c, line 538 . ( gdb ) c Continuing. Thread 1 hit Breakpoint 1 , start_kernel () at init/main.c:538 538 { ( gdb ) l 533 { 534 rest_init () ; 535 } 536 537 asmlinkage __visible void __init start_kernel ( void ) 538 { 539 char *command_line ; 540 char *after_dashes ; 541 542 set_task_stack_end_magic ( & init_task ) ; ( gdb )","tags":"linux","url":"https://kernelgo.org/kernel-debug-using-qemu.html","loc":"https://kernelgo.org/kernel-debug-using-qemu.html"},{"title":"Tips on Linux","text":"生成内核符号表 make tags ARCH = x86 make cscope ARCH = x86 密码输错超过最大次数，解锁骚操作 pam_tally2 --user root pam_tally2 -r -u root 更新let's encrypt证书 yum -y install yum-utils yum-config-manager --enable rhui-REGION-rhel-server-extras rhui-REGION-rhel-server-optional yum install python2-certbot-nginx certbot --nginx certbot renew --dry-run ftrace查看pCPU上的调度情况 这里以查看pCPU2的调度状况为例 mount -t debugfs none /sys/kernel/debug/ cd /sys/kernel/debug/tracing echo 1 > events/sched/enable ; sleep 2 ; echo 0 > events/sched/enable cat per_cpu/cpu2/trace > /home/trace.txt 可以看到PCPU上的调度情况 # tracer : nop # # entries - in - buffer / entries - written : 381200 / 1199928 #P : 24 # # _ -----=> irqs-off # / _ ----=> need-resched # | / _ ---=> hardirq/softirq # || / _ --=> preempt-depth # ||| / delay # TASK - PID CPU # |||| TIMESTAMP FUNCTION # | | | |||| | | < idle >- 0 [ 002 ] d ... 158158.084267 : sched_switch : prev_comm = swapper / 2 prev_pid = 0 prev_prio = 120 prev_state = R ==> next_comm = CPU 0 / KVM next_pid = 2046 next_prio = 120 CPU 0 / KVM - 2046 [ 002 ] d ... 158158.084280 : sched_stat_runtime : comm = CPU 0 / KVM pid = 2046 runtime = 16239 [ ns ] vruntime = 105862773195 [ ns ] CPU 0 / KVM - 2046 [ 002 ] d ... 158158.084281 : sched_switch : prev_comm = CPU 0 / KVM prev_pid = 2046 prev_prio = 120 prev_state = S ==> next_comm = swapper / 2 next_pid = 0 next_prio = 120 < idle >- 0 [ 002 ] dNh . 158158.085265 : sched_wakeup : comm = CPU 0 / KVM pid = 2046 prio = 120 success = 1 target_cpu = 002 < idle >- 0 [ 002 ] d ... 158158.085266 : sched_switch : prev_comm = swapper / 2 prev_pid = 0 prev_prio = 120 prev_state = R ==> next_comm = CPU 0 / KVM next_pid = 2046 next_prio = 120 CPU 0 / KVM - 2046 [ 002 ] d ... 158158.085280 : sched_stat_runtime : comm = CPU 0 / KVM pid = 2046 runtime = 16148 [ ns ] vruntime = 105862789343 [ ns ] CPU 0 / KVM - 2046 [ 002 ] d ... 158158.085281 : sched_switch : prev_comm = CPU 0 / KVM prev_pid = 2046 prev_prio = 120 prev_state = S ==> next_comm = swapper / 2 next_pid = 0 next_prio = 120 < idle >- 0 [ 002 ] dNh . 158158.086265 : sched_wakeup : comm = CPU 0 / KVM pid = 2046 prio = 120 success = 1 target_cpu = 002 测试NUMA NODE内存性能 这里以加压100M为例 # 关闭 NMI watchdog ，防止压力过大把系统搞死 echo 0 > / proc / sys / kernel / nmi_watchdog # 打开 shell的cgroup隔离 echo $$ > / sys / fs / cgroup / cpuset / tasks 获取 Stream加压工具 git clone https : // github . com / jeffhammond / STREAM cd STREAM - master gcc - O - DSTREAM_ARRAY_SIZE = 100000000 stream . c - o stream . 100 M # 获取 pcm带宽测试工具 git clone https : // github . com / opcm / pcm chmod + x . / pcm - memory . x . / pcm - memory . x 查看带宽 如何查看任务的调度延时 通过perf sched 查看某段时间内进程的调度延时 perf sched record -- sleep 1 #观察调度延迟 perf sched latency -- sort = max No newline at end of file : set binary noeol 如何查找一个系统调用的定义在哪个文件中 grep - E \"SYSCALL_DEFINE[0-6]\\(listen\" - nr perf查看虚拟机性能数据 pid = $ ( pgrep qemu - kvm ) perf kvm stat record - p $ pid perf kvm stat report wget下载指定目录的指定文件到 wget - c - r - nd - np - k - L - p - A \"qemu*.rpm\" http : // mirrors . aliyun . com / centos / 7 . 6 . 1810 / updates / x86_64 / Packages / - c 断点续传， - r 递归， - nd不在本地创建对应文件夹 ， - np不下载父目录 ， - A 接受哪些类型的文件（这里可以试一个 glob表达式 ） 查询设备/设备类属性 设备属性 virsh qemu - monitor - command vmname '{\"arguments\": {\"typename\": \"virtio-net-pci\"}, \"execute\":\"device-list-properties\"}' 设备类属性 virsh qemu - monitor - command vmname '{\"arguments\": {\"typename\": \"virtio-net-pci\"}, \"execute\":\"qom-list-properties\"}' 设备实例的属性 virsh qemu - monitor - command fangying --hmp \"info qtree\" | less 配置yum proxy # append to / etc / yum . conf proxy = http : // xxxx proxy_user = xxxx proxy_password = xxxx","tags":"linux","url":"https://kernelgo.org/linux-tips.html","loc":"https://kernelgo.org/linux-tips.html"},{"title":"Insight Into VFIO","text":"本文则主要探讨一下VFIO实现中的关键点，主要包括： VFIO中如实现对直通设备的I/O地址空间访问的？ VFIO中如何实现MSI/MSI-X，Interrupt Remapping，以及Posted Interrupt的支持？ VFIO中是如何建立DMA Remapping映射关系？ VFIO中又是如何支持设备热插拔的？ 上面4个问题你能回答上来吗？ 1.VFIO中如实现对直通设备的I/O地址空间访问？ 在设备直通的场景下guest OS到底该如何访问设备I/O空间？ 有两种方法可选： 方法A：直接呈现，将设备在主机上的PCI BAR呈现给guest，并通过VMCS的I/O bitmap和EPT页表使guest访问设备的PIO和MMIO都不引起VM-Exit，这样guest驱动程序可以直接访问设备的I/O地址空间。 方法B：建立转换表，呈现虚拟的PCI BAR给guest，当guest访问到虚拟机的I/O地址空间时VMM截获操作并通过转换表将I/O请求转发到设备在主机上的I/O地址空间上。 方法A看起来很高效，因为直接呈现的方式下不引入VM-Exit，但实际上是有问题的！ 原因是 ： 设备的PCI BAR空间是由host的BIOS配置并由host操作系统直接使用的， guest的PCI BAR空间是由guest的虚拟BIOS（例如Seabios）配置的， 那么问题来了，到底该由谁来配置设备的PCI BAR空间呢？肯定不能两个都生效否则就打架了！ 我们应该阻止guest来修改真实设备的PCI BAR地址以防止造成host上PCI设备的BAR空间冲突导致可能出现的严重后果。 所以我们要选择方案B，建立转换表，明白这一点很重要！ 对于直通设备的PIO访问而言，通过设置VMCS的I/O bitmap控制guest访问退出到VMM中然后通过转换表（模拟的方式）将PIO操作转发到真实物理设备上。对于MMIO的访问，可以通过EPT方式将虚拟的MMIO地址空间映射到物理设备的MMIO地址空间上，这样guest访问MMIO时并不需要VM-Exit。 直通设备的PCI Config Space模拟 PCI配置空间是用来报告设备I/O信息的区域，可以通过PIO或者MMIO方式进行访问。 设备直通场景的配置空间并不直接呈现给guest而是由VFIO配合qemu进行模拟的。 vfio_realize函数中， QEMU会读取物理设备的PCI配置空间以此为基础然后对配置空间做些改动然后呈现给虚拟机。 /* Get a copy of config space */ // 读取设备的原始PCI Config Space信息 ret = pread ( vdev -> vbasedev . fd , vdev -> pdev . config , MIN ( pci_config_size ( & vdev -> pdev ), vdev -> config_size ), vdev -> config_offset ); // 调用vfio-pci内核中的vfio_pci_read实现 ...... /* vfio emulates a lot for us, but some bits need extra love */ vdev -> emulated_config_bits = g_malloc0 ( vdev -> config_size ); // 我们可以选择性的Enable/Disable一些Capability /* QEMU can choose to expose the ROM or not */ memset ( vdev -> emulated_config_bits + PCI_ROM_ADDRESS , 0xff , 4 ); /* QEMU can also add or extend BARs */ memset ( vdev -> emulated_config_bits + PCI_BASE_ADDRESS_0 , 0xff , 6 * 4 ); // 调用vfio_add_emulated_word修改模拟的PCI配置空间信息 vfio_add_emulated_word /* * Clear host resource mapping info. If we choose not to register a * BAR, such as might be the case with the option ROM, we can get * confusing, unwritable, residual addresses from the host here. */ memset ( & vdev -> pdev . config [ PCI_BASE_ADDRESS_0 ], 0 , 24 ); memset ( & vdev -> pdev . config [ PCI_ROM_ADDRESS ], 0 , 4 ); vfio_bars_prepare ( vdev ); // 重点分析 vfio_bars_register ( vdev ); // 重点分析 vfio_add_capabilities ( vdev , errp ); 通常MSI/MSIX等信息都需要被QEMU修改，因为这些都是QEMU使用VFIO去模拟的。 直通设备MMIO（BAR空间）映射 vfio_realize函数中会对直通设备的MMIO空间进行映射，大致包含以下几个步骤: 调用vfio_populate_device从VFIO中查询出设备的BAR空间信息 把设备的MMIO（BAR空间）重映射（mmap）到QEMU进程的虚拟地址空间 将该段虚拟机地址空间标记为RAM类型注册给虚拟机 这样一来，guest访问MMIO地址空间时直接通过EPT翻译到HPA不需要VM-Exit。我们分析下具体流程： vfio_realize |-> vfio_populate_device |-> vfio_region_setup |-> vfio_get_region_info // call ioct VFIO_DEVICE_GET_REGION_INFO |-> memory_region_init_io // init region->mem MR as I/O |-> vfio_bars_prepare -> vfio_bar_prepare // probe info of each pci bar from PCI cfg space |-> vfio_bars_register -> vfio_bar_register |-> memory_region_init_io // int bar->mr |-> memory_region_add_subregion // add bar->mr into region->mem MR |-> vfio_region_mmap |-> mmap // map device bar space into QEMU process address space -> iova |-> memory_region_init_ram_device_ptr // register iova into VM physical AS |-> memory_region_add_subregion // add region->mmaps[i].mem into region->mem MR |-> pci_register_bar 为了方便理解这个过程，我画了一张示意图： QEMU首先调用vfio_region_mmap， 通过mmap region->vbasedev->fd 把设备MMIO映射到QEMU进程的虚拟地址空间， 这实际上通过调用vfio-pci内核驱动vfio_pci_mmap -> remap_pfn_range， remap_pfn_range 是内核提供的API， 可以将一段连续的物理地址空间映射到进程的虚拟地址空间， 这里用它将设备的BAR空间的MMIO先映射到QEMU进程的虚拟地址空间再注册给虚拟机。 static int vfio_pci_mmap ( void * device_data , struct vm_area_struct * vma ) { req_len = vma -> vm_end - vma -> vm_start ; // MMIO size vma -> vm_pgoff = ( pci_resource_start ( pdev , index ) >> PAGE_SHIFT ) + pgoff ; // MMIO page address return remap_pfn_range ( vma , vma -> vm_start , vma -> vm_pgoff , req_len , vma -> vm_page_prot ); } 再来看下QEMU是如何注册这段虚拟地址(IOVA)到虚拟机的。 vfio_region_mmap调用memory_region_init_ram_device_ptr把前面mmap过来的 这段IOVA作为RAM类型设备注册给虚拟机。 int vfio_region_mmap ( VFIORegion * region ) { name = g_strdup_printf ( \"%s mmaps[%d]\" , memory_region_name ( region -> mem ), i ); memory_region_init_ram_device_ptr ( & region -> mmaps [ i ]. mem , memory_region_owner ( region -> mem ), name , region -> mmaps [ i ]. size , region -> mmaps [ i ]. mmap ); memory_region_add_subregion ( region -> mem , region -> mmaps [ i ]. offset , & region -> mmaps [ i ]. mem ); } memory_region_init_ram_device_ptr中会标志 mr->ram = true， 那么QEMU就会通过kvm_set_phys_mem注册这段内存给虚拟机（是RAM类型才会建立EPT映射关系）， 这样KVM就会为这段地址空间建立EPT页表， 虚拟机访问设备的MMIO空间时通过EPT页表翻直接访问不需要VM-Exit。 例如，网卡的收发包场景，虚拟机可以直接操作真实网卡的相关寄存器（MMIO映射）而没有陷入先出开销，大幅度提升了虚拟化场景下的I/O性能。 static void kvm_set_phys_mem ( KVMMemoryListener * kml , MemoryRegionSection * section , bool add ) { if ( ! memory_region_is_ram ( mr )) { // mr->ram = true 会注册到KVM if ( writeable || ! kvm_readonly_mem_allowed ) { return ; } else if ( ! mr -> romd_mode ) { /* If the memory device is not in romd_mode, then we actually want * to remove the kvm memory slot so all accesses will trap. */ add = false ; } } ram = memory_region_get_ram_ptr ( mr ) + section -> offset_within_region + ( start_addr - section -> offset_within_address_space ); kvm_set_user_memory_region // 作为RAM设备注册到KVM中 } 2.VFIO中如何实现MSI/MSI-X，Interrupt Remapping，以及Posted Interrupt的支持？ 对于VFIO设备直通而言，设备中断的处理方式共有4种: INTx 最传统的PCI设备引脚Pin方式 MSI/MSI-X方式 Interrupt Remapping方式 VT-d Posted Interrupt方式 那么它们分别是如何设计实现的呢？ 这里我们来重点探索一下MSI/MSI-X的实现方式以及VT-d Posted Interrupt方式。 如果忘了MSI和MSI-X的知识点可以看下《 PCI Local Bus Specification Revision 3.0 》的Chapter 6有比较详细的介绍。 先看下QEMU这边中断初始化和中断使能相关的函数调用关系图： vfio_realize |-> vfio_get_device // get device info: num_irqs, num_regions, flags |-> vfio_msix_early_setup // get MSI-X info: table_bar,table_offset, pba_ -> pci_device_route_intx_to_irqbar,pba_offset, entries |-> vfio_pci_fixup_msix_region |-> vfio_pci_relocate_msix |-> vfio_add_capabilities |-> vfio_add_std_cap |-> vfio_msi_setup -> msi_init |-> vfio_msix_setup -> msix_init |-> vfio_intx_enable // enable intx |-> pci_device_route_intx_to_irq |-> event_notifier_init & vdev -> intx . interrupt |-> ioctl VFIO_DEVICE_SET_IRQS kvm_cpu_exec ... |-> vfio_pci_write_config |-> vfio_msi_enable |-> event_notifier_init // init eventfd as irqfd |-> vfio_add_kvm_msi_virq ... -> kvm_irqchip_assign_irqfd |-> vfio_enable_vectors false |-> vfio_msix_enable |-> vfio_msix_vector_do_use -> msix_vector_use |-> vfio_msix_vector_release |-> msix_set_vector_notifiers 从图中可以看出，直通设备初始化时候会从物理设备的PCI配置空间读取INTx、MSI、MSI-X的相关信息并且进行一些必要的初始化（setup）再进行中断使能（enable）的。 根据调试的结果来看，INTx的enable是最早的， 而MSI/MSI-X初始化是在guest启动后进行enable。 这里以MSI-X为例， 首先调用vfio_msix_early_setup函数从硬件设备的PCI配置空间查询MSI-X相关信息包括: MSI-X Table Size ：MSI-X Table 大小 MSI-X Table BAR Indicator ：MSI-X Table存放的BAR空间编号 MSI-X Table Offset ：存放MSI-X Table在Table BAR空间中的偏移量 MSI-X PBA BIR ：存放MSI-X 的Pending Bit Array的BAR空间编号 MSI-X PBA Offset ：存放MSI-X Table在PBA BAR空间中的偏移量 获取必要信息之后，通过vfio_msix_setup来完成直通设备的MSI-X的初始化工作， 包括调用pci_add_capability为设备添加PCI_CAP_ID_MSIX Capability， 并注册MSI-X的BAR空间到虚拟机的物理地址空间等。 static void vfio_msix_early_setup ( VFIOPCIDevice * vdev , Error ** errp ) { if ( pread ( fd , & ctrl , sizeof ( ctrl ), vdev -> config_offset + pos + PCI_MSIX_FLAGS ) != sizeof ( ctrl )) { error_setg_errno ( errp , errno , \"failed to read PCI MSIX FLAGS\" ); return ; } if ( pread ( fd , & table , sizeof ( table ), vdev -> config_offset + pos + PCI_MSIX_TABLE ) != sizeof ( table )) { error_setg_errno ( errp , errno , \"failed to read PCI MSIX TABLE\" ); return ; } if ( pread ( fd , & pba , sizeof ( pba ), vdev -> config_offset + pos + PCI_MSIX_PBA ) != sizeof ( pba )) { error_setg_errno ( errp , errno , \"failed to read PCI MSIX PBA\" ); return ; } ctrl = le16_to_cpu ( ctrl ); table = le32_to_cpu ( table ); pba = le32_to_cpu ( pba ); msix = g_malloc0 ( sizeof ( * msix )); msix -> table_bar = table & PCI_MSIX_FLAGS_BIRMASK ; msix -> table_offset = table & ~ PCI_MSIX_FLAGS_BIRMASK ; msix -> pba_bar = pba & PCI_MSIX_FLAGS_BIRMASK ; msix -> pba_offset = pba & ~ PCI_MSIX_FLAGS_BIRMASK ; msix -> entries = ( ctrl & PCI_MSIX_FLAGS_QSIZE ) + 1 ; } static int vfio_msix_setup ( VFIOPCIDevice * vdev , int pos , Error ** errp ) { vdev -> msix -> pending = g_malloc0 ( BITS_TO_LONGS ( vdev -> msix -> entries ) * sizeof ( unsigned long )); ret = msix_init ( & vdev -> pdev , vdev -> msix -> entries , vdev -> bars [ vdev -> msix -> table_bar ]. mr , vdev -> msix -> table_bar , vdev -> msix -> table_offset , vdev -> bars [ vdev -> msix -> pba_bar ]. mr , vdev -> msix -> pba_bar , vdev -> msix -> pba_offset , pos , & err ); memory_region_set_enabled ( & vdev -> pdev . msix_pba_mmio , false ); if ( object_property_get_bool ( OBJECT ( qdev_get_machine ()), \"vfio-no-msix-emulation\" , NULL )) { memory_region_set_enabled ( & vdev -> pdev . msix_table_mmio , false ); } } 最后guest启动后调用vfio_msix_enable使能MSI-X中断。 irqfd 和 ioeventfd 我们知道QEMU本身有一套完整的模拟PCI设备INTx、MSI、MSI-X中断机制， 其实现方式是irqfd（QEMU中断注入到guest）和ioeventfd（guest中断通知到QEMU）， 内部实现都是基于内核提供的eventfd机制。 看代码的时候一直没明白设备中断绑定的irqfd是在什么时候注册的，从代码上看不是中断enable时候。 后来结合代码调试才明白， 原来中断enable时候将设备的MSI/MSI-X BAR空间映射用MMIO方式注册给了虚拟机（参考msix_init, msi_init函数实现）， 当虚拟机内部第一次访问MSI-X Table BAR空间的MMIO时会退出到用户态完成irqfd的注册， 调用堆栈为： #0 kvm_irqchip_assign_irqfd #1 in kvm_irqchip_add_irqfd_notifier_gsi #2 in vfio_add_kvm_msi_virq #3 in vfio_msix_vector_do_use #4 in vfio_msix_vector_use #5 in msix_fire_vector_notifier #6 in msix_handle_mask_update #7 in msix_table_mmio_write #8 in memory_region_write_accessor #9 in access_with_adjusted_size #10 in memory_region_dispatch_write #11 in address_space_write_continue #12 in address_space_write #13 in address_space_rw #14 in kvm_cpu_exec 关于irqfd的社区patch可以从这里获取 https://lwn.net/Articles/332924 。 备注：Alex Williamson在最新的VFIO模块中加入新特性，支持直接使用物理设备的MSIX BAR空间， 这样一来可以直接将物理设备的MSI-X BAR空间直接mmap过来然后呈现给虚拟机， guest直接使用而不用再进行模拟了。 commit ae0215b2bb56a9d5321a185dde133bfdd306a4c0 Author : Alexey Kardashevskiy < aik @ ozlabs . ru > Date : Tue Mar 13 11 : 17 : 31 2018 - 0600 vfio - pci : Allow mmap of MSIX BAR At the moment we unconditionally avoid mapping MSIX data of a BAR and emulate MSIX table in QEMU . However it is 1 ) not always necessary as a platform may provide a paravirt interface for MSIX configuration ; 2 ) can affect the speed of MMIO access by emulating them in QEMU when frequently accessed registers share same system page with MSIX data , this is particularly a problem for systems with the page size bigger than 4 KB . A new capability - VFIO_REGION_INFO_CAP_MSIX_MAPPABLE - has been added to the kernel [ 1 ] which tells the userspace that mapping of the MSIX data is possible now . This makes use of it so from now on QEMU tries mapping the entire BAR as a whole and emulate MSIX on top of that . [ 1 ] https : // git . kernel . org / pub / scm / linux / kernel / git / torvalds / linux . git / commit / ? id = a32295c612c57990d17fb0f41e7134394b2f35f6 Signed - off - by : Alexey Kardashevskiy < aik @ ozlabs . ru > Reviewed - by : David Gibson < david @ gibson . dropbear . id . au > Signed - off - by : Alex Williamson < alex . williamson @ redhat . com > 3. VFIO中是如何建立DMA Remapping映射关系？ 前面的文章中我们反复提到VT-d DMA Remapping的原理和意义，那么在vfio中这又是如何实现的呢？ 实现的原理其实不难，我们知道QEMU会维护虚拟机的物理地址空间映射关系， 而VT-d DMA Remapping需要建立GPA->HVA地址空间的映射关系， 那么当虚拟机的地址空间布局发生变化时我们都会尝试更新对应的DMA Remapping关系， 这是通过vfio_memory_listener来实现的。 static const MemoryListener vfio_memory_listener = { . region_add = vfio_listener_region_add , . region_del = vfio_listener_region_del , }; vfio_connect_container函数中会将vfio_memory_listener注册给QEMU的物理地址空间address_space_memory， 这样vfio_memory_listener会监听虚拟机的物理地址空间变化， 调用对应的回调函数更新DMA Remapping关系。 memory_listener_register ( & container -> listener , container -> space -> as ); 那么说了很久的IOMMU页表是如何创建和更新的呢？ 看下vfio_listener_region_add/vfio_listener_region_del函数的实现就知道。 在该函数中先check对应的section是否是RAM（只对RAM类型的区域进行DMA Remapping）， 再进行一些Sanity Check后调用vfio_dma_map将映射关系建立起来， 所以重点还是在于vfio_dma_map的函数实现。 static void vfio_listener_region_add ( MemoryListener * listener , MemoryRegionSection * section ) { VFIOContainer * container = container_of ( listener , VFIOContainer , listener ); hwaddr iova , end ; Int128 llend , llsize ; void * vaddr ; int ret ; VFIOHostDMAWindow * hostwin ; bool hostwin_found ; if ( vfio_listener_skipped_section ( section )) { // do dma_map only if MRS is RAM type trace_vfio_listener_region_add_skip ( section -> offset_within_address_space , section -> offset_within_address_space + int128_get64 ( int128_sub ( section -> size , int128_one ()))); return ; } if ( unlikely (( section -> offset_within_address_space & ~ TARGET_PAGE_MASK ) != ( section -> offset_within_region & ~ TARGET_PAGE_MASK ))) { error_report ( \"%s received unaligned region\" , __func__ ); return ; } iova = TARGET_PAGE_ALIGN ( section -> offset_within_address_space ); llend = int128_make64 ( section -> offset_within_address_space ); llend = int128_add ( llend , section -> size ); llend = int128_and ( llend , int128_exts64 ( TARGET_PAGE_MASK )); if ( int128_ge ( int128_make64 ( iova ), llend )) { return ; } end = int128_get64 ( int128_sub ( llend , int128_one ())); if ( container -> iommu_type == VFIO_SPAPR_TCE_v2_IOMMU ) { hwaddr pgsize = 0 ; /* For now intersections are not allowed, we may relax this later */ QLIST_FOREACH ( hostwin , & container -> hostwin_list , hostwin_next ) { if ( ranges_overlap ( hostwin -> min_iova , hostwin -> max_iova - hostwin -> min_iova + 1 , section -> offset_within_address_space , int128_get64 ( section -> size ))) { ret = - 1 ; goto fail ; } } ret = vfio_spapr_create_window ( container , section , & pgsize ); if ( ret ) { goto fail ; } vfio_host_win_add ( container , section -> offset_within_address_space , section -> offset_within_address_space + int128_get64 ( section -> size ) - 1 , pgsize ); #ifdef CONFIG_KVM if ( kvm_enabled ()) { VFIOGroup * group ; IOMMUMemoryRegion * iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); struct kvm_vfio_spapr_tce param ; struct kvm_device_attr attr = { . group = KVM_DEV_VFIO_GROUP , . attr = KVM_DEV_VFIO_GROUP_SET_SPAPR_TCE , . addr = ( uint64_t )( unsigned long ) & param , }; if ( ! memory_region_iommu_get_attr ( iommu_mr , IOMMU_ATTR_SPAPR_TCE_FD , & param . tablefd )) { QLIST_FOREACH ( group , & container -> group_list , container_next ) { param . groupfd = group -> fd ; if ( ioctl ( vfio_kvm_device_fd , KVM_SET_DEVICE_ATTR , & attr )) { error_report ( \"vfio: failed to setup fd %d \" \"for a group with fd %d: %s\" , param . tablefd , param . groupfd , strerror ( errno )); return ; } trace_vfio_spapr_group_attach ( param . groupfd , param . tablefd ); } } } #endif } hostwin_found = false ; QLIST_FOREACH ( hostwin , & container -> hostwin_list , hostwin_next ) { if ( hostwin -> min_iova <= iova && end <= hostwin -> max_iova ) { hostwin_found = true ; break ; } } if ( ! hostwin_found ) { error_report ( \"vfio: IOMMU container %p can't map guest IOVA region\" \" 0x%\" HWADDR_PRIx \"..0x%\" HWADDR_PRIx , container , iova , end ); ret = - EFAULT ; goto fail ; } memory_region_ref ( section -> mr ); // increase ref of MR by one if ( memory_region_is_iommu ( section -> mr )) { // guest IOMMU emulation VFIOGuestIOMMU * giommu ; IOMMUMemoryRegion * iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); trace_vfio_listener_region_add_iommu ( iova , end ); /* * FIXME: For VFIO iommu types which have KVM acceleration to * avoid bouncing all map/unmaps through qemu this way, this * would be the right place to wire that up (tell the KVM * device emulation the VFIO iommu handles to use). */ giommu = g_malloc0 ( sizeof ( * giommu )); giommu -> iommu = iommu_mr ; giommu -> iommu_offset = section -> offset_within_address_space - section -> offset_within_region ; giommu -> container = container ; llend = int128_add ( int128_make64 ( section -> offset_within_region ), section -> size ); llend = int128_sub ( llend , int128_one ()); iommu_notifier_init ( & giommu -> n , vfio_iommu_map_notify , IOMMU_NOTIFIER_ALL , section -> offset_within_region , int128_get64 ( llend )); QLIST_INSERT_HEAD ( & container -> giommu_list , giommu , giommu_next ); memory_region_register_iommu_notifier ( section -> mr , & giommu -> n ); memory_region_iommu_replay ( giommu -> iommu , & giommu -> n ); return ; } /* Here we assume that memory_region_is_ram(section->mr)==true */ vaddr = memory_region_get_ram_ptr ( section -> mr ) + // get hva section -> offset_within_region + ( iova - section -> offset_within_address_space ); trace_vfio_listener_region_add_ram ( iova , end , vaddr ); llsize = int128_sub ( llend , int128_make64 ( iova )); // calc map size if ( memory_region_is_ram_device ( section -> mr )) { hwaddr pgmask = ( 1ULL << ctz64 ( hostwin -> iova_pgsizes )) - 1 ; if (( iova & pgmask ) || ( int128_get64 ( llsize ) & pgmask )) { trace_vfio_listener_region_add_no_dma_map ( memory_region_name ( section -> mr ), section -> offset_within_address_space , int128_getlo ( section -> size ), pgmask + 1 ); return ; } } ret = vfio_dma_map ( container , iova , int128_get64 ( llsize ), // do VFIO_IOMMU_MAP_DMA vaddr , section -> readonly ); if ( ret ) { error_report ( \"vfio_dma_map(%p, 0x%\" HWADDR_PRIx \", \" \"0x%\" HWADDR_PRIx \", %p) = %d (%m)\" , container , iova , int128_get64 ( llsize ), vaddr , ret ); if ( memory_region_is_ram_device ( section -> mr )) { /* Allow unexpected mappings not to be fatal for RAM devices */ return ; } goto fail ; } return ; fail : if ( memory_region_is_ram_device ( section -> mr )) { error_report ( \"failed to vfio_dma_map. pci p2p may not work\" ); return ; } /* * On the initfn path, store the first error in the container so we * can gracefully fail. Runtime, there's not much we can do other * than throw a hardware error. */ if ( ! container -> initialized ) { if ( ! container -> error ) { container -> error = ret ; } } else { hw_error ( \"vfio: DMA mapping failed, unable to continue\" ); } } vfio_dma_map函数中会传入建立DMA Remapping的基本信息， 这里是用数据结构vfio_iommu_type1_dma_map来描述， 然后交给内核去做DMA Remapping。 struct vfio_iommu_type1_dma_map map = { . argsz = sizeof ( map ), . flags = VFIO_DMA_MAP_FLAG_READ , // flags . vaddr = ( __u64 )( uintptr_t ) vaddr , // HVA . iova = iova , // iova -> gpa . size = size , // map size }; 该函数对应的内核调用栈见下面的图，其主要流程包含2个步骤，简称pin和map： vfio_pin_pages_remote 把虚拟机的物理内存都pin住，物理内存不能交换 vfio_iommu_map 创建虚拟机domain的IOMMU页表，DMA Remapping地址翻译时候使用 值得注意的是在pin步骤中， pin虚拟机物理内存是调用get_user_pages_fast来实现的， 如果虚拟机的内存未申请那么会先将内存申请出来， 这个过程可能会非常耗时并且会持有进程的mmap_sem大锁。 vfio_dma_do_map -------------------------------------------------------------- vfio_pin_map_dma |-> vfio_pin_pages_remote // pin pages in memory |-> vaddr_get_pfn | get_user_pages_fast | gup_pud_range gup_pud_range gup_pte_range get_page // pin page, without mm->mmap_sem held | get_user_pages_unlocked __get_user_pages_locked __get_user_pages | handle_mm_fault __handle_mm_fault // do_page_fault process alloc pud -> pmd -> pte handle_pte_fault do_anonymous_page alloc_zeroed_user_highpage_movable ... alloc page with __GFP_ZERO | _get_page atomic_inc ( & page_count ) // pin page | get_user_pages_remote __get_user_pages_locked |-> vfio_iommu_map // create IOMMU domain page table |-> iommu_map |-> intel_iommu_map // intel-iommu.c |-> domain_pfn_mapping |-> pfn_to_dma_pte 同理，vfio_listener_region_add回调函数实现了DMA Remapping关系的反注册。 4. VFIO中又是如何支持设备热插拔的？ vfio热插拔仍然是走的QEMU设备热插拔流程，大致流程如下： qdev_device_add |-> device_set_realized |-> hotplug_handler_plug |-> piix4_device_plug_cb |-> piix4_send_gpe // inject ACPI GPE to guest OS |-> pci_qdev_realize |-> vfio_realize device_set_realized函数负责将设备上线， piix4_send_gpe函数中会注入一个ACPI GPE事件通知GuestOS， vfio_realize函数负责整个vfio的设备初始化流程。 那么vfio_realize中主要做了哪些事情呢？ 可以用下面这一张图概括~ 点击链接查看原图 vfio_realize的具体流程这里就不再展开免得啰嗦！ 感兴趣的请自己分析代码去，所有的vfio实现细节都在这里面。","tags":"virtualization","url":"https://kernelgo.org/vfio-insight.html","loc":"https://kernelgo.org/vfio-insight.html"},{"title":"Intel IOMMU Introduction","text":"对于Intel的硬件辅助虚拟化方案而言核心的两大技术分别是VT-x和VT-d。 其中VT-x中主要引入了non-root模式(VMCS)以及EPT页表等技术，主要关注于vCPU的虚拟化和内存虚拟化。 而VT-d的引入则是重点关注设备直通(passthrough)方面（即IO虚拟化）。 VT-x中在non-root模式下，MMU直接使用EPT page table来完成GPA->HVA->HPA的两级翻译， VT-d中在non-root模式下，则由IOMMU来使用Context Table和IOMMU page table完成设备DMA请求过程中的HPA->HVA->GPA的翻译． 二者极为相似，唯一的不同之处在于CPU访问内存（直通设备IO Memory）是通过MMU查找EPT页表完成地址翻译， 而直通设备访问内存的请求则是通过IOMMU查找IOMMU页表来完成地址翻译的。本文重点来探索一下Intel IOMMU的工作机制。 硬件结构 先看下一个典型的X86物理服务器视图： 在多路服务器上我们可以有多个DMAR Unit（这里可以直接理解为多个IOMMU硬件）， 每个DMAR会负责处理其下挂载设备的DMA请求进行地址翻译。 例如上图中， PCIE Root Port (dev:fun) (14:0)下面挂载的所有设备的DMA请求由DMAR #1负责处理， PCIE Root Port (dev:fun) (14:1)下面挂载的所有设备的DMA请求由DMAR #2负责处理， 而DMAR #3下挂载的是一个Root-Complex集成设备[29:0]，这个设备的DMA请求被DMAR #3承包， DMAR #4的情况比较复杂，它负责处理Root-Complex集成设备[30:0]以及I/OxAPIC设备的DMA请求。 这些和IOMMU相关的硬件拓扑信息需要BIOS通过ACPI表呈现给OS，这样OS才能正确驱动IOMMU硬件工作。 关于硬件拓扑信息呈现，这里有几个概念需要了解一下： DRHD: DMA Remapping Hardware Unit Definition 用来描述DMAR Unit(IOMMU)的基本信息 RMRR: Reserved Memory Region Reporting 用来描述那些保留的物理地址，这段地址空间不被重映射 ATSR: Root Port ATS Capability 仅限于有Device-TLB的情形，Root Port需要向OS报告支持ATS的能力 RHSA: Remapping Hardware Static Affinity Remapping亲和性，在有NUMA的系统下可以提升DMA Remapping的性能 BIOS通过在ACPI表中提供一套DMA Remapping Reporting Structure 信息来表述物理服务器上的IOMMU拓扑信息， 这样OS在加载IOMMU驱动的时候就知道如何建立映射关系了。 附：我们可以使用一些工具将ACPI表相关信息Dump出来查看 # acpidump --table DMAR -b > dmar.out # iasl -d dmar.out # cat dmar.dsl 数据结构 Intel IOMMU Driver的关键数据结构可以描述为（ 点击链接查看原图 ）： 按照自上而下的视图来看，首先是IOMMU硬件层面， struct dmar_drhd_unit数据结构从系统BIOS角度去描述了一个IOMMU硬件： list 用来把所有的DRHD串在一个链表中便于维护 acpi_dmar_head *hdr 指向IOMMU设备的ACPI表信息 device_cnt 表示当前IOMMU管理的设备数量 include_all 表示该IOMMU是否管理平台上所有的设备（单IOMMU的物理物理服务器） reg_base_addr 表示IOMMU的寄存器基地址 intel_iommu *iommu 指针指向struct intel_iommu数据结构 struct intel_iommu 进一步详细描述了IOMMU的所以相关信息 cap和ecap 记录IOMMU硬件的Capability和Extended Capability信息 root_entry 指向了此IOMMU的Root Entry Table ir_table 指向了IOMMU的Interrupt Remapping Table（中断重映射表） struct iommu_device iommu 从linux设备驱动的角度描述这个IOMMU并用来绑定sysfs struct dmar_domain ***domains 比较关键，它记录了这个IOMMU下面管理的所有dmar_domain信息 在虚拟化场景下多个设备可以直通给同一个虚拟机，他们共享一个IOMMU Page Table， 这种映射关系就是通过DMAR Domain来表述的， 也就是说多个直通设备可以加入到一个DMAR Domain中， 他们之间使用同一套页表完成地址DMA 请求的地址翻译。 那我们接着往下走，来看DMAR Domain： struct dmar_domain 数据结构用来描述DMAR Domain这种映射关系的 struct list_head devices 链表记录了这个Domain中的所有设备 struct iova_domain iovad 数据结构用一个红黑树来记录iova->hpa的地址翻译关系 struct dma_pte *pgd 这个指针指向了IOMMU页表的基地址是IOMMU页表的入口 bool has_iotlb_device 表示这个Domain里是否有具备IO-TLB的设备 struct iommu_domain domain 主要包含了iommu_ops *ops指针，记录了一堆与domain相关的操作 Intel IOMMU初始化 首先探测平台环境上是否有IOMMU硬件：IOMMU_INIT_POST(detect_intel_iommu)， detect_intel_iommu函数中调用dmar_table_detect函数从ACPI表中查询DMAR相关内容： /** * dmar_table_detect - checks to see if the platform supports DMAR devices */ static int __init dmar_table_detect ( void ) { acpi_status status = AE_OK ; /* if we could find DMAR table, then there are DMAR devices */ status = acpi_get_table ( ACPI_SIG_DMAR , 0 , & dmar_tbl ); if ( ACPI_SUCCESS ( status ) && ! dmar_tbl ) { pr_warn ( \"Unable to map DMAR \\n \" ); status = AE_NOT_FOUND ; } return ACPI_SUCCESS ( status ) ? 0 : - ENOENT ; } 如果查询到信息就validate_drhd_cb验证DRHD的有效性设置iommu_detected = 1， 如果查询不到DMAR信息那么认为没有IOMMU硬件，跳过后续初始化流程。 接着pci_iommu_init中调用x86_init.iommu.iommu_init()来初始化Intel IOMMU，主要的流程为： intel_iommu_init |-> dmar_table_init -> parse_dmar_table -> dmar_walk_dmar_table // 重点分析 |-> dmar_dev_scope_init |-> dmar_acpi_dev_scope_init -> dmar_acpi_insert_dev_scope // 重点分析 |-> dmar_pci_bus_add_dev -> dmar_insert_dev_scope |-> bus_register_notifier |-> dmar_init_reserved_ranges // init RMRR |-> init_no_remapping_devices // init no remapping devices |-> init_dmars // 重点分析 |-> dma_ops = & intel_dma_ops |-> iommu_device_sysfs_add , iommu_device_set_ops , iommu_device_register |-> bus_set_iommu ( & pci_bus_type , & intel_iommu_ops ) |-> bus_register_notifier ( & pci_bus_type , & device_nb ) 在dmar_table_init函数中我们完成了DMA Remapping相关的ACPI表解析流程，这个parse_dmar_table的函数实现非常精妙，不禁让人感叹！它将每种Remapping Structure Types的解析函数封装成dmar_res_callback，然后调用dmar_walk_dmar_table通过一个for循环撸一遍就完成了全部的解析，代码精简思路清晰、一气呵成。 static int __init parse_dmar_table ( void ) { struct acpi_table_dmar * dmar ; int drhd_count = 0 ; int ret ; struct dmar_res_callback cb = { . print_entry = true , . ignore_unhandled = true , . arg [ ACPI_DMAR_TYPE_HARDWARE_UNIT ] = & drhd_count , . cb [ ACPI_DMAR_TYPE_HARDWARE_UNIT ] = & dmar_parse_one_drhd , . cb [ ACPI_DMAR_TYPE_RESERVED_MEMORY ] = & dmar_parse_one_rmrr , . cb [ ACPI_DMAR_TYPE_ROOT_ATS ] = & dmar_parse_one_atsr , . cb [ ACPI_DMAR_TYPE_HARDWARE_AFFINITY ] = & dmar_parse_one_rhsa , . cb [ ACPI_DMAR_TYPE_NAMESPACE ] = & dmar_parse_one_andd , }; /* * Do it again, earlier dmar_tbl mapping could be mapped with * fixed map. */ dmar_table_detect (); // 重新detect dmar table /* * ACPI tables may not be DMA protected by tboot, so use DMAR copy * SINIT saved in SinitMleData in TXT heap (which is DMA protected) */ dmar_tbl = tboot_get_dmar_table ( dmar_tbl ); dmar = ( struct acpi_table_dmar * ) dmar_tbl ; if ( ! dmar ) return - ENODEV ; if ( dmar -> width < PAGE_SHIFT - 1 ) { pr_warn ( \"Invalid DMAR haw \\n \" ); return - EINVAL ; } pr_info ( \"Host address width %d \\n \" , dmar -> width + 1 ); ret = dmar_walk_dmar_table ( dmar , & cb ); //遍历ACPI表完成解析 if ( ret == 0 && drhd_count == 0 ) pr_warn ( FW_BUG \"No DRHD structure found in DMAR table \\n \" ); return ret ; } dmar_dev_scope_init函数负责完成IOMMU的Device Scope解析。 dmar_acpi_insert_dev_scope中多层的遍历，建立了IOMMU和设备之间的映射关系。 static void __init dmar_acpi_insert_dev_scope ( u8 device_number , struct acpi_device * adev ) { struct dmar_drhd_unit * dmaru ; struct acpi_dmar_hardware_unit * drhd ; struct acpi_dmar_device_scope * scope ; struct device * tmp ; int i ; struct acpi_dmar_pci_path * path ; for_each_drhd_unit ( dmaru ) { drhd = container_of ( dmaru -> hdr , struct acpi_dmar_hardware_unit , header ); for ( scope = ( void * )( drhd + 1 ); ( unsigned long ) scope < (( unsigned long ) drhd ) + drhd -> header . length ; scope = (( void * ) scope ) + scope -> length ) { if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_NAMESPACE ) continue ; if ( scope -> enumeration_id != device_number ) continue ; path = ( void * )( scope + 1 ); pr_info ( \"ACPI device \\\" %s \\\" under DMAR at %llx as %02x:%02x.%d \\n \" , dev_name ( & adev -> dev ), dmaru -> reg_base_addr , scope -> bus , path -> device , path -> function ); for_each_dev_scope ( dmaru -> devices , dmaru -> devices_cnt , i , tmp ) if ( tmp == NULL ) { dmaru -> devices [ i ]. bus = scope -> bus ; dmaru -> devices [ i ]. devfn = PCI_DEVFN ( path -> device , path -> function ); rcu_assign_pointer ( dmaru -> devices [ i ]. dev , get_device ( & adev -> dev )); return ; } BUG_ON ( i >= dmaru -> devices_cnt ); } } pr_warn ( \"No IOMMU scope found for ANDD enumeration ID %d (%s) \\n \" , device_number , dev_name ( & adev -> dev )); } init_dmars函数最后再对描述IOMMU的intel_iommu结构进行初始化，主要的流程包括： init_dmars |-> intel_iommu_init_qi // qeueu invalidation |-> iommu_init_domains |-> init_translation_status |-> iommu_alloc_root_entry // 创建 Root Entry |-> translation_pre_enabled |-> iommu_set_root_entry |-> iommu_prepare_rmrr_dev |-> iommu_prepare_isa // 0 - 16 MiB 留给 ISA设备 |-> dmar_set_interrupt // IOMMU中断初始化 这里不再展开，但每个点都值得探索一下，例如： IOMMU中断是用来做什么的？ iommu_prepare_identity_map 是在做什么？ 一个IOMMU最多支持多少个DMAR Domain？ qeueue invalidation是用来做什么的？ 可以多问自己一些问题带着问题去看代码，从代码中找到答案，从更深层次去分析问题，理解特性。 参考文献 https://software.intel.com/sites/default/files/managed/c5/15/vt-directed-io-spec.pdf https://elixir.bootlin.com/linux/v4.16.12/source/drivers/iommu/intel-iommu.c","tags":"virtualization","url":"https://kernelgo.org/intel_iommu.html","loc":"https://kernelgo.org/intel_iommu.html"},{"title":"MMIO Emulation","text":"我们知道X86体系结构上对设备进行访问可以通过PIO方式和MMIO(Memory Mapped I/O)两种方式进行， 那么QEMU-KVM具体是如何实现设备MMIO访问的呢？ MMIO是直接将设备I/O映射到物理地址空间内，虚拟机物理内存的虚拟化又是通过EPT机制来完成的， 那么模拟设备的MMIO实现也需要利用EPT机制．虚拟机的EPT页表是在 EPT_VIOLATION 异常处理的时候建立起来的， 对于模拟设备而言访问MMIO肯定要触发 VM_EXIT 然后交给QEMU/KVM去处理，那么怎样去标志MMIO访问异常呢？ 查看Intel SDM知道这是通过利用 EPT_MISCONFIG 来实现的．那么 EPT_VIOLATION 与 EPT_MISCONFIG 的区别是什么? EXIT_REASON_EPT_VIOLATION is similar to a \"page not present\" pagefault . EXIT_REASON_EPT_MISCONFIG is similar to a \"reserved bit set\" pagefault . EPT_VIOLATION 表示的是对应的物理页不存在，而 EPT_MISCONFIG 表示EPT页表中有非法的域． 那么这里有２个问题需要弄清楚． 1 KVM如何标记EPT是MMIO类型 ? hardware_setup时候虚拟机如果开启了ept支持就调用ept_set_mmio_spte_mask初始化shadow_mmio_mask， 设置EPT页表项最低3bit为：110b就会触发ept_msconfig（110b表示该页可读可写但是还未分配或者不存在，这显然是一个错误的EPT页表项）. static void ept_set_mmio_spte_mask ( void ) { /* * EPT Misconfigurations can be generated if the value of bits 2:0 * of an EPT paging-structure entry is 110b (write/execute). */ kvm_mmu_set_mmio_spte_mask ( VMX_EPT_RWX_MASK , VMX_EPT_MISCONFIG_WX_VALUE ); } 同时还要对EPT的一些特殊位进行标记来标志该spte表示MMIO而不是虚拟机的物理内存，例如这里 ( 1 ) set the special mask : SPTE_SPECIAL_MASK ． ( 2 ) reserved physical address bits : the setting of a bit in the range 51 : 12 that is beyond the logical processor ' s physical - address width 关于EPT_MISCONFIG在SDM中有详细说明． void kvm_mmu_set_mmio_spte_mask ( u64 mmio_mask , u64 mmio_value ) { BUG_ON (( mmio_mask & mmio_value ) != mmio_value ); shadow_mmio_value = mmio_value | SPTE_SPECIAL_MASK ; shadow_mmio_mask = mmio_mask | SPTE_SPECIAL_MASK ; } EXPORT_SYMBOL_GPL ( kvm_mmu_set_mmio_spte_mask ); static void kvm_set_mmio_spte_mask ( void ) { u64 mask ; int maxphyaddr = boot_cpu_data . x86_phys_bits ; /* * Set the reserved bits and the present bit of an paging-structure * entry to generate page fault with PFER.RSV = 1. */ /* Mask the reserved physical address bits. */ mask = rsvd_bits ( maxphyaddr , 51 ); /* Set the present bit. */ mask |= 1ull ; #ifdef CONFIG_X86_64 /* * If reserved bit is not supported, clear the present bit to disable * mmio page fault. */ if ( maxphyaddr == 52 ) mask &= ~ 1ull ; #endif kvm_mmu_set_mmio_spte_mask ( mask , mask ); } KVM在建立EPT页表项之后设置了这些标志位再访问对应页的时候会触发EPT_MISCONFIG退出了，然后调用handle_ept_misconfig-->handle_mmio_page_fault来完成MMIO处理操作． KVM内核相关代码 ： handle_ept_misconfig --> kvm_emulate_instruction --> x86_emulate_instruction --> x86_emulate_insn writeback --> segmented_write --> emulator_write_emulated --> emulator_read_write --> emulator_read_write_onepage --> ops -> read_write_mmio [ write_mmio ] --> vcpu_mmio_write --> kvm_io_bus_write --> __kvm_io_bus_write --> kvm_iodevice_write --> dev -> ops -> write [ ioeventfd_write ] 最后会调用到 ioeventfd_write ，写 eventfd给QEMU发送通知事件 /* MMIO/PIO writes trigger an event if the addr/val match */ static int ioeventfd_write ( struct kvm_vcpu * vcpu , struct kvm_io_device * this , gpa_t addr , int len , const void * val ) { struct _ioeventfd * p = to_ioeventfd ( this ); if ( ! ioeventfd_in_range ( p , addr , len , val )) return - EOPNOTSUPP ; eventfd_signal ( p -> eventfd , 1 ); return 0 ; } 2 QEMU如何标记设备的MMIO ? 这里以e1000网卡模拟为例，设备初始化MMIO时候时候注册的MemoryRegion为IO类型（不是RAM类型）． static void e1000_mmio_setup ( E1000State * d ) { int i ; const uint32_t excluded_regs [] = { E1000_MDIC , E1000_ICR , E1000_ICS , E1000_IMS , E1000_IMC , E1000_TCTL , E1000_TDT , PNPMMIO_SIZE }; // 这里注册MMIO，调用memory_region_init_io，mr->ram = false！！！ memory_region_init_io ( & d -> mmio , OBJECT ( d ), & e1000_mmio_ops , d , \"e1000-mmio\" , PNPMMIO_SIZE ); memory_region_add_coalescing ( & d -> mmio , 0 , excluded_regs [ 0 ]); for ( i = 0 ; excluded_regs [ i ] != PNPMMIO_SIZE ; i ++ ) memory_region_add_coalescing ( & d -> mmio , excluded_regs [ i ] + 4 , excluded_regs [ i + 1 ] - excluded_regs [ i ] - 4 ); memory_region_init_io ( & d -> io , OBJECT ( d ), & e1000_io_ops , d , \"e1000-io\" , IOPORT_SIZE ); } 结合QEMU-KVM内存管理知识我们知道， QEMU调用kvm_set_phys_mem注册虚拟机的物理内存到KVM相关的数据结构中的时候 会调用memory_region_is_ram来判断该段物理地址空间是否是RAM设备， 如果不是RAM设备直接return了． static void kvm_set_phys_mem ( KVMMemoryListener * kml , MemoryRegionSection * section , bool add ) { ...... if ( ! memory_region_is_ram ( mr )) { if ( writeable || ! kvm_readonly_mem_allowed ) { return ; // 设备MR不是RAM但可以写，那么这里直接return不注册到kvm里面 } else if ( ! mr -> romd_mode ) { /* If the memory device is not in romd_mode, then we actually want * to remove the kvm memory slot so all accesses will trap. */ add = false ; } } ...... } 对于MMIO类型的内存QEMU不会调用kvm_set_user_memory_region对其进行注册， 那么KVM会认为该段内存的pfn类型为KVM_PFN_NOSLOT， 进而调用set_mmio_spte来设置该段地址对应到spte， 而该函数中会判断pfn是否为NOSLOT标记以确认这段地址空间为MMIO． static bool set_mmio_spte ( struct kvm_vcpu * vcpu , u64 * sptep , gfn_t gfn , kvm_pfn_t pfn , unsigned access ) { if ( unlikely ( is_noslot_pfn ( pfn ))) { mark_mmio_spte ( vcpu , sptep , gfn , access ); return true ; } return false ; } 3 总结 MMIO是通过设置spte的保留位来标志的． 虚拟机内部第一次访问MMIO的gpa时，发生了EPT_VIOLATION然后check gpa发现对应的pfn不存在（QEMU没有注册），那么认为这是个MMIO，于是set_mmio_spte来标志它的spte是一个MMIO． 后面再次访问这个gpa时就发生EPT_MISCONFIG了，进而愉快地调用handle_ept_misconfig -> handle_mmio_page_fault -> x86_emulate_instruction 来处理所有的MMIO操作了．","tags":"virtualization","url":"https://kernelgo.org/mmio.html","loc":"https://kernelgo.org/mmio.html"},{"title":"VFIO Introduction","text":"Virtual Function I/O (VFIO) 是一种现代化的设备直通方案，它充分利用了VT-d/AMD-Vi技术提供的DMA Remapping和Interrupt Remapping特性， 在保证直通设备的DMA安全性同时可以达到接近物理设备的I/O的性能。 用户态进程可以直接使用VFIO驱动直接访问硬件，并且由于整个过程是在IOMMU的保护下进行因此十分安全， 而且非特权用户也是可以直接使用。 换句话说，VFIO是一套完整的用户态驱动(userspace driver)方案，因为它可以安全地把设备I/O、中断、DMA等能力呈现给用户空间。 为了达到最高的IO性能，虚拟机就需要VFIO这种设备直通方式，因为它具有低延时、高带宽的特点，并且guest也能够直接使用设备的原生驱动。 这些优异的特点得益于VFIO对VT-d/AMD-Vi所提供的DMA Remapping和Interrupt Remapping机制的应用。 VFIO使用DMA Remapping为每个Domain建立独立的IOMMU Page Table将直通设备的DMA访问限制在Domain的地址空间之内保证了用户态DMA的安全性， 使用Interrupt Remapping来完成中断重映射和Interrupt Posting来达到中断隔离和中断直接投递的目的。 1. VFIO 框架简介 整个VFIO框架设计十分简洁清晰，可以用下面的一幅图描述： + -------------------------------------------+ | | | VFIO Interface | | | + ---------------------+---------------------+ | | | | vfio_iommu | vfio_pci | | | | + ---------------------+---------------------+ | | | | iommu driver | pci_bus driver | | | | + ---------------------+---------------------+ 最上层的是VFIO Interface Layer，它负责向用户态提供统一访问的接口，用户态通过约定的ioctl设置和调用VFIO的各种能力。 中间层分别是vfio_iommu和vfio_pci，vfio_iommu是VFIO对iommu层的统一封装主要用来实现DMAP Remapping的功能，即管理IOMMU页表的能力。 vfio_pci是VFIO对pci设备驱动的统一封装，它和用户态进程一起配合完成设备访问直接访问，具体包括PCI配置空间模拟、PCI Bar空间重定向，Interrupt Remapping等。 最下面的一层则是硬件驱动调用层，iommu driver是与硬件平台相关的实现，例如它可能是intel iommu driver或amd iommu driver或者ppc iommu driver， 而同时vfio_pci会调用到host上的pci_bus driver来实现设备的注册和反注册等操作。 在了解VFIO之前需要了解3个基本概念：device, group, container，它们在逻辑上的关系如上图所示。 Group 是IOMMU能够进行DMA隔离的最小硬件单元，一个group内可能只有一个device，也可能有多个device，这取决于物理平台上硬件的IOMMU拓扑结构。 设备直通的时候一个group里面的设备必须都直通给一个虚拟机。 不能够让一个group里的多个device分别从属于2个不同的VM，也不允许部分device在host上而另一部分被分配到guest里， 因为就这样一个guest中的device可以利用DMA攻击获取另外一个guest里的数据，就无法做到物理上的DMA隔离。 另外，VFIO中的group和iommu group可以认为是同一个概念。 Device 指的是我们要操作的硬件设备，不过这里的\"设备\"需要从IOMMU拓扑的角度去理解。如果该设备是一个硬件拓扑上独立的设备，那么它自己就构成一个iommu group。 如果这里是一个multi-function设备，那么它和其他的function一起组成一个iommu group，因为多个function设备在物理硬件上就是互联的， 他们可以互相访问对方的数据，所以必须放到一个group里隔离起来。值得一提的是，对于支持PCIe ACS特性的硬件设备，我们可以认为他们在物理上是互相隔离的。 Container 是一个和地址空间相关联的概念，这里可以简单把它理解为一个VM Domain的物理内存空间。 从上图可以看出，一个或多个device从属于某个group，而一个或多个group又从属于一个container。 如果要将一个device直通给VM，那么先要找到这个设备从属的iommu group，然后将整个group加入到container中即可。关于如何使用VFIO可以参考内核文档： vfio.txt 2. VFIO 数据结构关系 Linux内核设备驱动充分利用了\"一切皆文件\"的思想，VFIO驱动也不例外，VFIO中为了方便操作device, group, container等对象将它们和对应的设备文件进行绑定。 VFIO驱动在加载的时候会创建一个名为 /dev/vfio/vfio 的文件，而这个文件的句柄关联到了vfio_container上，用户态进程打开这个文件就可以初始化和访问vfio_container。 当我们把一个设备直通给虚拟机时，首先要做的就是将这个设备从host上进行解绑，即解除host上此设备的驱动，然后将设备驱动绑定为\"vfio-pci\"， 在完成绑定后会新增一个 /dev/vfio/$groupid 的文件，其中$groupid为此PCI设备的iommu group id， 这个id号是在操作系统加载iommu driver遍历扫描host上的PCI设备的时候就已经分配好的，可以使用 readlink -f /sys/bus/pci/devices/$bdf/iommu_group 来查询。 类似的， /dev/vfio/$groupid 这个文件的句柄被关联到vfio_group上，用户态进程打开这个文件就可以管理这个iommu group里的设备。 然而VFIO中并没有为每个device单独创建一个文件，而是通过VFIO_GROUP_GET_DEVICE_FD这个ioctl来获取device的句柄，然后再通过这个句柄来管理设备。 VFIO框架中很重要的一部分是要完成DMA Remapping，即为Domain创建对应的IOMMU页表，这个部分是由vfio_iommu_driver来完成的。 vfio_container包含一个指针记录vfio_iommu_driver的信息，在x86上vfio_iommu_driver的具体实现是由vfio_iommu_type1来完成的。 其中包含了vfio_iommu, vfio_domain, vfio_group, vfio_dma等关键数据结构（注意这里是iommu里面的）， vfio_iommu可以认为是和container概念相对应的iommu数据结构，在虚拟化场景下每个虚拟机的物理地址空间映射到一个vfio_iommu上。 vfio_group可以认为是和group概念对应的iommu数据结构，它指向一个iommu_group对象，记录了着iommu_group的信息。 vfio_domain这个概念尤其需要注意，这里绝不能把它理解成一个虚拟机domain，它是一个与DRHD（即IOMMU硬件）相关的概念， 它的出现就是为了应对多IOMMU硬件的场景，我们知道在大规格服务器上可能会有多个IOMMU硬件，不同的IOMMU硬件有可能存在差异， 例如IOMMU 0支持IOMMU_CACHE而IOMMU 1不支持IOMMU_CACHE（当然这种情况少见，大部分平台上硬件功能是具备一致性的），这时候我们不能直接将分别属于不同IOMMU硬件管理的设备直接加入到一个container中， 因为它们的IOMMU页表SNP bit是不一致的。 因此，一种合理的解决办法就是把一个container划分多个vfio_domain，当然在大多数情况下我们只需要一个vfio_domain就足够了。 处在同一个vfio_domain中的设备共享IOMMU页表区域，不同的vfio_domain的页表属性又可以不一致，这样我们就可以支持跨IOMMU硬件的设备直通的混合场景。 经过上面的介绍和分析，我们可以把VFIO各个组件直接的关系用下图表示( 点击链接查看原图 )，读者可以按照图中的关系去阅读相关代码实现。 3. VFIO 中的技术关键点 除了DMA Remapping这一关键点之外，在虚拟化场景下VFIO还需要解决下面一些关键问题，需要进行探讨： VFIO对完备的设备访问支持：其中包括MMIO， I/O Port，PCI 配置空间，PCI BAR空间； VFIO中高效的设备中断机制，其中包括MSI/MSI-X，Interrupt Remapping，以及Posted Interrupt等； VFIO对直通设备热插拔支持。","tags":"virtualization","url":"https://kernelgo.org/vfio-introduction.html","loc":"https://kernelgo.org/vfio-introduction.html"},{"title":"VT-d DMA Remapping","text":"本文主要探讨一下VT-d DMA Remapping机制。在分析DMA Remapping之前回顾下什么是DMA，DMA是指在不经过CPU干预的情况下外设直接访问(Read/Write)主存(System Memroy)的能力。 DMA带来的最大好处是：CPU不再需要干预外设对内存的访问过程，而是可以去做其他的事情，这样就大大提高了CPU的利用率。 在设备直通(Device Passthough)的虚拟化场景下，直通设备在工作的时候同样要使用DMA技术来访问虚拟机的主存以提升IO性能。那么问题来了，直接分配给某个特定的虚拟机的，我们必须要保证直通设备DMA的安全性，一个VM的直通设备不能通过DMA访问到其他VM的内存，同时也不能直接访问Host的内存，否则会造成极其严重的后果。因此，必须对直通设备进行\"DMA隔离\"和\"DMA地址翻译\"，隔离将直通设备的DMA访问限制在其所在VM的物理地址空间内保证不发生访问越界，地址翻译则保证了直通设备的DMA能够被正确重定向到虚拟机的物理地址空间内。为什么直通设备会存在DMA访问的安全性问题呢？原因也很简单：由于直通设备进行DMA操作的时候guest驱动直接使用gpa来访问内存的，这就导致如果不加以隔离和地址翻译必然会访问到其他VM的物理内存或者破坏Host内存，因此必须有一套机制能够将gpa转换为对应的hpa这样直通设备的DMA操作才能够顺利完成。 VT-d DMA Remapping的引入就是为了解决直通设备DMA隔离和DMA地址翻译的问题，下面我们将对其原理进行分析，主要参考资料是 Intel VT-d SPEC Chapter 3 。 1 DMA Remapping 简介 VT-d DMA Remapping的硬件能力主要是由IOMMU来提供，通过引入根Context Entry和IOMMU Domain Page Table等机制来实现直通设备隔离和DMA地址转换的目的。那么具体是怎样实现的呢？下面将对其进行介绍。 根据DMA Request是否包含地址空间标志(address-space-identifier)我们将DMA Request分为2类： Requests without address-space-identifier: 不含地址空间标志的DMA Request，这种一般是endpoint devices的普通请求，请求内容仅包含请求的类型(read/write/atomics)，DMA请求的address/size以及请求设备的标志符等。 Requests with address-space-identifier: 包含地址空间描述标志的DMA Request，此类请求需要包含额外信息以提供目标进程的地址空间标志符(PASID)，以及Execute-Requested (ER) flag和 Privileged-mode-Requested 等细节信息。 为了简单，通常称上面两类DMA请求简称为：Requests-without-PASID和Requests-with-PASID。本节我们只讨论Requests-without-PASID，后面我们会在讨论Shared Virtual Memory的文中单独讨论Requests-with-PASID。 首先要明确的是DMA Isolation是以Domain为单位进行隔离的，在虚拟化环境下可以认为每个VM的地址空间为一个Domain，直通给这个VM的设备只能访问这个VM的地址空间这就称之为\"隔离\"。根据软件的使用模型不同，直通设备的DMA Address Space可能是某个VM的Guest Physical Address Space或某个进程的虚拟地址空间（由分配给进程的PASID定义）或是由软件定义的一段抽象的IO Virtual Address space (IOVA)，总之DMA Remapping就是要能够将设备发起的DMA Request进行DMA Translation重映射到对应的HPA上。下面的图描述了DMA Translation的原理，这和MMU将虚拟地址翻译成物理地址的过程非常的类似。 值得一提的是，Host平台上可能会存在一个或者多个DMA Remapping硬件单元，而每个硬件单元支持在它管理的设备范围内的所有设备的DMA Remapping。例如，你的台式机CPU Core i7 7700k在MCH中只集成一个DMA Remapping硬件单元(IOMMU)，但在多路服务器上可能集成有多个DMA Remapping硬件单元。每个硬件单元负责管理挂载到它所在的PCIe Root Port下所有设备的DMA请求。BIOS会将平台上的DMA Remapping硬件信息通过ACPI协议报告给操作系统，再由操作系统来初始化和管理这些硬件设备。 为了实现DMA隔离，我们需要对直通设备进行标志，而这是通过PCIe的Request ID来完成的。根据PCIe的SPEC，每个PCIe设备的请求都包含了PCI Bus/Device/Function信息，通过BDF号我们可以唯一确定一个PCIe设备。 同时为了能够记录直通设备和每个Domain的关系，VT-d引入了root-entry/context-entry的概念，通过查询root-entry/context-entry表就可以获得直通设备和Domain之间的映射关系。 Root-table是一个4K页，共包含了256项root-entry，分别覆盖了PCI的Bus0-255，每个root-entry占16-Byte，记录了当前PCI Bus上的设备映射关系，通过PCI Bus Number进行索引。 Root-table的基地址存放在Root Table Address Register当中。Root-entry中记录的关键信息有： Present Flag：代表着该Bus号对应的Root-Entry是否呈现，CTP域是否初始化； Context-table pointer (CTP)：CTP记录了当前Bus号对应点Context Table的地址。 同样每个context-table也是一个4K页，记录一个特定的PCI设备和它被分配的Domain的映射关系，即对应Domain的DMA地址翻译结构信息的地址。 每个root-entry包含了该Bus号对应的context-table指针，指向一个context-table，而每张context-table包又含256个context-entry， 其中每个entry对应了一个Device Function号所确认的设备的信息。通过2级表项的查询我们就能够获得指定PCI被分配的Domain的地址翻译结构信息。Context-entry中记录的信息有： Present Flag：表示该设备对应的context-entry是否被初始化，如果当前平台上没有该设备Preset域为0，索引到该设备的请求也会被block掉。 Translation Type：表示哪种请求将被允许； Address Width：表示该设备被分配的Domain的地址宽度； Second-level Page-table Pointer：二阶页表指针提供了DMA地址翻译结构的HPA地址（这里仅针对Requests-without-PASID而言）； Domain Identifier: Domain标志符表示当前设备的被分配到的Domain的标志，硬件会利用此域来标记context-entry cache，这里有点类似VPID的意思； Fault Processing Disable Flag：此域表示是否需要选择性的disable此entry相关的remapping faults reporting。 因为多个设备有可能被分配到同一个Domain，这时只需要将其中每个设备context-entry项的 Second-level Page-table Pointer 设置为对同一个Domain的引用， 并将Domain ID赋值为同一个Domian的就行了。 2 DMA隔离和地址翻译 VT-d中引入root-table和context-table的目的比较明显，这些额外的table的存在就是为了记录每个直通设备和其被分配的Domain之间的映射关系。 有了这个映射关系后，DMA隔离的实现就变得非常简单。 IOMMU硬件会截获直通设备发出的请求，然后根据其Request ID查表找到对应的Address Translation Structure即该Domain的IOMMU页表基地址， 这样一来该设备的DMA地址翻译就只会按这个Domain的IOMMU页表的方式进行翻译，翻译后的HPA必然落在此Domain的地址空间内（这个过程由IOMMU硬件中自动完成）， 而不会访问到其他Domain的地址空间，这样就达到了DMA隔离的目的。 DMA地址翻译的过程和虚拟地址翻译的过程是完全一致的，唯一不同的地方在于MMU地址翻译是将进程的虚拟地址(HVA)翻译成物理地址(HPA)，而IOMMU地址翻译则是将虚拟机物理地址空间内的GPA翻译成HPA。IOMMU页表和MMU页表一样，都采用了多级页表的方式来进行翻译。例如，对于一个48bit的GPA地址空间的Domain而言，其IOMMU Page Table共分4级，每一级都是一个4KB页含有512个8-Byte的目录项。和MMU页表一样，IOMMU页表页支持2M/1G大页内存，同时硬件上还提供了IO-TLB来缓存最近翻译过的地址来提升地址翻译的速度。","tags":"virtualization","url":"https://kernelgo.org/dma-remapping.html","loc":"https://kernelgo.org/dma-remapping.html"},{"title":"VT-d Posted Interrupt","text":"VT-d Interrupt Remapping的引入改变了以往设备中断的投递方式，Remapping格式的中断请求不再包含目标CPU的APIC-ID、中断vector号、投递方式等重要信息，而是仅仅提供了一个16 bit的interrupt_index用来索引中断重定向表项(IRTE)，这个改变带来的最大好处是提升了中断处理的灵活性。在虚拟化的环境下，为了提升虚拟机的中断实时性，Intel在Interrupt Remapping的基础上加以改进 引入了Interrupt Posting机制，从硬件层面实现了中断隔离和中断自动迁移等重要特性。 1 Interrupt Posting 简介 VT-d Interrupt Posting是基于Interrupt Remapping的一种扩展的中断处理方式，其主要用途是在虚拟化场景下，可以大幅提升VMM处理直通设备中断的效率。硬件通过Capability Register(CAP_REG)的PI位来报告interrupt posting capability。 根据前面介绍 Interrupt Remapping 的文章可以知道，所有的Remapping格式中断请求都需要通过中断重映射表来投递，IRTE中的Mode域(IM)用来指定这个remappable中断请求是interrupt-remapping方式还是interrupt-posting方式。 IRTE的IM位为0表示中断按照remappable方式处理； IRTE的IM位为1表示中断按照posted方式来处理。 在Interrupt Posting模式下，新增了一个与VCPU相关的内存数据结构叫做\"Posted Interrupt Descriptor\"(PD)，这是一个64-Byte对齐的数据结构并且直接被硬件用来记录将要post的中断请求。PD结构包含以下的域： Posted Interrupt Request (PIR)域，提供记录需要post的中断占256bit每个bit代表一个中断号。 Outstanding Notification (ON)域，由硬件来自动更新，用来表示是否有中断请求pending。当此位为0时，硬件通过修改其为1来产生一个通知事件告知中断请求到来。接收这个通知事件的实体(处理器或者软件)在处理这个posted interrupt时后必须将其清零。 Suppress Notification (SN)域，表示non-urgent中断请求的通知事件是否要被supressed(抑制)。 Notification Vector (NV)域，用来指定产生posted-interrupt\"通知事件\"(notification event)的vector号。 Notification Destination (NDST)域，用来指定此中断要投递的VCP所运行物理CPU的APIC-ID。 在Interrupt Posting模式下IRTE格式相当于Remapping模式有很大不同（参考附录），IRTE的格式相对于Remapping模式新增了以下几个域： 中断请求对应的Posted Interrupt Descriptor数据结构地址，包含高地址和低地址2个域； Urgent (URG)标志来指定中断请求是否需要实时处理； 一个用来指定要post的vector号的Vector域，与Remapping格式不同的是posted-format 的IRTEs的Vector域是用来决定Posted Interrupt Descriptor里的PIR域的哪个bit要置位。 2 Interrupt Posting 的硬件处理步骤 当一个Remapping格式的中断请求IM位为1时，意味着这个中断请求要按照Interrupt Posting方式进行处理。整个过程中断硬件处理流程如下: 如果中断请求索引到的IRTE的IM位被置位(1b)： 硬件按照posted format解读IRTE，如果IRTE的格式检查不通过，那么该请求被blocked。如果检查通过从IRTE中提取Posted Interrupt Descriptor的地址(PDA-L/PDA-H)，中断请求的vector号以及中断请求是否为URG等信息。 硬件会对Posted Interrupt Descriptor内存数据结构执行一个read-modify-write原子操作： 首先读取PD的内容并对其进行检测，如果发现格式不对（例如reserved域不为0）那么将该请求block掉。如果检测通过那么获取当前的PIR,ON,NV,NDST域信息后，按照下面的规则对PD进行原子更新： 根据IRTE的Vecotr域设置PIR对应的bit 计算出 X = ((ON == 0) & (URG | (SN == 0))), 如果X==1那么把ON置位。 如果X==1，那么产生一个\"通知事件中断\"，并且这个中断的属性为： NSDT表示VCPU所在的CPU的physical APIC-ID (注意：xAPIC和x2APIC模式下的不同) NV域指定了被用来通知目的CPU有个posted-interrupt已在pending的\"通知事件\"的中断向量。（注意不是要post的中断请求vector号，这个仅仅用做通知用） Delivery mode域被强制设定为Fixed (000b) Re-direction Hint域强制清零 (0b) Triger Mode域被设置为Edge (0b) Trigger Mode Level域被设置为Asserted (1b) 3 Interrupt Posting 的软件处理步骤 当一个设备被直通给虚拟机后，虚拟机初始化的过程中VMM会设置好此设备的MSI/MSI-X中断对应的IRTE并标志IM位为1b，标志这是一个Posted Interrupt。当直通设备投递一个中断后，硬件首先会去查询irq对应的IRTE并从IRTE中提取记录的Posted Interrupt Descriptor地址和vector信息，然后更新PIR域和ON域并且将vector信息写入到VCPU的vAPIC Page中，直接给处于None Root模式的VCPU注入一个中断，整个过程不需要VMM的介入从而十分高效。Intel的虚拟化专家FengWu使用下面的图很好的描述了Interrupt Posting的处理过程： 从上面的描述来看，Interrupt Posting是不是看起来很简单？然而，实际实现上却还是要复杂多，不过也不要被吓到额！ 从软件层面来说，VMM需要参与进来做以下一些额外的工作来使能Interrupt Posting机制： 为虚拟机的每个VCPU分配一个PD用来存放此VCPU的Posted Interrupt信息（PD的地址会被记录到VCPU的VMSC里面）； VMM需要在每个PCPU上安排2个中断vector用来接受通知事件： 其中一个物理vector被称之为'Active Notification Vector' (ANV)，它被用来post通知事件到处于Running状态的VCPU上（这个IPI中断是guest接收的）。 另一个物理vector被称之为'Wake-up Notification Vector' (WNV)，它被用来post通知事件到处于Blocked状态的VCPU上（这个IPI中断是host接收的）。 对于直通到此虚拟机的直通设备，VMM都会干预进来（因为虚拟机的IOxAPIC,LAPIC等都是kvm内核模块来模拟的），VMM能够知道到每个VCPU上的vector号分配情况； 对于每个直通设备的中断： VMM会为每个中断源分配一个IRTE，并且把对应的guest分配的vecotr号填入到IRTE的vector域。 VMM会将每个VCPU对应的PD地址填入到此中断源的对用的IRTE地址域。 如果此中断需要立即处理，那么VMM会将对此中断源对应的IRTE中URG域置成1。 同时VMM还需要为VCPU使能APICv特性（包括了'virtual-interrupt delivery'和'process posted interrupts'），并且将此VCPU的VMCS域POSTED_INTR_NV配置为ANV，并将申请的PD的地址配置到VMCS的POSTED_INTR_DESC_ADDR域来告诉VCPU它关联的PD在哪儿。（注：这些操作在VCPU初始化流程中完成） 在VCPU调度的过程中，VMM需要按照下面的方式来管理VCPU的调度状态： 当VCPU被scheduler选中调度进来运行的的时候，此时VCPU的状态被标志为'Active'状态。这个时候VMM需要将PD的NV域更新为ANV的值。 同时在这种场景下，此VCPU上接受的Posted Interrupt中断会被直接复制到vAPIC Page中，guest在非根模式下就能直接处理此中断，而不需要VMM的参与。 当一个VCPU被抢占（Preempted），例如时间片到期了，这时候需要将PD的SN域标志为1，即将VCPU更新为'Preempted'状态，告诉硬件当前VCPU已经没在非根模式下运行了。 此时，这个VCPU上的non-urgent中断都会被接受但不会产生通知事件。 但如果这个VCPU上有标志为URG类型的中断时，VMM同时也会将PD的NV域修改为WNV，这样一来VMM就能够将URG中断请求投递给处于not running状态的VCPU，并进行适当的软件处理（例如，抢占正在同一个物理CPU上运行状态的其他VCPU，并将自己调度进来）。 当一个VCPU执行了hlt指令或者触发了ple，VMM也会干预进来将VCPU给block出来，并且将VCPU状态标识为Hlted状态。在此状态下VMM需要将VCPU对应的PD的NV域设置为WNV。这样一来，当中断请求post到此VCPU时，VMM能够接受到Wake-up Notification Event事件通知并做出适当的软件操作。（例如：立即对此VCPU进行一次调度） 当VCPU重新进入非根模式或者从hlt恢复执行时（注意这个时候vCPU还没进入根模式下），VMM对此VCPU上处于pending状态的posted interrupt进行处理： 首先将PD的NV域设置为ANV以标志VCPU为Active状态； 扫描PD的PIR域检测是否有处于pending状态的posted interrupt请求； 如果有处于pending状态的posted interrupt请求，VMM会在LAPIC上生成一个vector号为ANV的self-IPI(注意：在还未真正enter guest之前当前CPU处于关中断状态)。 那么当VCPU刚刚打开中断，准备进入到非根模式下的时候，就立刻接受到一个self-IPI，那么处理器硬件这时候就会将它当做posted-interrupt通知事件来处理，立刻从LAPIC中读取pending的中断并进行处理。 这样的好处是将guest对于posted interrupt的处理完全off load到处理器硬件上。 VMM同样能够将 posted interrupt processing 技术应用到模拟设备产生的虚拟设备中断处理上（不仅仅是直通设备额）。而这只需VMM执行原子操作'post'一个虚拟中断到PD上，并给PD中NDST的逻辑CPU发送一个IPI作为通知事件告诉该CPU有posted interrupt到来。（这里说的是VT-x的Posted Interrupt） 当VCPU在不同PCPU之间进行迁移时，VMM会对VCPU对应的PD的NDST域进行更新，将NDST改为VCPU要迁移的目的PCPU的APIC-ID。这也就是说，在VCPU迁移的过程中我们也顺便完成了中断迁移。这样一来，新的posted interrupt 通知事件到来时就会被自动路由的新的PCPU上，是不是很巧妙？ 备注：请思考，ANV和WNV通知事件的目的分别是什么？ ANV事件通知的目的是： 当vCPU被抢占或者处于Post Block阶段，这个时候vCPU是处于Runnable状态（已经被调度器选中，但还没进入非根模式enter_guest），这个时候如果收到直通设备中断，那么就需要ANV事件来告知一下，这样在vCPU进入非根模式下能够立刻处理这个中断。 WNV事件通知的目的是：当vCPU被Block出来的时候会进入休眠状态，没有特殊事件不会被调度器调度，那么这时候来中断了就用WNV来通知一下， 将vCPU唤醒进来参与调度，让vCPU及时处理中断。 再次引用一下FengWu的图片来说明下VCPU在发生状态改变的时候，VMM做了哪些操作来保证posted interrupt能够顺利完成，请读者配合上面的描述自行进行梳理。 4 附: Posting格式中断重映射表项的格式","tags":"virtualization","url":"https://kernelgo.org/posted-interrupt.html","loc":"https://kernelgo.org/posted-interrupt.html"},{"title":"VT-d Interrupt Remapping","text":"Intel VT-d 虚拟化方案主要目的是解决IO虚拟化中的安全和性能这两个问题，这其中最为核心的技术就是DMA Remapping和Interrupt Remapping。 DMA Remapping通过IOMMU页表方式将直通设备对内存的访问限制到特定的domain中，在提高IO性能的同时完成了直通设备的隔离，保证了直通设备DMA的安全性。Interrupt Remapping则提供IO设备的中断重映射和路由功能，来达到中断隔离和中断迁移的目的，提升了虚拟化环境下直通设备的中断处理效率。 思考一下为什么要搞中断重映射这么一套东西呢？直通设备的中断不能直通到虚拟机内部吗？ 我们知道直通场景下直通设备的MSI/MSI-X Msg信息都是由Guest直接分配的，那么问题来了设备发送中断的时候写的Msg地址是GPA，肯定不能直接往host上投递，否则就乱套了。在虚拟化场景下，直通设备的中断是无法直接投递到Guest中的，那么我们该怎么办？我们可以由IOMMU截获中断，先将其中断映射到host的某个中断上，然后再重定向（由VMM投递）到Guest内部。明白这一点，很重要！ 下面对VT-d Interrupt Remapping机制进行一点分析，主要参考资料是 Intel VT-d SPEC Chapter 5 。 1 Interrupt Remapping 简介 Interrupt Remapping的出现改变了x86体系结构上的中断投递方式，外部中断源发出的中断请求格式发生了较大的改变， 中断请求会先被中断重映射硬件截获后再通过查询中断重映射表的方式最终投递到目标CPU上。 这些外部设备中断源则包括了中断控制器(I/OxAPICs)以及MSI/MSIX兼容设备PCI/PCIe设备等。 Interrupt Remapping是需要硬件来支持的，这里的硬件应该主要是指的IOMMU（尽管intel手册并没有直接说明），Interrupt Remapping的Capability是通过Extended Capability Register来报告的。 在没有使能Interrupt Remapping的情况下，设备中断请求格式称之为 Compatibility format ，其结构主要包含一个32bit的Address和一个32bit的Data字段，Address字段包含了中断要投递的目标CPU的APIC ID信息，Data字段主要包含了要投递的vecotr号和投递方式。结构如下图： 其中Address的bit 4为Interrupt Format位，用来标志这个Request是Compatibility format（bit4=0）还是Remapping format (bit 4=1)。 在开启了Interrupt Remapping之后，设备的中断请求格式称之为 Remapping format ，其结构同样由一个32bit的Address和一个32bit的Data字段构成。但与Compatibility format不同的是此时Adress字段不再包含目标CPU的APIC ID信息而是提供了一个16bit的HANDLE索引，并且Address的bit 4为\"1\"表示Request为Remapping format。同时bit 3是一个标识位(SHV)，用来标志Request是否包含了SubHandle，当该位置位时表示Data字段的低16bit为SubHandle索引。Remapping format的中断请求格式如下图： 在Interrupt Remapping模式下，硬件查询系统软件在内存中预设的中断重映射表(Interrupt Remapping Table)来投递中断。中断重映射表由中断重映射表项(Interrupt Remapping Table Entry)构成，每个IRTE占用128bit（具体格式介绍见文末），中断重映射表的基地址存放在Interrupt Remapping Table Address Register中。硬件通过下面的方式去计算中断的interrupt_index： if ( address . SHV == 0 ) { interrupt_index = address . handle ; } else { interrupt_index = ( address . handle + data . subhandle ) ; } 中断重映射硬件通过interrupt_index去重映射表中索引对应的IRTE，中断重映射硬件可以缓存那些经常使用的IRTE以提升性能。(注:由于handle为16bit，故每个IRT包含65536个IRTE，占用1MB内存空间) 2 外设的中断投递方式和中断处理 针对不同的中断源，需要采用不同的方式来投递Remapping格式的中断。 对I/OxAPIC而言，其Remapping格式中断投递格式如下图，软件需要按图中的格式来发起Remapping中断请求，这就要求需要修改\"中断重定向表项\"(Interrupt Redirection Table Entry)，读者可以参考 wiki 对比下RTE相比于Compatibility格式有哪些不同。值得注意的是bit48这里需要设置为\"1\"用来标志此RTE为Remapping format，并且RTE的bit10:8固定为000b(即没有SubHandle)。而且vector字段必须和IRTE的vector字段相同！ 对于MSI和MSI-X而言，其Remapping格式中断投递格式如下图，值得注意的是在Remapping格式下MSI中断支持multiple vector（大于32个中断向量），但软件必须连续分配N个连续的IRTE并且interrupt_index对应HANDLE号必须为N个连续的IRTE的首个。同样bit 4必须为\"1\"用来表示中断请求为Remapping格式。Data位全部设置为\"0\"。 中断重映射的硬件处理步骤如下： 硬件识别到物理地址0xFEEx_xxxx范围内的DWORD写请时，将该请求认定为中断请求； 当Interrupt Remapping没有使能时，所有的中断都按照Compatibility format来处理； 当Intgrrupt Remapping被使能时，中断请求处理流程如下： 如果来的中断请求为Compatibility format： 先检测IRTA寄存器的EIME位，如果该位为\"1\"那么Compatibility format的中断被blocked，否则Compatibility format中断请求都按照pass-through方式处理（传统方式）。 如果来的中断请求为Remapping format： 先检测reserved fileds是否为0，如果检查失败那么中断请求被blocked。接着硬件按照上面提到的算法计算出interrupt_index并检测其是否合法，如果该interrupt_index合法那么根据interrupt_index索引中断重映射表找到对应的IRTE，然后检测IRTE中的Present位，如果Preset位为0那么中断请求被blocked，如果Present位为1，硬件校验IRTE其他field合法后按照IRTE的约定产生一条中断请求。 中断重映射的软件处理步骤如下： 分配一个IRTE并且按照IRTE的格式要求填好IRTE的每个属性； 按照Remapping format的要求对中断源进行编程，在合适的时候触发一个Remapping format格式的中断请求。 附：Remapping格式中断重映射表项的格式 Interrupt Remapping格式的中断重映射表项的格式为（下篇会介绍Interrupt Posting格式的中断重映射表项）: 其中比较关键的中断描述信息为： Present域(P)：0b表示此IRTE还没有被分配到任何的中断源，索引到此IRTE的Remapping中断将被blocked掉，1b表示此IRTE是有效的，已经被分配到某个设备。 Destination Mode域(DM)：0b表示Destination ID域为Physical APIC-ID，1b表示Destination ID域是Logical APIC-ID。 IRTE Mode域(IM)：0b表示此中断请求是一个Remapped Interrupt中断请求，1b表示此中断请求是一个Posted Interrupt中断请求。 Vector域(V)：共8个Byte表示了此Remapped Interrupt中断请求的vector号(Remapped Interrupt)。 Destination ID域(DST)：表示此中断请求的目标CPU，根据当前Host中断方式不同具有不同的格式。xAPIC Mode (Cluster)为bit[40:47]， xAPIC Mode (Flat)和xAPIC Mode (Physical)为bit[47:40]， x2APIC Mode (Cluster)和x2APIC Mode (Physical)为bit[31:0]。 SID, SQ, SVT则联合起来表示了中断请求的设备PCI/PCI-e request-id信息。","tags":"virtualization","url":"https://kernelgo.org/interrupt-remapping.html","loc":"https://kernelgo.org/interrupt-remapping.html"},{"title":"kprobe kretprobe example","text":"有时候想知道下发某个操作后内核在做些什么，这个时候就要内内核进行调试， 然而KGDB这种方法操作起来相对麻烦，这个时候我们就可以使用kprobe来探测内核的行为。 介绍kprobe和kretprobe的文档为: https://www.kernel.org/doc/Documentation/kprobes.txt 划重点 ，这里是文档和很多博客没有解释清楚的地方： 对于kprobe而言，理论上它可以probe任何一个地方（只需要指定某个代码段地址就行了，例如函数的地址）； 对于kprobe而言，pre_handler回调函数执行是发生在probe断点执行之前，post_handler回调执行是发生在probe断点 单步执行 之后，而不是函数返回之前； 对于kretprobe而言，一般只用来探测函数，entry_handler回调执行是在函数入口的地方（这时候我们可以探测函数的入参），handler回调函数执行是发生在函数准备返回的时候，注意这个时候参数都已经弹栈，我们只能探测函数的返回值。 下面举一个最简单的例子，介绍如何使用kprobe来查看 inet_bind 这个函数的调用情况。 inet_bind 函数是在发生ipv4 socket bind阶段调用的一个内核函数。 #include <linux/kernel.h> #include <linux/module.h> #include <linux/sched/clock.h> #include <linux/kprobes.h> #include <linux/errno.h> #include <linux/stddef.h> #include <linux/bug.h> #include <linux/ptrace.h> #include <linux/socket.h> #include <linux/kmod.h> #include <linux/sched.h> #include <linux/string.h> #include <linux/sockios.h> #include <linux/net.h> #include <linux/inet.h> #include <net/ip.h> #include <net/tcp.h> #include <linux/skbuff.h> #include <net/sock.h> #include <net/inet_common.h> /* For each probe you need to allocate a kprobe structure */ static struct kprobe kp = { . symbol_name = \"inet_bind\" , }; /* kprobe pre_handler: called just before the probed instruction is executed */ static int handler_pre ( struct kprobe * p , struct pt_regs * regs ) { #ifdef CONFIG_ARM64 struct socket * sock = regs -> regs [ 0 ]; struct sockaddr * uaddr = regs -> regs [ 1 ]; #endif #ifdef CONFIG_X86 struct socket * sock = regs -> di ; struct sockaddr * uaddr = regs -> si ; #endif struct sockaddr_in * addr = ( struct sockaddr_in * ) uaddr ; unsigned short snum = ntohs ( addr -> sin_port ); pr_info ( \"%s name:%s pid:%d socket bind port=%d \\n \" , p -> symbol_name , current -> comm , task_pid_nr ( current ), snum ); return 0 ; } /* kprobe post_handler: called after the probed instruction is executed */ static void handler_post ( struct kprobe * p , struct pt_regs * regs , unsigned long flags ) { pr_info ( \"%s called \\n \" , __func__ ); } /* * fault_handler: this is called if an exception is generated for any * instruction within the pre- or post-handler, or when Kprobes * single-steps the probed instruction. */ static int handler_fault ( struct kprobe * p , struct pt_regs * regs , int trapnr ) { printk ( KERN_INFO \"fault_handler: p->addr = 0x%p, trap #%dn\" , p -> addr , trapnr ); /* Return 0 because we don't handle the fault. */ return 0 ; } static int __init kprobe_init ( void ) { int ret ; kp . pre_handler = handler_pre ; kp . post_handler = handler_post ; kp . fault_handler = handler_fault ; ret = register_kprobe ( & kp ); if ( ret < 0 ) { printk ( KERN_INFO \"register_kprobe failed, returned %d \\n \" , ret ); return ret ; } printk ( KERN_INFO \"Planted kprobe at %p \\n \" , kp . addr ); return 0 ; } static void __exit kprobe_exit ( void ) { unregister_kprobe ( & kp ); printk ( KERN_INFO \"kprobe at %p unregistered \\n \" , kp . addr ); } module_init ( kprobe_init ) module_exit ( kprobe_exit ) MODULE_LICENSE ( \"GPL\" ); 值得一提的是，kprobe里面我们在probe某个函数的时候，获取函数参数的时候是和体系结构相关的。 例如：在x86平台上，根据C ABI ptrace 接口规范，函数的参数和pt_regs的对应关系是： struct pt_regs { /* * C ABI says these regs are callee-preserved. They aren't saved on kernel entry * unless syscall needs a complete, fully filled \"struct pt_regs\". */ unsigned long r15 ; unsigned long r14 ; unsigned long r13 ; unsigned long r12 ; unsigned long bp ; unsigned long bx ; /* These regs are callee-clobbered. Always saved on kernel entry. */ unsigned long r11 ; unsigned long r10 ; unsigned long r9 ; unsigned long r8 ; unsigned long ax ; unsigned long cx ; // mapped to arg[3] unsigned long dx ; // mapped to arg[2] unsigned long si ; // mapped to arg[1] unsigned long di ; // mapped to arg[0] /* * On syscall entry, this is syscall#. On CPU exception, this is error code. * On hw interrupt, it's IRQ number: */ unsigned long orig_ax ; /* Return frame for iretq */ unsigned long ip ; unsigned long cs ; unsigned long flags ; unsigned long sp ; unsigned long ss ; /* top of stack page */ }; 在ARM64平台上，根据C ABI ptrace 规范， 函数的参数和pt_regs的对应关系是：入参args[0]对应了regs[0]，入参args[1]对应regs[1]依此类推。 /* * This struct defines the way the registers are stored on the stack during an * exception. Note that sizeof(struct pt_regs) has to be a multiple of 16 (for * stack alignment). struct user_pt_regs must form a prefix of struct pt_regs. */ struct pt_regs { union { struct user_pt_regs user_regs ; struct { u64 regs [ 31 ]; u64 sp ; u64 pc ; u64 pstate ; }; }; u64 orig_x0 ; #ifdef __AARCH64EB__ u32 unused2 ; s32 syscallno ; #else s32 syscallno ; u32 unused2 ; #endif u64 orig_addr_limit ; /* Only valid when ARM64_HAS_IRQ_PRIO_MASKING is enabled. */ u64 pmr_save ; u64 stackframe [ 2 ]; }; 使用下面的Makefile文件，对其进行编译。 obj-m : = kprobe_example.o CROSS_COMPILE = '' KDIR : = /lib/modules/ $( shell uname -r ) /build all: make -C $( KDIR ) M = $( PWD ) modules clean: rm -f *.ko *.o *.mod.o *.mod.c .*.cmd *.symvers modul* 编译完成后会生产一个名为 kprobe_example.ko 的内核模块文件，执行 insmod kprobe_example.ko 后内核模块立即生效，通过 dmesg 命令可以查看到 inet_bind 这个函数的调用情况。从dmesg日志可以看到： [ 46071 . 632951 ] inet_bind name : test pid : 68248 socdmket bind port = 49152 [ 46071 . 632984 ] inet_bind name : test pid : 68248 socket bind port = 49152 [ 46071 . 632995 ] inet_bind name : test pid : 68248 socket bind port = 49152 kretprobe可以用来探测函数的返回值，示例中我们用它来探测 inet_release 函数的返回值和执行时间： /* * kretprobe_example.c * * Here's a sample kernel module showing the use of return probes to * report the return value and total time taken for probed function * to run. * * usage: insmod kretprobe_example.ko func=<func_name> * * If no func_name is specified, inet_release is instrumented * * For more information on theory of operation of kretprobes, see * Documentation/kprobes.txt * * Build and insert the kernel module as done in the kprobe example. * You will see the trace data in /var/log/messages and on the console * whenever the probed function returns. (Some messages may be suppressed * if syslogd is configured to eliminate duplicate messages.) */ #include <linux/kernel.h> #include <linux/module.h> #include <linux/kprobes.h> #include <linux/ktime.h> #include <linux/limits.h> #include <linux/sched.h> static char func_name [ NAME_MAX ] = \"inet_release\" ; module_param_string ( func , func_name , NAME_MAX , S_IRUGO ); MODULE_PARM_DESC ( func , \"Function to kretprobe; this module will report the\" \" function's execution time\" ); /* per-instance private data */ struct my_data { ktime_t entry_stamp ; }; /* Here we use the entry_hanlder to timestamp function entry */ static int entry_handler ( struct kretprobe_instance * ri , struct pt_regs * regs ) { struct my_data * data ; if ( ! current -> mm ) return 1 ; /* Skip kernel threads */ data = ( struct my_data * ) ri -> data ; data -> entry_stamp = ktime_get (); return 0 ; } /* * Return-probe handler: Log the return value and duration. Duration may turn * out to be zero consistently, depending upon the granularity of time * accounting on the platform. */ static int ret_handler ( struct kretprobe_instance * ri , struct pt_regs * regs ) { int retval = regs_return_value ( regs ); struct my_data * data = ( struct my_data * ) ri -> data ; s64 delta ; ktime_t now ; now = ktime_get (); delta = ktime_to_ns ( ktime_sub ( now , data -> entry_stamp )); printk ( KERN_INFO \"%s returned %d and took %lld ns to execute \\n \" , func_name , retval , ( long long ) delta ); return 0 ; } static struct kretprobe my_kretprobe = { . handler = ret_handler , . entry_handler = entry_handler , . data_size = sizeof ( struct my_data ), /* Probe up to 20 instances concurrently. */ . maxactive = 20 , }; static int __init kretprobe_init ( void ) { int ret ; my_kretprobe . kp . symbol_name = func_name ; ret = register_kretprobe ( & my_kretprobe ); if ( ret < 0 ) { printk ( KERN_INFO \"register_kretprobe failed, returned %d \\n \" , ret ); return - 1 ; } printk ( KERN_INFO \"Planted return probe at %s: %p \\n \" , my_kretprobe . kp . symbol_name , my_kretprobe . kp . addr ); return 0 ; } static void __exit kretprobe_exit ( void ) { unregister_kretprobe ( & my_kretprobe ); printk ( KERN_INFO \"kretprobe at %p unregistered \\n \" , my_kretprobe . kp . addr ); /* nmissed > 0 suggests that maxactive was set too low. */ printk ( KERN_INFO \"Missed probing %d instances of %s \\n \" , my_kretprobe . nmissed , my_kretprobe . kp . symbol_name ); } module_init ( kretprobe_init ) module_exit ( kretprobe_exit ) MODULE_LICENSE ( \"GPL\" ); 探测的结果输出如下： [ 60362 .085372 ] inet_release returned 0 and took 16360 ns to execute [ 60362 .091124 ] inet_release returned 0 and took 8880 ns to execute [ 60362 .091147 ] inet_release returned 0 and took 7640 ns to execute [ 60362 .091173 ] inet_release returned 0 and took 7900 ns to execute [ 60362 .941665 ] inet_release returned 0 and took 9100 ns to execute [ 60363 .099577 ] inet_release returned 0 and took 9240 ns to execute [ 60363 .126682 ] inet_release returned 0 and took 6000 ns to execute [ 60363 .153610 ] inet_release returned 0 and took 9060 ns to execute [ 60363 .153820 ] inet_release returned 0 and took 3220 ns to execute [ 60363 .154699 ] inet_release returned 0 and took 3260 ns to execute [ 60363 .159178 ] inet_release returned 0 and took 3200 ns to execute [ 60363 .180098 ] inet_release returned 0 and took 3080 ns to execute","tags":"linux","url":"https://kernelgo.org/kprobe.html","loc":"https://kernelgo.org/kprobe.html"}]};